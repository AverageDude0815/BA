{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ISeKjfAJm5f"
   },
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J0yQz-PLJm5h"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcuPLrk3Jm5i"
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6I_DKS3Jm5j",
    "outputId": "e21a163d-925a-4ced-abfc-6ff380c43de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sflUHHt_Jm5k",
    "outputId": "fdf00f60-5751-46c6-d47d-5eedb678937d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llw9SqliJm5l"
   },
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TG7vqM9SJm5m",
    "outputId": "8dced9e3-8754-4fdb-da69-68a53a4e530b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "#X_train = torch.rand(256, 10)\n",
    "#X_test = torch.rand(256, 10)\n",
    "#y_train = torch.randint(10, (256,))\n",
    "#y_test = torch.randint(10, (256,))\n",
    "#X_train = torch.tensor([[0], [1], [2], [3], [4], [5]])\n",
    "#y_train = torch.tensor([[10], [8], [6], [4], [2], [0]])\n",
    "#X_test = torch.tensor([[1], [3], [5]])\n",
    "#y_test = torch.tensor([[8], [4], [0]])\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "#model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "#model = nn.Linear(1, 1)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHC67elPJm5o"
   },
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zubTsO4JJm5o",
    "outputId": "cee51ba2-1af9-4ffd-ce87-ef9c65c820a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: Tesla P100-PCIE-16GB\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fU7OC3LJm5p"
   },
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HthNLrCFJm5p",
    "outputId": "2ea79b09-16e0-425c-f7eb-36318191538b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t47y46R-Jm5q"
   },
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bbREbzmiJm5r"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        # stop tau from increasing infinetely\n",
    "        self.upper_bound_tau = 1e7\n",
    "        self.lower_bound_tau = 1.5\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, epoch):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        average_lr = 0\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            #print('enter adasecant')\n",
    "            average_lr += adasecant(self, params_with_grad, next_gradients, epoch)\n",
    "\n",
    "        return average_lr / len(self.param_groups)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor], epoch: int):\n",
    "\n",
    "    average_lr = 0\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        epsilon = 1e-7\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "\n",
    "        sgd = -0.01 * g\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        # normalization of gradients\n",
    "        #g = g / torch.linalg.norm(optimizer.mean_gradients[i])\n",
    "        #g_next = g_next / torch.linalg.norm(optimizer.mean_gradients[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = 0 for first iteration because no second derivative can be made yet\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        gamma = optimizer.gamma_numerators[i] / (optimizer.gamma_denomenators[i] + epsilon)\n",
    "\n",
    "        if epochs == 0:\n",
    "            corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma + epsilon)\n",
    "        else:\n",
    "            corrected_gradient = g\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            # delta = -lr * corrected_gradient, lr = 1 as initialization\n",
    "            delta = -copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])        \n",
    "        \n",
    "        lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "              - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        lr[lr != lr] = 0.0\n",
    "        average_lr += torch.mean(lr).item()\n",
    "        \n",
    "        optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / (optimizer.mean_delta_squares[i] + epsilon))\n",
    "                             * optimizer.taus[i] + 1)\n",
    "        optimizer.taus[i] = torch.where(optimizer.taus[i] < optimizer.lower_bound_tau,\n",
    "                                        torch.full_like(optimizer.taus[i], optimizer.lower_bound_tau),\n",
    "                                        optimizer.taus[i])\n",
    "        optimizer.taus[i] = torch.where(optimizer.taus[i] > optimizer.upper_bound_tau,\n",
    "                                        torch.full_like(optimizer.taus[i], optimizer.upper_bound_tau),\n",
    "                                        optimizer.taus[i])\n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if epoch < 1:\n",
    "            new_delta = sgd\n",
    "        else:\n",
    "            new_delta = -lr * corrected_gradient\n",
    "        #new_delta = torch.where(torch.abs(new_delta) > torch.abs(g), -g, new_delta)\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "        debug = False\n",
    "        if debug:\n",
    "            if i == 1:\n",
    "                print('tau', optimizer.taus[i][0], '\\n')\n",
    "                print('lr', lr[0], '\\n')\n",
    "                #print('corrected g', corrected_gradient[0], '\\n')\n",
    "                print('new delta', new_delta[0], '\\n')\n",
    "                print('sgd', sgd[0], '\\n')\n",
    "                #print('params', params[i][0], '\\n')\n",
    "                         \n",
    "    return average_lr / len(params)\n",
    "\n",
    "                \n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wCrkWwr_Jm5u"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRXQegFPJm5x"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pc_Ry-U2Jm5x"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Y1I5SlnVJm5y"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            #print(yhat)\n",
    "            #batch_loss = f_loss(yhat, batch['y'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                #batch_loss = f_loss(yhat, next_batch['y'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                lr = optimizer.step(epoch)\n",
    "                lr_history.append(lr)\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)} - LR: {lr}')#, '\\n\\n')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaO1e_X-Jm5y",
    "outputId": "2d5cc20d-999d-4b6a-d6a5-b500d51a2dbd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 1.622083822631836 - LR: 0.0018131215260983745\n",
      "Epoch 2/30 - Loss: 1.2955612731933595 - LR: 1.029560168342172e-05\n",
      "Epoch 3/30 - Loss: 1.2848935498046874 - LR: 1.5560381482463766e-06\n",
      "Epoch 4/30 - Loss: 1.285487164001465 - LR: 2.803159522511311e-07\n",
      "Epoch 5/30 - Loss: 1.284006354675293 - LR: 7.284508852038074e-08\n",
      "Epoch 6/30 - Loss: 1.2855260858154296 - LR: 3.3141772602914684e-08\n",
      "Epoch 7/30 - Loss: 1.2882461846923827 - LR: 2.3820311249023108e-08\n",
      "Epoch 8/30 - Loss: 1.2861499606323241 - LR: 1.6820403364060168e-08\n",
      "Epoch 9/30 - Loss: 1.2852928857421875 - LR: 1.4136934750932083e-08\n",
      "Epoch 10/30 - Loss: 1.284898246154785 - LR: 1.1435915555031646e-08\n",
      "Epoch 11/30 - Loss: 1.2856852818298339 - LR: 1.0048449857870771e-08\n",
      "Epoch 12/30 - Loss: 1.283951203918457 - LR: 9.18882942495995e-09\n",
      "Epoch 13/30 - Loss: 1.2835168127441405 - LR: 9.176085497182576e-09\n",
      "Epoch 14/30 - Loss: 1.283827283630371 - LR: 9.15048762210865e-09\n",
      "Epoch 15/30 - Loss: 1.2838384057617187 - LR: 8.377323609009125e-09\n",
      "Epoch 16/30 - Loss: 1.2869505114746094 - LR: 8.182082051320939e-09\n",
      "Epoch 17/30 - Loss: 1.2841000936889648 - LR: 8.181905103258937e-09\n",
      "Epoch 18/30 - Loss: 1.286258508911133 - LR: 8.18168231373021e-09\n",
      "Epoch 19/30 - Loss: 1.2839205366516113 - LR: 7.135652011926977e-09\n",
      "Epoch 20/30 - Loss: 1.2854714938354492 - LR: 7.068847242325384e-09\n",
      "Epoch 21/30 - Loss: 1.2839931036376953 - LR: 6.936149031482971e-09\n",
      "Epoch 22/30 - Loss: 1.2864672058105469 - LR: 6.8984283805073756e-09\n",
      "Epoch 23/30 - Loss: 1.285482200317383 - LR: 6.893470167455892e-09\n",
      "Epoch 24/30 - Loss: 1.284470980682373 - LR: 6.880784458341953e-09\n",
      "Epoch 25/30 - Loss: 1.284956610107422 - LR: 6.8805327886890054e-09\n",
      "Epoch 26/30 - Loss: 1.2854786431884766 - LR: 6.8805451658849055e-09\n",
      "Epoch 27/30 - Loss: 1.2849418856811523 - LR: 6.8805630440571285e-09\n",
      "Epoch 28/30 - Loss: 1.285065782775879 - LR: 6.8805735875943566e-09\n",
      "Epoch 29/30 - Loss: 1.2870873037719726 - LR: 6.880557084666371e-09\n",
      "Epoch 30/30 - Loss: 1.2835830834960937 - LR: 6.88057542125311e-09\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "3GwWYm1GJm5z",
    "outputId": "ab6135fc-b7e2-4f2b-efb5-17bd048131e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb09814f4d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "aF-QQ7gcJm5z",
    "outputId": "be8ac8cf-fcc9-4c94-8ac0-7593114a7ce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.001)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)\n",
    "plt.ylim(0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "tZ_yPh9mJm5z",
    "outputId": "1697ec95-cfa7-4dd4-9ea5-f55530c58834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5105\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "KPeq8KD9Jm5z",
    "outputId": "8c0c9e46-6b22-4f43-8473-61446df87b0d"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_ce4c806d-ba07-497a-a0be-9972b32f8c68\", \"train_loss\", 579)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_60ab0aa2-9a9d-4df8-8521-3c015abfda63\", \"val_loss\", 584)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_3671d764-2731-4ad2-b6ef-8d9dca5a8c67\", \"val_accuracy\", 218)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_196be7c3-a628-4ca2-a589-1ebe624e7c34\", \"lr_history\", 258373)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "AdaSecant (Training Environment).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
