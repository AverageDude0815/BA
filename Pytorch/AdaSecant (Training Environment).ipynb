{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            #print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # TODO: fix IF(torch.is_nonzero(...sum())) with element-wise where\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        #print(i)\n",
    "        \n",
    "        # normalization of gradients\n",
    "        g = g / torch.linalg.norm(g)\n",
    "        g_next = g_next / torch.linalg.norm(g_next)\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = 0 for first iteration because no second derivative can be made yet\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        gamma[gamma != gamma] = 0.0\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            # delta = -lr * corrected_gradient, lr = 1 as initialization\n",
    "            delta = -copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])        \n",
    "        \n",
    "        # should I update moving averages for g, gamma as well? -> memory size will be set again later\n",
    "        \n",
    "        lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "              - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        lr[lr != lr] = 0.0\n",
    "        \n",
    "        optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2/ optimizer.mean_delta_squares[i])\n",
    "                             * optimizer.taus[i] + 1)\n",
    "        optimizer.taus[i][optimizer.taus[i] != optimizer.taus[i]] = 1.0\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "        #debug = True\n",
    "        debug = False\n",
    "        if debug:\n",
    "            if i == 1:\n",
    "                print('g', g)\n",
    "                print('mg', optimizer.mean_gradients[i])\n",
    "                print('g_next', g_next)\n",
    "                print('mgs', optimizer.mean_gradient_squares[i])\n",
    "                print('gamma', gamma)\n",
    "                print('md', optimizer.mean_deltas[i])\n",
    "                print('mds', optimizer.mean_delta_squares[i])\n",
    "                print('ma', optimizer.mean_alphas[i])\n",
    "                print('mas', optimizer.mean_alpha_squares[i])\n",
    "                print('mdas', optimizer.mean_delta_times_alphas[i])\n",
    "                print('corrected_gradient', corrected_gradient)\n",
    "                print('lr', lr)\n",
    "                print('tau', optimizer.taus[i])\n",
    "                print('new delta', new_delta)\n",
    "                print('params', params[i])\n",
    "\n",
    "                \n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 4.255293130874634\n",
      "Epoch 2/100 - Loss: 5.101239502429962\n",
      "Epoch 3/100 - Loss: 2.7414482831954956\n",
      "Epoch 4/100 - Loss: 2.4651829600334167\n",
      "Epoch 5/100 - Loss: 2.4312589168548584\n",
      "Epoch 6/100 - Loss: 2.360778272151947\n",
      "Epoch 7/100 - Loss: 2.331010341644287\n",
      "Epoch 8/100 - Loss: 2.3080314993858337\n",
      "Epoch 9/100 - Loss: 2.2986353039741516\n",
      "Epoch 10/100 - Loss: 2.288422167301178\n",
      "Epoch 11/100 - Loss: 2.2923445105552673\n",
      "Epoch 12/100 - Loss: 2.2804678082466125\n",
      "Epoch 13/100 - Loss: 2.2785401344299316\n",
      "Epoch 14/100 - Loss: 2.2764678597450256\n",
      "Epoch 15/100 - Loss: 2.273044168949127\n",
      "Epoch 16/100 - Loss: 2.2726548314094543\n",
      "Epoch 17/100 - Loss: 2.271200120449066\n",
      "Epoch 18/100 - Loss: 2.27096951007843\n",
      "Epoch 19/100 - Loss: 2.269701063632965\n",
      "Epoch 20/100 - Loss: 2.268016815185547\n",
      "Epoch 21/100 - Loss: 2.265764534473419\n",
      "Epoch 22/100 - Loss: 2.263558328151703\n",
      "Epoch 23/100 - Loss: 2.2613911032676697\n",
      "Epoch 24/100 - Loss: 2.2606326937675476\n",
      "Epoch 25/100 - Loss: 2.260775327682495\n",
      "Epoch 26/100 - Loss: 2.259996712207794\n",
      "Epoch 27/100 - Loss: 2.2605369091033936\n",
      "Epoch 28/100 - Loss: 2.2600203156471252\n",
      "Epoch 29/100 - Loss: 2.25934636592865\n",
      "Epoch 30/100 - Loss: 2.258575737476349\n",
      "Epoch 31/100 - Loss: 2.2583022713661194\n",
      "Epoch 32/100 - Loss: 2.258255660533905\n",
      "Epoch 33/100 - Loss: 2.2577247619628906\n",
      "Epoch 34/100 - Loss: 2.2578346133232117\n",
      "Epoch 35/100 - Loss: 2.2574912905693054\n",
      "Epoch 36/100 - Loss: 2.257362484931946\n",
      "Epoch 37/100 - Loss: 2.257404148578644\n",
      "Epoch 38/100 - Loss: 2.2574421763420105\n",
      "Epoch 39/100 - Loss: 2.2574169039726257\n",
      "Epoch 40/100 - Loss: 2.2573376297950745\n",
      "Epoch 41/100 - Loss: 2.2573034167289734\n",
      "Epoch 42/100 - Loss: 2.257347822189331\n",
      "Epoch 43/100 - Loss: 2.2573102712631226\n",
      "Epoch 44/100 - Loss: 2.2573349475860596\n",
      "Epoch 45/100 - Loss: 2.2572704553604126\n",
      "Epoch 46/100 - Loss: 2.2570992708206177\n",
      "Epoch 47/100 - Loss: 2.257135272026062\n",
      "Epoch 48/100 - Loss: 2.2570762038230896\n",
      "Epoch 49/100 - Loss: 2.2570926547050476\n",
      "Epoch 50/100 - Loss: 2.2570942044258118\n",
      "Epoch 51/100 - Loss: 2.2570980191230774\n",
      "Epoch 52/100 - Loss: 2.257096827030182\n",
      "Epoch 53/100 - Loss: 2.257050633430481\n",
      "Epoch 54/100 - Loss: 2.257041275501251\n",
      "Epoch 55/100 - Loss: 2.257071852684021\n",
      "Epoch 56/100 - Loss: 2.257062077522278\n",
      "Epoch 57/100 - Loss: 2.2570276260375977\n",
      "Epoch 58/100 - Loss: 2.257033407688141\n",
      "Epoch 59/100 - Loss: 2.2570412158966064\n",
      "Epoch 60/100 - Loss: 2.25703763961792\n",
      "Epoch 61/100 - Loss: 2.2570298314094543\n",
      "Epoch 62/100 - Loss: 2.257047712802887\n",
      "Epoch 63/100 - Loss: 2.2570412158966064\n",
      "Epoch 64/100 - Loss: 2.257041335105896\n",
      "Epoch 65/100 - Loss: 2.2570359110832214\n",
      "Epoch 66/100 - Loss: 2.257037878036499\n",
      "Epoch 67/100 - Loss: 2.2570366859436035\n",
      "Epoch 68/100 - Loss: 2.2570388317108154\n",
      "Epoch 69/100 - Loss: 2.257036507129669\n",
      "Epoch 70/100 - Loss: 2.2570351362228394\n",
      "Epoch 71/100 - Loss: 2.257035255432129\n",
      "Epoch 72/100 - Loss: 2.257033884525299\n",
      "Epoch 73/100 - Loss: 2.257037043571472\n",
      "Epoch 74/100 - Loss: 2.25703501701355\n",
      "Epoch 75/100 - Loss: 2.2570350766181946\n",
      "Epoch 76/100 - Loss: 2.257034957408905\n",
      "Epoch 77/100 - Loss: 2.2570347785949707\n",
      "Epoch 78/100 - Loss: 2.2570348381996155\n",
      "Epoch 79/100 - Loss: 2.257034659385681\n",
      "Epoch 80/100 - Loss: 2.2570345401763916\n",
      "Epoch 81/100 - Loss: 2.257034718990326\n",
      "Epoch 82/100 - Loss: 2.257034957408905\n",
      "Epoch 83/100 - Loss: 2.2570345401763916\n",
      "Epoch 84/100 - Loss: 2.2570347785949707\n",
      "Epoch 85/100 - Loss: 2.2570348381996155\n",
      "Epoch 86/100 - Loss: 2.2570343613624573\n",
      "Epoch 87/100 - Loss: 2.257034480571747\n",
      "Epoch 88/100 - Loss: 2.257034420967102\n",
      "Epoch 89/100 - Loss: 2.2570343613624573\n",
      "Epoch 90/100 - Loss: 2.257034480571747\n",
      "Epoch 91/100 - Loss: 2.257034480571747\n",
      "Epoch 92/100 - Loss: 2.257034420967102\n",
      "Epoch 93/100 - Loss: 2.2570343017578125\n",
      "Epoch 94/100 - Loss: 2.257034480571747\n",
      "Epoch 95/100 - Loss: 2.2570343017578125\n",
      "Epoch 96/100 - Loss: 2.257034420967102\n",
      "Epoch 97/100 - Loss: 2.2570343613624573\n",
      "Epoch 98/100 - Loss: 2.2570343613624573\n",
      "Epoch 99/100 - Loss: 2.257034122943878\n",
      "Epoch 100/100 - Loss: 2.2570343017578125\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2319278ae50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2319287f640>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
