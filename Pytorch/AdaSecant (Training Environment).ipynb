{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # TODO: fix IF(torch.is_nonzero(...sum())) with element-wise where\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # TODO later: build in normalization of gradients\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = g for first iteration ==> lr = 0\n",
    "            #alpha = copy.deepcopy(g)\n",
    "            \n",
    "            # alpha = 0 for first iteration because no second derivative can be made yet\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        if torch.is_nonzero(optimizer.gamma_denomenators[i].sum()):\n",
    "            # normal calculation of variance reduction term gamma\n",
    "            gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        else:\n",
    "            # gradient is equal to expected gradient in first epoch\n",
    "            # gamma is therefore zero (equivalent to no variance reduction)\n",
    "            gamma = torch.zeros_like(optimizer.gamma_denomenators[i])\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            # delta = lr * corrected_gradient, lr = 1 as initialization\n",
    "            delta = copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        if i == 1:\n",
    "            print('md', optimizer.mean_deltas[i])\n",
    "            print('mds', optimizer.mean_delta_squares[i])\n",
    "            print('ma', optimizer.mean_alphas[i])\n",
    "            print('mas', optimizer.mean_alpha_squares[i])\n",
    "        \n",
    "        \n",
    "        # should I update moving averages for g, gamma, and alpha as well? -> memory size will be set again later\n",
    "        \n",
    "        if torch.is_nonzero(optimizer.mean_alphas[i].sum()) or torch.is_nonzero(optimizer.mean_alpha_squares[i].sum()):\n",
    "            # normal calculation of lr\n",
    "            lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "                 - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        else:\n",
    "            # Catch initial exception by setting lr = 1. Because: lr = 0 ==> delta = 0 ==> optimization stops \n",
    "            lr = torch.ones_like(g)\n",
    "        \n",
    "        if i == 1:\n",
    "            print('corrected_gradient', corrected_gradient)\n",
    "            print('lr', lr)\n",
    "        \n",
    "        if torch.is_nonzero(optimizer.mean_delta_squares[i].sum()):\n",
    "            optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / optimizer.mean_delta_squares[i])\n",
    "                                 * optimizer.taus[i] + 1)\n",
    "        else:\n",
    "            optimizer.taus[i] = torch.ones_like(optimizer.taus[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('tau', optimizer.taus[i])\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        if i == 1:\n",
    "            print('new delta', new_delta)\n",
    "        \n",
    "        if i == 1:\n",
    "            print('params', params[i])\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    #prepare_model(model)\n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([-0.0468,  0.0126,  0.0332, -0.0726,  0.0382, -0.0334, -0.0230,  0.0685,\n",
      "        -0.0158,  0.0019], device='cuda:0')\n",
      "mds tensor([2.1915e-03, 1.5935e-04, 1.1036e-03, 5.2778e-03, 1.4556e-03, 1.1154e-03,\n",
      "        5.3013e-04, 4.6963e-03, 2.4900e-04, 3.4357e-06], device='cuda:0')\n",
      "ma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0468,  0.0126,  0.0332, -0.0726,  0.0382, -0.0334, -0.0230,  0.0685,\n",
      "        -0.0158,  0.0019], device='cuda:0')\n",
      "lr tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([ 0.0468, -0.0126, -0.0332,  0.0726, -0.0382,  0.0334,  0.0230, -0.0685,\n",
      "         0.0158, -0.0019], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.2675, -0.2129, -0.2290, -0.0179, -0.1301,  0.1541,  0.0178, -0.0899,\n",
      "         0.1616,  0.1016], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([ 0.0468, -0.0126, -0.0332,  0.0726, -0.0382,  0.0334,  0.0230, -0.0685,\n",
      "         0.0158, -0.0019], device='cuda:0')\n",
      "mds tensor([2.1915e-03, 1.5935e-04, 1.1036e-03, 5.2778e-03, 1.4556e-03, 1.1154e-03,\n",
      "        5.3013e-04, 4.6963e-03, 2.4900e-04, 3.4357e-06], device='cuda:0')\n",
      "ma tensor([ 0.0050, -0.0240, -0.0354,  0.0718, -0.0279,  0.0223,  0.0357, -0.0596,\n",
      "         0.0139,  0.0032], device='cuda:0')\n",
      "mas tensor([2.5077e-05, 5.7592e-04, 1.2512e-03, 5.1551e-03, 7.7659e-04, 4.9746e-04,\n",
      "        1.2710e-03, 3.5523e-03, 1.9193e-04, 1.0128e-05], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0418, -0.0114, -0.0022, -0.0008,  0.0103, -0.0111,  0.0126,  0.0089,\n",
      "        -0.0019,  0.0050], device='cuda:0')\n",
      "lr tensor([ 0.0000e+00, -5.9605e-08,  1.1921e-07,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1921e-07,  1.1649e+00],\n",
      "       device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([ 0.0000e+00, -6.7801e-10,  2.5656e-10,  0.0000e+00, -0.0000e+00,\n",
      "         0.0000e+00, -0.0000e+00, -0.0000e+00, -2.2960e-10, -5.8664e-03],\n",
      "       device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.2675, -0.2129, -0.2290, -0.0179, -0.1301,  0.1541,  0.0178, -0.0899,\n",
      "         0.1616,  0.0957], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([ 0.0000e+00, -6.7801e-10,  2.5656e-10,  0.0000e+00, -0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -0.0000e+00, -2.2960e-10, -5.8664e-03],\n",
      "       device='cuda:0')\n",
      "mds tensor([0.0000e+00, 4.5970e-19, 6.5825e-20, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 5.2717e-20, 3.4414e-05], device='cuda:0')\n",
      "ma tensor([ 0.1047,  0.0944, -0.1192, -0.1234,  0.0879,  0.1169,  0.1849, -0.0662,\n",
      "        -0.1869, -0.0960], device='cuda:0')\n",
      "mas tensor([0.0110, 0.0089, 0.0142, 0.0152, 0.0077, 0.0137, 0.0342, 0.0044, 0.0349,\n",
      "        0.0092], device='cuda:0')\n",
      "corrected_gradient tensor([ 0.0629,  0.0830, -0.1214, -0.1242,  0.0982,  0.1058,  0.1975, -0.0573,\n",
      "        -0.1888, -0.0910], device='cuda:0')\n",
      "lr tensor([0.0000e+00, 1.4362e-08, 4.3038e-09, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00], device='cuda:0')\n",
      "tau tensor([nan, 1., 1., nan, nan, nan, nan, nan, 1., 1.], device='cuda:0')\n",
      "new delta tensor([-0.0000e+00, -1.1927e-09,  5.2239e-10,  0.0000e+00, -0.0000e+00,\n",
      "        -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "       device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.2675, -0.2129, -0.2290, -0.0179, -0.1301,  0.1541,  0.0178, -0.0899,\n",
      "         0.1616,  0.0957], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "Epoch 1/1 - Loss: 2.7049211859703064\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19306db9df0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1930be88be0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.109375\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
