{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # TODO: fix IF(torch.is_nonzero(...sum())) with element-wise where\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # normalization of gradients\n",
    "        g = g / torch.linalg.norm(g)\n",
    "        g_next = g_next / torch.linalg.norm(g_next)\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = g for first iteration ==> lr = 0\n",
    "            #alpha = copy.deepcopy(g)\n",
    "            \n",
    "            # alpha = 0 for first iteration because no second derivative can be made yet\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        if torch.is_nonzero(optimizer.gamma_denomenators[i].sum()):\n",
    "            # normal calculation of variance reduction term gamma\n",
    "            gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        else:\n",
    "            # gradient is equal to expected gradient in first epoch\n",
    "            # gamma is therefore zero (equivalent to no variance reduction)\n",
    "            gamma = torch.zeros_like(optimizer.gamma_denomenators[i])\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            # delta = -lr * corrected_gradient, lr = 1 as initialization\n",
    "            delta = -copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        if i == 1:\n",
    "            print('md', optimizer.mean_deltas[i])\n",
    "            print('mds', optimizer.mean_delta_squares[i])\n",
    "            print('ma', optimizer.mean_alphas[i])\n",
    "            print('mas', optimizer.mean_alpha_squares[i])\n",
    "            print('mdas', optimizer.mean_delta_times_alphas[i])\n",
    "        \n",
    "        \n",
    "        # should I update moving averages for g, gamma, and alpha as well? -> memory size will be set again later\n",
    "        \n",
    "        if torch.is_nonzero(optimizer.mean_alphas[i].sum()) or torch.is_nonzero(optimizer.mean_alpha_squares[i].sum()):\n",
    "            # normal calculation of lr\n",
    "            lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "                 - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        else:\n",
    "            # Catch initial exception by setting lr = 1. Because: lr = 0 ==> delta = 0 ==> optimization stops \n",
    "            lr = torch.ones_like(g)\n",
    "        \n",
    "        if i == 1:\n",
    "            print('corrected_gradient', corrected_gradient)\n",
    "            print('lr', lr)\n",
    "        \n",
    "        if torch.is_nonzero(optimizer.mean_delta_squares[i].sum()):\n",
    "            optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / optimizer.mean_delta_squares[i])\n",
    "                                 * optimizer.taus[i] + 1)\n",
    "        else:\n",
    "            optimizer.taus[i] = torch.ones_like(optimizer.taus[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('tau', optimizer.taus[i])\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        if i == 1:\n",
    "            print('new delta', new_delta)\n",
    "        \n",
    "        if i == 1:\n",
    "            print('params', params[i])\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([ 0.2770, -0.0609,  0.4674, -0.0585, -0.6502, -0.1524, -0.4061,  0.0724,\n",
      "         0.0133, -0.2853], device='cuda:0')\n",
      "mds tensor([7.6751e-02, 3.7134e-03, 2.1843e-01, 3.4170e-03, 4.2275e-01, 2.3220e-02,\n",
      "        1.6492e-01, 5.2347e-03, 1.7810e-04, 8.1392e-02], device='cuda:0')\n",
      "ma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mdas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "corrected_gradient tensor([-0.2770,  0.0609, -0.4674,  0.0585,  0.6502,  0.1524,  0.4061, -0.0724,\n",
      "        -0.0133,  0.2853], device='cuda:0')\n",
      "lr tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([ 0.2770, -0.0609,  0.4674, -0.0585, -0.6502, -0.1524, -0.4061,  0.0724,\n",
      "         0.0133, -0.2853], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.1088,  0.0548,  0.1713, -0.3181, -0.4391,  0.1437, -0.6384,  0.3742,\n",
      "         0.0515, -0.2700], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([ 0.2770, -0.0609,  0.4674, -0.0585, -0.6502, -0.1524, -0.4061,  0.0724,\n",
      "         0.0133, -0.2853], device='cuda:0')\n",
      "mds tensor([7.6751e-02, 3.7134e-03, 2.1843e-01, 3.4170e-03, 4.2275e-01, 2.3220e-02,\n",
      "        1.6492e-01, 5.2347e-03, 1.7810e-04, 8.1392e-02], device='cuda:0')\n",
      "ma tensor([ 0.1835,  0.1662,  0.6295, -0.2977, -0.6276,  0.1230, -0.6013,  0.7837,\n",
      "        -0.2100, -0.7164], device='cuda:0')\n",
      "mas tensor([0.0337, 0.0276, 0.3963, 0.0886, 0.3938, 0.0151, 0.3615, 0.6141, 0.0441,\n",
      "        0.5132], device='cuda:0')\n",
      "mdas tensor([ 0.0508, -0.0101,  0.2942,  0.0174,  0.4080, -0.0187,  0.2442,  0.0567,\n",
      "        -0.0028,  0.2044], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0935,  0.2272,  0.1622, -0.2393,  0.0226,  0.2754, -0.1952,  0.7113,\n",
      "        -0.2234, -0.4311], device='cuda:0')\n",
      "lr tensor([0.0000e+00, 7.3325e-01, 5.9605e-08, 1.4901e-08, 0.0000e+00, 2.4784e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.2710e-01, 2.9802e-08], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([ 0.0000e+00, -1.6656e-01, -9.6665e-09,  3.5653e-09, -0.0000e+00,\n",
      "        -6.8242e-01,  0.0000e+00, -0.0000e+00,  2.8387e-02,  1.2847e-08],\n",
      "       device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.1088, -0.1118,  0.1713, -0.3181, -0.4391, -0.5387, -0.6384,  0.3742,\n",
      "         0.0799, -0.2700], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([ 0.0000e+00, -1.6656e-01, -9.6665e-09,  3.5653e-09, -0.0000e+00,\n",
      "        -6.8242e-01,  0.0000e+00,  0.0000e+00,  2.8387e-02,  1.2847e-08],\n",
      "       device='cuda:0')\n",
      "mds tensor([0.0000e+00, 2.7741e-02, 9.3441e-17, 1.2711e-17, 0.0000e+00, 4.6569e-01,\n",
      "        0.0000e+00, 0.0000e+00, 8.0582e-04, 1.6504e-16], device='cuda:0')\n",
      "ma tensor([ 0.4821, -0.8579,  0.0993,  0.6673,  0.2098, -0.1096, -0.0198, -0.7769,\n",
      "         0.0318,  0.2555], device='cuda:0')\n",
      "mas tensor([2.3241e-01, 7.3604e-01, 9.8684e-03, 4.4525e-01, 4.4001e-02, 1.2015e-02,\n",
      "        3.9402e-04, 6.0364e-01, 1.0089e-03, 6.5258e-02], device='cuda:0')\n",
      "mdas tensor([ 0.0000e+00,  1.4289e-01, -9.6027e-10,  2.3790e-09,  0.0000e+00,\n",
      "         7.4802e-02,  0.0000e+00,  0.0000e+00,  9.0164e-04,  3.2818e-09],\n",
      "       device='cuda:0')\n",
      "corrected_gradient tensor([ 0.3886, -0.6308,  0.2615,  0.4280,  0.2324,  0.1657, -0.2150, -0.0656,\n",
      "        -0.1916, -0.1756], device='cuda:0')\n",
      "lr tensor([ 0.0000e+00,  0.0000e+00,  1.9462e-07,  0.0000e+00,  0.0000e+00,\n",
      "        -4.7684e-07,  0.0000e+00,  0.0000e+00, -5.9605e-08,  3.5527e-15],\n",
      "       device='cuda:0')\n",
      "tau tensor([nan, 1., 1., 1., nan, 1., nan, nan, 1., 1.], device='cuda:0')\n",
      "new delta tensor([-0.0000e+00,  0.0000e+00, -5.0895e-08, -0.0000e+00, -0.0000e+00,\n",
      "         7.9030e-08,  0.0000e+00,  0.0000e+00, -1.1420e-08,  6.2391e-16],\n",
      "       device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.1088, -0.1118,  0.1713, -0.3181, -0.4391, -0.5387, -0.6384,  0.3742,\n",
      "         0.0799, -0.2700], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "Epoch 1/1 - Loss: 2.7075011134147644\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21491af7fa0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2149252ee80>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOHUlEQVR4nO3c34tc533H8fenUkQJSbFdybYsyV011UXVUogYhCG9CPUPJMVYvuiFDYmFcyEMNTi0wVXqf8CJoTGmxkakBpm4mEASIoyCYru5VeqVY8uoiuONSKqNFHuTCyfgCyHy7cUetevNSDu7Z1a76+f9gmHmnPOcmedhwG/NmVmnqpAkteuPVnoCkqSVZQgkqXGGQJIaZwgkqXGGQJIat36lJ7AUGzdurImJiZWehiStKSdPnvx1VW2av39NhmBiYoLJycmVnoYkrSlJfjFsv5eGJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxYwlBkj1J3k4yleTQkONJ8lR3/FSSXfOOr0vy4yQvjWM+kqTR9Q5BknXA08BeYCdwf5Kd84btBXZ0t4PAM/OOPwKc6TsXSdLijeMTwW5gqqrOVtVF4EVg/7wx+4Hna9YJ4LokmwGSbAU+B3xjDHORJC3SOEKwBTg3Z3u62zfqmCeBR4HfX+1FkhxMMplkcmZmpteEJUn/bxwhyJB9NcqYJHcD71XVyYVepKoOV9WgqgabNm1ayjwlSUOMIwTTwLY521uB8yOO+QxwT5KfM3tJ6e+SfHMMc5IkjWgcIXgN2JFke5INwH3A0XljjgIPdL8eug14v6ouVNVXqmprVU105/1nVX1+DHOSJI1ofd8nqKpLSR4GjgPrgOeq6nSSh7rjzwLHgH3AFPAB8GDf15UkjUeq5l/OX/0Gg0FNTk6u9DQkaU1JcrKqBvP3+5fFktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjRtLCJLsSfJ2kqkkh4YcT5KnuuOnkuzq9m9L8sMkZ5KcTvLIOOYjSRpd7xAkWQc8DewFdgL3J9k5b9heYEd3Owg80+2/BPxTVf0lcBvwD0POlSQto3F8ItgNTFXV2aq6CLwI7J83Zj/wfM06AVyXZHNVXaiq1wGq6nfAGWDLGOYkSRrROEKwBTg3Z3uaP/yP+YJjkkwAnwZ+NIY5SZJGNI4QZMi+WsyYJJ8Avg18qap+O/RFkoNJJpNMzszMLHmykqQPG0cIpoFtc7a3AudHHZPkY8xG4IWq+s6VXqSqDlfVoKoGmzZtGsO0JUkwnhC8BuxIsj3JBuA+4Oi8MUeBB7pfD90GvF9VF5IE+HfgTFX96xjmIklapPV9n6CqLiV5GDgOrAOeq6rTSR7qjj8LHAP2AVPAB8CD3emfAb4AvJXkjW7fv1TVsb7zkiSNJlXzL+evfoPBoCYnJ1d6GpK0piQ5WVWD+fv9y2JJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJatxYQpBkT5K3k0wlOTTkeJI81R0/lWTXqOdKkpZX7xAkWQc8DewFdgL3J9k5b9heYEd3Owg8s4hzJUnLaByfCHYDU1V1tqouAi8C++eN2Q88X7NOANcl2TziuZKkZTSOEGwBzs3Znu72jTJmlHMBSHIwyWSSyZmZmd6TliTNGkcIMmRfjThmlHNnd1YdrqpBVQ02bdq0yClKkq5k/RieYxrYNmd7K3B+xDEbRjhXkrSMxvGJ4DVgR5LtSTYA9wFH5405CjzQ/XroNuD9qrow4rmSpGXU+xNBVV1K8jBwHFgHPFdVp5M81B1/FjgG7AOmgA+AB692bt85SZJGl6qhl+RXtcFgUJOTkys9DUlaU5KcrKrB/P3+ZbEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjeoUgyQ1JXk7yTnd//RXG7UnydpKpJIfm7H8iyU+SnEry3STX9ZmPJGnx+n4iOAS8WlU7gFe77Q9Jsg54GtgL7ATuT7KzO/wy8NdV9TfAT4Gv9JyPJGmR+oZgP3Cke3wEuHfImN3AVFWdraqLwIvdeVTVD6rqUjfuBLC153wkSYvUNwQ3VdUFgO7+xiFjtgDn5mxPd/vm+yLw/Z7zkSQt0vqFBiR5Bbh5yKHHRnyNDNlX817jMeAS8MJV5nEQOAhw6623jvjSkqSFLBiCqrrjSseSvJtkc1VdSLIZeG/IsGlg25ztrcD5Oc9xALgbuL2qiiuoqsPAYYDBYHDFcZKkxel7aegocKB7fAD43pAxrwE7kmxPsgG4rzuPJHuAfwbuqaoPes5FkrQEfUPwOHBnkneAO7ttktyS5BhA92Xww8Bx4Azwrao63Z3/b8AngZeTvJHk2Z7zkSQt0oKXhq6mqn4D3D5k/3lg35ztY8CxIeP+os/rS5L68y+LJalxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxvUKQ5IYkLyd5p7u//grj9iR5O8lUkkNDjn85SSXZ2Gc+kqTF6/uJ4BDwalXtAF7ttj8kyTrgaWAvsBO4P8nOOce3AXcC/9NzLpKkJegbgv3Ake7xEeDeIWN2A1NVdbaqLgIvdudd9nXgUaB6zkWStAR9Q3BTVV0A6O5vHDJmC3BuzvZ0t48k9wC/rKo3F3qhJAeTTCaZnJmZ6TltSdJl6xcakOQV4OYhhx4b8TUyZF8l+Xj3HHeN8iRVdRg4DDAYDPz0IEljsmAIquqOKx1L8m6SzVV1Iclm4L0hw6aBbXO2twLngU8B24E3k1ze/3qS3VX1q0WsQZLUQ99LQ0eBA93jA8D3hox5DdiRZHuSDcB9wNGqequqbqyqiaqaYDYYu4yAJF1bfUPwOHBnkneY/eXP4wBJbklyDKCqLgEPA8eBM8C3qup0z9eVJI3JgpeGrqaqfgPcPmT/eWDfnO1jwLEFnmuiz1wkSUvjXxZLUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1LlW10nNYtCQzwC9Weh5LsBH49UpP4hpqbb3gmluxVtf8Z1W1af7ONRmCtSrJZFUNVnoe10pr6wXX3IqP2pq9NCRJjTMEktQ4Q3BtHV7pCVxjra0XXHMrPlJr9jsCSWqcnwgkqXGGQJIaZwjGKMkNSV5O8k53f/0Vxu1J8naSqSSHhhz/cpJKsnH5Z91P3zUneSLJT5KcSvLdJNdds8kv0gjvW5I81R0/lWTXqOeuVktdc5JtSX6Y5EyS00keufazX5o+73N3fF2SHyd56drNuqeq8jamG/A14FD3+BDw1SFj1gE/A/4c2AC8Ceycc3wbcJzZP5jbuNJrWu41A3cB67vHXx12/mq4LfS+dWP2Ad8HAtwG/GjUc1fjreeaNwO7usefBH76UV/znOP/CPwH8NJKr2fUm58Ixms/cKR7fAS4d8iY3cBUVZ2tqovAi915l30deBRYK9/i91pzVf2gqi51404AW5d3uku20PtGt/18zToBXJdk84jnrkZLXnNVXaiq1wGq6nfAGWDLtZz8EvV5n0myFfgc8I1rOem+DMF43VRVFwC6+xuHjNkCnJuzPd3tI8k9wC+r6s3lnugY9VrzPF9k9l9aq9Eoa7jSmFHXv9r0WfP/STIBfBr40finOHZ91/wks/+Q+/0yzW9ZrF/pCaw1SV4Bbh5y6LFRn2LIvkry8e457lrq3JbLcq153ms8BlwCXljc7K6ZBddwlTGjnLsa9Vnz7MHkE8C3gS9V1W/HOLflsuQ1J7kbeK+qTib57LgntpwMwSJV1R1XOpbk3csfi7uPiu8NGTbN7PcAl20FzgOfArYDbya5vP/1JLur6ldjW8ASLOOaLz/HAeBu4PbqLrKuQlddwwJjNoxw7mrUZ80k+RizEXihqr6zjPMcpz5r/nvgniT7gD8G/iTJN6vq88s43/FY6S8pPko34Ak+/MXp14aMWQ+cZfY/+pe/jPqrIeN+ztr4srjXmoE9wH8Dm1Z6LQusc8H3jdlrw3O/RPyvxbznq+3Wc80BngeeXOl1XKs1zxvzWdbQl8UrPoGP0g34U+BV4J3u/oZu/y3AsTnj9jH7K4qfAY9d4bnWSgh6rRmYYvZ66xvd7dmVXtNV1voHawAeAh7qHgd4ujv+FjBYzHu+Gm9LXTPwt8xeUjk1573dt9LrWe73ec5zrKkQ+L+YkKTG+ashSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWrc/wLouA/ZRwywxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10546875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
