{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([6, 1])\n",
      "y_train: torch.Size([6, 1])\n",
      "X_test: torch.Size([3, 1])\n",
      "y_test: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "#X_train = torch.rand(256, 10)\n",
    "#X_test = torch.rand(256, 10)\n",
    "#y_train = torch.randint(10, (256,))\n",
    "#y_test = torch.randint(10, (256,))\n",
    "X_train = torch.tensor([[0], [1], [2], [3], [4], [5]])\n",
    "y_train = torch.tensor([[10], [8], [6], [4], [2], [0]])\n",
    "X_test = torch.tensor([[1], [3], [5]])\n",
    "y_test = torch.tensor([[8], [4], [0]])\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "#model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        # stop tau from increasing infinetely\n",
    "        self.upper_bound_tau = 1e7\n",
    "        self.lower_bound_tau = 1.5\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            #print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "\n",
    "        epsilon = 1e-7\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        # normalization of gradients\n",
    "        g = g / torch.linalg.norm(optimizer.mean_gradients[i])\n",
    "        g_next = g_next / torch.linalg.norm(optimizer.mean_gradients[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = 0 for first iteration because no second derivative can be made yet\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        gamma = optimizer.gamma_numerators[i] / (optimizer.gamma_denomenators[i] + epsilon)\n",
    "        #gamma[gamma != gamma] = 0.0\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma + epsilon)\n",
    "        #corrected_gradient[corrected_gradient != corrected_gradient] = 0.0\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            # delta = -lr * corrected_gradient, lr = 1 as initialization\n",
    "            delta = -copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])        \n",
    "        \n",
    "        lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "              - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        lr[lr != lr] = 0.0\n",
    "        \n",
    "        optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / (optimizer.mean_delta_squares[i] + epsilon))\n",
    "                             * optimizer.taus[i] + 1)\n",
    "        optimizer.taus[i] = torch.where(optimizer.taus[i] < optimizer.lower_bound_tau,\n",
    "                                        torch.full_like(optimizer.taus[i], optimizer.lower_bound_tau),\n",
    "                                        optimizer.taus[i])\n",
    "        optimizer.taus[i] = torch.where(optimizer.taus[i] > optimizer.upper_bound_tau,\n",
    "                                        torch.full_like(optimizer.taus[i], optimizer.upper_bound_tau),\n",
    "                                        optimizer.taus[i])\n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        #optimizer.taus[i][optimizer.taus[i] != optimizer.taus[i]] = 2.2\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "        debug = False\n",
    "        if debug:\n",
    "            print('lr', lr, '\\n')\n",
    "            print('corrected g', corrected_gradient, '\\n')\n",
    "            print('new delta', new_delta, '\\n')\n",
    "            print('params', params[i], '\\n')\n",
    "\n",
    "                \n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            #print(yhat)\n",
    "            batch_loss = f_loss(yhat, batch['y'].float().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].float().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')#, '\\n\\n')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 512.1243743896484 \n",
      "\n",
      "\n",
      "Epoch 2/100 - Loss: 1248.24267578125 \n",
      "\n",
      "\n",
      "Epoch 3/100 - Loss: 1134.7515360514324 \n",
      "\n",
      "\n",
      "Epoch 4/100 - Loss: 957.1084289550781 \n",
      "\n",
      "\n",
      "Epoch 5/100 - Loss: 748.346773147583 \n",
      "\n",
      "\n",
      "Epoch 6/100 - Loss: 499.0196812947591 \n",
      "\n",
      "\n",
      "Epoch 7/100 - Loss: 347.9877522786458 \n",
      "\n",
      "\n",
      "Epoch 8/100 - Loss: 288.8027178446452 \n",
      "\n",
      "\n",
      "Epoch 9/100 - Loss: 201.51711146036783 \n",
      "\n",
      "\n",
      "Epoch 10/100 - Loss: 128.45670064290366 \n",
      "\n",
      "\n",
      "Epoch 11/100 - Loss: 101.09466298421223 \n",
      "\n",
      "\n",
      "Epoch 12/100 - Loss: 76.71779696146648 \n",
      "\n",
      "\n",
      "Epoch 13/100 - Loss: 43.58969497680664 \n",
      "\n",
      "\n",
      "Epoch 14/100 - Loss: 30.39157247543335 \n",
      "\n",
      "\n",
      "Epoch 15/100 - Loss: 28.130548397699993 \n",
      "\n",
      "\n",
      "Epoch 16/100 - Loss: 26.78829526901245 \n",
      "\n",
      "\n",
      "Epoch 17/100 - Loss: 23.872721195220947 \n",
      "\n",
      "\n",
      "Epoch 18/100 - Loss: 21.08266528447469 \n",
      "\n",
      "\n",
      "Epoch 19/100 - Loss: 19.923609415690105 \n",
      "\n",
      "\n",
      "Epoch 20/100 - Loss: 19.404008547465008 \n",
      "\n",
      "\n",
      "Epoch 21/100 - Loss: 18.579259872436523 \n",
      "\n",
      "\n",
      "Epoch 22/100 - Loss: 17.758721351623535 \n",
      "\n",
      "\n",
      "Epoch 23/100 - Loss: 17.27797345320384 \n",
      "\n",
      "\n",
      "Epoch 24/100 - Loss: 18.101590792338055 \n",
      "\n",
      "\n",
      "Epoch 25/100 - Loss: 16.340303262074787 \n",
      "\n",
      "\n",
      "Epoch 26/100 - Loss: 15.698445002237955 \n",
      "\n",
      "\n",
      "Epoch 27/100 - Loss: 15.659647941589355 \n",
      "\n",
      "\n",
      "Epoch 28/100 - Loss: 16.087239027023315 \n",
      "\n",
      "\n",
      "Epoch 29/100 - Loss: 14.28725783030192 \n",
      "\n",
      "\n",
      "Epoch 30/100 - Loss: 10.77231607834498 \n",
      "\n",
      "\n",
      "Epoch 31/100 - Loss: 9.383607546488443 \n",
      "\n",
      "\n",
      "Epoch 32/100 - Loss: 8.69801664352417 \n",
      "\n",
      "\n",
      "Epoch 33/100 - Loss: 8.859844843546549 \n",
      "\n",
      "\n",
      "Epoch 34/100 - Loss: 9.014376481374105 \n",
      "\n",
      "\n",
      "Epoch 35/100 - Loss: 8.498998721440634 \n",
      "\n",
      "\n",
      "Epoch 36/100 - Loss: 8.573902328809103 \n",
      "\n",
      "\n",
      "Epoch 37/100 - Loss: 9.210795720418295 \n",
      "\n",
      "\n",
      "Epoch 38/100 - Loss: 7.361976782480876 \n",
      "\n",
      "\n",
      "Epoch 39/100 - Loss: 7.978377342224121 \n",
      "\n",
      "\n",
      "Epoch 40/100 - Loss: 7.982885281244914 \n",
      "\n",
      "\n",
      "Epoch 41/100 - Loss: 7.740227778752645 \n",
      "\n",
      "\n",
      "Epoch 42/100 - Loss: 11.035605351130167 \n",
      "\n",
      "\n",
      "Epoch 43/100 - Loss: 9.13381322224935 \n",
      "\n",
      "\n",
      "Epoch 44/100 - Loss: 8.789216121037802 \n",
      "\n",
      "\n",
      "Epoch 45/100 - Loss: 8.69927453994751 \n",
      "\n",
      "\n",
      "Epoch 46/100 - Loss: 8.140618622303009 \n",
      "\n",
      "\n",
      "Epoch 47/100 - Loss: 7.990019083023071 \n",
      "\n",
      "\n",
      "Epoch 48/100 - Loss: 7.75536568959554 \n",
      "\n",
      "\n",
      "Epoch 49/100 - Loss: 7.603506247202556 \n",
      "\n",
      "\n",
      "Epoch 50/100 - Loss: 7.307866255442302 \n",
      "\n",
      "\n",
      "Epoch 51/100 - Loss: 7.123099684715271 \n",
      "\n",
      "\n",
      "Epoch 52/100 - Loss: 7.033856431643168 \n",
      "\n",
      "\n",
      "Epoch 53/100 - Loss: 6.949090639750163 \n",
      "\n",
      "\n",
      "Epoch 54/100 - Loss: 6.946933190027873 \n",
      "\n",
      "\n",
      "Epoch 55/100 - Loss: 6.8227113087972 \n",
      "\n",
      "\n",
      "Epoch 56/100 - Loss: 6.815848191579183 \n",
      "\n",
      "\n",
      "Epoch 57/100 - Loss: 6.7416378657023115 \n",
      "\n",
      "\n",
      "Epoch 58/100 - Loss: 6.734935998916626 \n",
      "\n",
      "\n",
      "Epoch 59/100 - Loss: 6.648923794428508 \n",
      "\n",
      "\n",
      "Epoch 60/100 - Loss: 6.658269087473552 \n",
      "\n",
      "\n",
      "Epoch 61/100 - Loss: 6.645241896311442 \n",
      "\n",
      "\n",
      "Epoch 62/100 - Loss: 6.644776503245036 \n",
      "\n",
      "\n",
      "Epoch 63/100 - Loss: 6.605282704035441 \n",
      "\n",
      "\n",
      "Epoch 64/100 - Loss: 6.591202179590861 \n",
      "\n",
      "\n",
      "Epoch 65/100 - Loss: 6.576165278752645 \n",
      "\n",
      "\n",
      "Epoch 66/100 - Loss: 6.580405553181966 \n",
      "\n",
      "\n",
      "Epoch 67/100 - Loss: 6.570132493972778 \n",
      "\n",
      "\n",
      "Epoch 68/100 - Loss: 6.587004522482554 \n",
      "\n",
      "\n",
      "Epoch 69/100 - Loss: 6.553418477376302 \n",
      "\n",
      "\n",
      "Epoch 70/100 - Loss: 6.5570723215738935 \n",
      "\n",
      "\n",
      "Epoch 71/100 - Loss: 6.570103724797566 \n",
      "\n",
      "\n",
      "Epoch 72/100 - Loss: 6.558591643969218 \n",
      "\n",
      "\n",
      "Epoch 73/100 - Loss: 6.56697662671407 \n",
      "\n",
      "\n",
      "Epoch 74/100 - Loss: 6.548564275105794 \n",
      "\n",
      "\n",
      "Epoch 75/100 - Loss: 6.551997900009155 \n",
      "\n",
      "\n",
      "Epoch 76/100 - Loss: 6.589335997899373 \n",
      "\n",
      "\n",
      "Epoch 77/100 - Loss: 6.596046686172485 \n",
      "\n",
      "\n",
      "Epoch 78/100 - Loss: 6.591866731643677 \n",
      "\n",
      "\n",
      "Epoch 79/100 - Loss: 6.591281374295552 \n",
      "\n",
      "\n",
      "Epoch 80/100 - Loss: 6.589828650156657 \n",
      "\n",
      "\n",
      "Epoch 81/100 - Loss: 6.5878528753916425 \n",
      "\n",
      "\n",
      "Epoch 82/100 - Loss: 6.586599508921306 \n",
      "\n",
      "\n",
      "Epoch 83/100 - Loss: 6.583150386810303 \n",
      "\n",
      "\n",
      "Epoch 84/100 - Loss: 6.582955837249756 \n",
      "\n",
      "\n",
      "Epoch 85/100 - Loss: 6.579646666844686 \n",
      "\n",
      "\n",
      "Epoch 86/100 - Loss: 6.576571424802144 \n",
      "\n",
      "\n",
      "Epoch 87/100 - Loss: 6.57453066110611 \n",
      "\n",
      "\n",
      "Epoch 88/100 - Loss: 6.574079116185506 \n",
      "\n",
      "\n",
      "Epoch 89/100 - Loss: 6.571225881576538 \n",
      "\n",
      "\n",
      "Epoch 90/100 - Loss: 6.571434895197551 \n",
      "\n",
      "\n",
      "Epoch 91/100 - Loss: 6.570822397867839 \n",
      "\n",
      "\n",
      "Epoch 92/100 - Loss: 6.569174766540527 \n",
      "\n",
      "\n",
      "Epoch 93/100 - Loss: 6.567511220773061 \n",
      "\n",
      "\n",
      "Epoch 94/100 - Loss: 6.567500114440918 \n",
      "\n",
      "\n",
      "Epoch 95/100 - Loss: 6.565689166386922 \n",
      "\n",
      "\n",
      "Epoch 96/100 - Loss: 6.563738584518433 \n",
      "\n",
      "\n",
      "Epoch 97/100 - Loss: 6.5631818373998 \n",
      "\n",
      "\n",
      "Epoch 98/100 - Loss: 6.561142206192017 \n",
      "\n",
      "\n",
      "Epoch 99/100 - Loss: 6.559990803400676 \n",
      "\n",
      "\n",
      "Epoch 100/100 - Loss: 6.558712402979533 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 2\n",
    "epochs = 100\n",
    "f_loss = F.mse_loss\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22c80d4de20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22c830ee520>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
