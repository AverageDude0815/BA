{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "Parameter containing:\n",
      "tensor([[-0.2026,  0.0802,  0.1864,  0.1556,  0.1961,  0.1832, -0.3142,  0.1757,\n",
      "         -0.2408, -0.2625],\n",
      "        [ 0.2683, -0.0877, -0.0190,  0.2238,  0.2496, -0.1994, -0.1143, -0.0334,\n",
      "         -0.1553,  0.2690],\n",
      "        [ 0.0797, -0.0405,  0.1398, -0.3045,  0.2751,  0.2007,  0.0178, -0.2272,\n",
      "         -0.0803,  0.2042],\n",
      "        [ 0.3013, -0.1717,  0.2273, -0.0526, -0.2902,  0.0883, -0.1911,  0.2672,\n",
      "         -0.0373, -0.0308],\n",
      "        [ 0.2150,  0.2921,  0.1988, -0.0246,  0.1659, -0.0638,  0.0380, -0.0587,\n",
      "          0.0324,  0.0970],\n",
      "        [ 0.2077,  0.0472, -0.0776, -0.0444, -0.0771,  0.2112,  0.1076,  0.2671,\n",
      "          0.1086,  0.1905],\n",
      "        [-0.0382,  0.0154,  0.0427,  0.0625, -0.2785, -0.2935, -0.2910, -0.1417,\n",
      "         -0.2142, -0.0707],\n",
      "        [-0.1938, -0.2773, -0.2428, -0.2357, -0.2460,  0.1720, -0.0445,  0.1801,\n",
      "         -0.2061, -0.1487],\n",
      "        [ 0.2764, -0.1083,  0.2596, -0.2260, -0.0495,  0.0896,  0.2538, -0.0385,\n",
      "         -0.0949,  0.1042],\n",
      "        [-0.0258, -0.2131, -0.1917,  0.1399,  0.0422,  0.2509,  0.1537,  0.2128,\n",
      "          0.0890, -0.0045]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0970,  0.3154,  0.2099,  0.2789, -0.2825,  0.3092,  0.0419, -0.2134,\n",
      "         0.1430, -0.2798], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2288,  0.0754,  0.1368,  0.1021, -0.0580,  0.0056, -0.1341, -0.2670,\n",
      "         -0.2250,  0.1343],\n",
      "        [ 0.0367,  0.1625, -0.2110,  0.2704,  0.1144,  0.1500, -0.0677, -0.0100,\n",
      "         -0.2393,  0.0491],\n",
      "        [-0.0288,  0.0118,  0.1924,  0.2353,  0.2246,  0.0533, -0.0453, -0.2982,\n",
      "         -0.2291,  0.0532],\n",
      "        [ 0.1468, -0.0457,  0.0512, -0.1021,  0.1363,  0.1563,  0.1346, -0.2991,\n",
      "         -0.2638, -0.2419],\n",
      "        [-0.0698,  0.0934, -0.1310, -0.0141,  0.1047, -0.1216, -0.3031, -0.0772,\n",
      "          0.0808,  0.1117],\n",
      "        [-0.0040,  0.0497,  0.0995,  0.0951,  0.0335, -0.0399,  0.0330, -0.1520,\n",
      "         -0.0791,  0.2567],\n",
      "        [ 0.1542,  0.0936, -0.2595,  0.2076,  0.0530,  0.2786, -0.2937,  0.0989,\n",
      "          0.1953, -0.1416],\n",
      "        [ 0.1094, -0.2309,  0.2028, -0.0383, -0.1074, -0.0340, -0.2509, -0.1405,\n",
      "          0.0021,  0.2834],\n",
      "        [-0.1588, -0.0403,  0.1193, -0.2251,  0.2440,  0.1022, -0.0348, -0.2374,\n",
      "          0.0224,  0.1252],\n",
      "        [-0.0034,  0.0004,  0.2621,  0.1449, -0.2425,  0.0150, -0.0547,  0.2467,\n",
      "         -0.2817, -0.0284]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0465,  0.0960, -0.2622,  0.2712, -0.2105, -0.2951, -0.0854,  0.0932,\n",
      "         0.2003, -0.1616], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p.grad))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p.grad))\n",
    "                    self.old_gradients.append(torch.zeros_like(p.grad))\n",
    "                    self.taus.append(torch.ones_like(p.grad))\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                    # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                    if p.grad is not None:\n",
    "                        d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "        return #loss\n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    for i, param in enumerate(params):\n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        # later: build in normalization of gradients\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next) * \n",
    "                                                       (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i]) * \n",
    "                                                         (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "\n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    print('test')\n",
    "    assert torch.all(torch.eq(1 / tau, torch.ones_like(tau) / tau))\n",
    "    print((1 - 1 / tau) * mean + (1 / tau) * new_value)\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter adasecant\n",
      "0\n",
      "test\n",
      "tensor([[ 0.0052, -0.0027, -0.0016,  0.0007,  0.0051,  0.0029,  0.0022,  0.0049,\n",
      "          0.0051,  0.0034],\n",
      "        [ 0.0005, -0.0004,  0.0031, -0.0011,  0.0029,  0.0004, -0.0004, -0.0022,\n",
      "         -0.0001, -0.0006],\n",
      "        [-0.0115, -0.0072, -0.0149, -0.0091, -0.0149, -0.0139, -0.0124, -0.0070,\n",
      "         -0.0114, -0.0121],\n",
      "        [ 0.0066,  0.0006,  0.0034,  0.0007,  0.0051,  0.0020,  0.0017,  0.0037,\n",
      "          0.0028, -0.0053],\n",
      "        [-0.0036, -0.0020, -0.0023,  0.0035, -0.0045,  0.0022, -0.0079, -0.0012,\n",
      "         -0.0027,  0.0021],\n",
      "        [ 0.0069, -0.0006,  0.0032,  0.0022,  0.0054,  0.0065,  0.0011,  0.0037,\n",
      "          0.0105,  0.0054],\n",
      "        [-0.0108, -0.0162, -0.0171, -0.0120, -0.0098, -0.0105, -0.0168, -0.0112,\n",
      "         -0.0084, -0.0056],\n",
      "        [ 0.0113,  0.0069,  0.0076,  0.0021,  0.0072,  0.0013,  0.0104,  0.0058,\n",
      "          0.0069, -0.0027],\n",
      "        [ 0.0047,  0.0143,  0.0136,  0.0160,  0.0101,  0.0094,  0.0118,  0.0104,\n",
      "          0.0033,  0.0047],\n",
      "        [-0.0021,  0.0085,  0.0042,  0.0104, -0.0001,  0.0025,  0.0055,  0.0024,\n",
      "         -0.0086, -0.0053]], device='cuda:0')\n",
      "test\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "test\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "1\n",
      "test\n",
      "tensor([ 0.0045,  0.0012, -0.0208,  0.0052, -0.0021,  0.0105, -0.0194,  0.0098,\n",
      "         0.0157,  0.0009], device='cuda:0')\n",
      "test\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "test\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "2\n",
      "test\n",
      "tensor([[-3.6954e-03,  1.3757e-03,  1.0711e-02, -1.3491e-03,  3.2787e-03,\n",
      "          3.4828e-03, -6.4855e-03, -9.3725e-03,  7.2196e-03, -4.4270e-03],\n",
      "        [-9.0381e-03,  1.4750e-02,  1.5191e-02,  1.3958e-02,  4.2574e-03,\n",
      "          2.5038e-02, -1.9307e-02, -2.3144e-02,  1.7543e-02,  3.7944e-04],\n",
      "        [ 9.6096e-03, -1.2627e-02, -1.7958e-02, -9.7391e-05, -7.3585e-03,\n",
      "         -1.6576e-02,  1.9892e-02,  2.3691e-02, -1.5629e-02,  1.4750e-03],\n",
      "        [-1.0472e-02, -6.4148e-03,  2.1884e-03, -8.5265e-03, -8.1287e-03,\n",
      "         -1.2248e-02,  7.4531e-03,  2.2379e-02, -6.8590e-03,  4.4788e-03],\n",
      "        [ 1.1291e-03, -2.1291e-03,  8.4547e-05, -4.4873e-03,  5.5285e-03,\n",
      "         -3.8604e-03,  2.4240e-03, -1.8281e-03,  2.1372e-03, -3.5142e-03],\n",
      "        [ 1.7755e-02, -1.1016e-02, -1.3708e-02, -1.8200e-02, -7.3960e-03,\n",
      "         -3.1689e-02,  1.9831e-02,  2.7749e-02, -2.0070e-02,  3.8380e-03],\n",
      "        [ 1.9834e-04,  1.6476e-02,  1.1883e-02,  1.0949e-02,  6.1962e-03,\n",
      "          2.4164e-02, -2.1247e-02, -2.9561e-02,  1.1991e-02, -1.0777e-03],\n",
      "        [-4.3154e-03,  1.1262e-02,  5.4776e-03,  9.2552e-03,  5.0694e-03,\n",
      "          2.4377e-02, -1.8883e-02, -2.1652e-02,  1.1926e-02,  3.1123e-03],\n",
      "        [ 2.8162e-03, -7.0701e-05, -4.8673e-03,  2.7551e-03, -6.5689e-04,\n",
      "          1.5080e-03,  4.4280e-03, -3.2902e-03, -3.2171e-03, -2.0115e-03],\n",
      "        [-3.9875e-03, -1.1607e-02, -9.0030e-03, -4.2573e-03, -7.9000e-04,\n",
      "         -1.4196e-02,  1.1895e-02,  1.5029e-02, -5.0419e-03, -2.2531e-03]],\n",
      "       device='cuda:0')\n",
      "test\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "test\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "3\n",
      "test\n",
      "tensor([ 0.0089,  0.0292, -0.0220, -0.0210, -0.0058, -0.0323,  0.0334,  0.0250,\n",
      "         0.0036, -0.0190], device='cuda:0')\n",
      "test\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "test\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "Epoch 1/1 - Loss: 2.324591636657715\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f7a9a74d00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f7a9c91eb0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11328125\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
