{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # TODO: fix IF(torch.is_nonzero(...sum())) with element-wise where\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # normalization of gradients\n",
    "        g = g / torch.linalg.norm(g)\n",
    "        g_next = g_next / torch.linalg.norm(g_next)\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = 0 for first iteration because no second derivative can be made yet\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        gamma[gamma != gamma] = 0.0\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            # delta = -lr * corrected_gradient, lr = 1 as initialization\n",
    "            delta = -copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])        \n",
    "        \n",
    "        # should I update moving averages for g, gamma as well? -> memory size will be set again later\n",
    "        \n",
    "        lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "              - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        lr[lr != lr] = 0.0\n",
    "        \n",
    "        optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2/ optimizer.mean_delta_squares[i])\n",
    "                             * optimizer.taus[i] + 1)\n",
    "        optimizer.taus[i][optimizer.taus[i] != optimizer.taus[i]] = 1.0\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "        debug = True\n",
    "        if debug:\n",
    "            if i == 1:\n",
    "                print('g', g)\n",
    "                print('mg', optimizer.mean_gradients[i])\n",
    "                print('g_next', g_next)\n",
    "                print('mgs', optimizer.mean_gradient_squares[i])\n",
    "                print('gamma', gamma)\n",
    "                print('md', optimizer.mean_deltas[i])\n",
    "                print('mds', optimizer.mean_delta_squares[i])\n",
    "                print('ma', optimizer.mean_alphas[i])\n",
    "                print('mas', optimizer.mean_alpha_squares[i])\n",
    "                print('mdas', optimizer.mean_delta_times_alphas[i])\n",
    "                print('corrected_gradient', corrected_gradient)\n",
    "                print('lr', lr)\n",
    "                print('tau', optimizer.taus[i])\n",
    "                print('new delta', new_delta)\n",
    "                print('params', params[i])\n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "g tensor([-0.2253,  0.0683, -0.2175, -0.1113, -0.6259,  0.3661,  0.2822, -0.2264,\n",
      "         0.2770,  0.3892], device='cuda:0')\n",
      "mg tensor([-0.2253,  0.0683, -0.2175, -0.1113, -0.6259,  0.3661,  0.2822, -0.2264,\n",
      "         0.2770,  0.3892], device='cuda:0')\n",
      "g_next tensor([ 0.1543,  0.4621, -0.4558, -0.1292, -0.3853,  0.3334,  0.3501,  0.2470,\n",
      "         0.2870, -0.1125], device='cuda:0')\n",
      "mgs tensor([0.0507, 0.0047, 0.0473, 0.0124, 0.3918, 0.1340, 0.0796, 0.0513, 0.0767,\n",
      "        0.1515], device='cuda:0')\n",
      "gamma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "md tensor([ 0.2253, -0.0683,  0.2175,  0.1113,  0.6259, -0.3661, -0.2822,  0.2264,\n",
      "        -0.2770, -0.3892], device='cuda:0')\n",
      "mds tensor([0.0507, 0.0047, 0.0473, 0.0124, 0.3918, 0.1340, 0.0796, 0.0513, 0.0767,\n",
      "        0.1515], device='cuda:0')\n",
      "ma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mdas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "corrected_gradient tensor([-0.2253,  0.0683, -0.2175, -0.1113, -0.6259,  0.3661,  0.2822, -0.2264,\n",
      "         0.2770,  0.3892], device='cuda:0')\n",
      "lr tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([0., -0., 0., 0., 0., -0., -0., 0., -0., -0.], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.2649, -0.1271, -0.1782,  0.1665,  0.1905,  0.2301, -0.0588,  0.2885,\n",
      "         0.1680, -0.1880], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "g tensor([ 0.1543,  0.4621, -0.4558, -0.1292, -0.3853,  0.3334,  0.3501,  0.2470,\n",
      "         0.2870, -0.1125], device='cuda:0')\n",
      "mg tensor([ 0.1543,  0.4621, -0.4558, -0.1292, -0.3853,  0.3334,  0.3501,  0.2470,\n",
      "         0.2870, -0.1125], device='cuda:0')\n",
      "g_next tensor([ 0.2528,  0.2140, -0.3360, -0.4634,  0.0642, -0.0632,  0.5610,  0.2729,\n",
      "         0.2325, -0.3336], device='cuda:0')\n",
      "mgs tensor([0.0238, 0.2135, 0.2078, 0.0167, 0.1485, 0.1111, 0.1226, 0.0610, 0.0824,\n",
      "        0.0127], device='cuda:0')\n",
      "gamma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "md tensor([ 0.1229, -0.0372,  0.1187,  0.0607,  0.3414, -0.1997, -0.1539,  0.1235,\n",
      "        -0.1511, -0.2123], device='cuda:0')\n",
      "mds tensor([0.0277, 0.0025, 0.0258, 0.0068, 0.2137, 0.0731, 0.0434, 0.0280, 0.0419,\n",
      "        0.0826], device='cuda:0')\n",
      "ma tensor([ 0.1725,  0.1790, -0.1083, -0.0082,  0.1094, -0.0149,  0.0309,  0.2152,\n",
      "         0.0046, -0.2281], device='cuda:0')\n",
      "mas tensor([6.5472e-02, 7.0488e-02, 2.5816e-02, 1.4632e-04, 2.6311e-02, 4.8614e-04,\n",
      "        2.0950e-03, 1.0184e-01, 4.5797e-05, 1.1442e-01], device='cuda:0')\n",
      "mdas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "corrected_gradient tensor([ 0.1543,  0.4621, -0.4558, -0.1292, -0.3853,  0.3334,  0.3501,  0.2470,\n",
      "         0.2870, -0.1125], device='cuda:0')\n",
      "lr tensor([ 0.6502,  0.1900,  0.9999,  6.7955,  2.8499, 12.2617,  4.5535,  0.5239,\n",
      "        30.2294,  0.8499], device='cuda:0')\n",
      "tau tensor([2.0000, 2.0000, 2.0000, 2.0000, 2.0000, 2.0000, 2.0000, 2.0000, 2.0000,\n",
      "        2.0000], device='cuda:0')\n",
      "new delta tensor([-0.1003, -0.0878,  0.4558,  0.8783,  1.0981, -4.0875, -1.5941, -0.1294,\n",
      "        -8.6768,  0.0956], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.1646, -0.2149,  0.2775,  1.0448,  1.2886, -3.8574, -1.6530,  0.1591,\n",
      "        -8.5088, -0.0925], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "g tensor([-0.3841, -0.1581,  0.5705,  0.3155,  0.3109, -0.4276, -0.0126,  0.0836,\n",
      "        -0.2897,  0.1785], device='cuda:0')\n",
      "mg tensor([-0.1149,  0.1520,  0.0573,  0.0931, -0.0372, -0.0471,  0.1687,  0.1653,\n",
      "        -0.0013,  0.0330], device='cuda:0')\n",
      "g_next tensor([-0.1943, -0.0989,  0.6320,  0.2394,  0.1626, -0.6413, -0.1125,  0.0091,\n",
      "        -0.1677,  0.1312], device='cuda:0')\n",
      "mgs tensor([0.0857, 0.1193, 0.2666, 0.0581, 0.1226, 0.1470, 0.0614, 0.0340, 0.0832,\n",
      "        0.0223], device='cuda:0')\n",
      "gamma tensor([ 2.3927,  0.2361, -0.1070,  0.5200,  0.7427, -0.3597, -0.3551, -0.4771,\n",
      "         0.7340,  0.4818], device='cuda:0')\n",
      "md tensor([ 2.1427e-02, -6.0218e-02,  2.7190e-01,  4.3232e-01,  6.8536e-01,\n",
      "        -1.9669e+00, -8.0857e-01, -2.9496e-03, -4.0264e+00, -7.2356e-02],\n",
      "       device='cuda:0')\n",
      "mds tensor([1.9669e-02, 4.8900e-03, 1.0851e-01, 3.5429e-01, 6.6466e-01, 7.6343e+00,\n",
      "        1.1788e+00, 2.2349e-02, 3.4244e+01, 4.9232e-02], device='cuda:0')\n",
      "ma tensor([-0.1506, -0.1843,  0.4074,  0.1977,  0.3761, -0.3540, -0.1480,  0.0259,\n",
      "        -0.2597,  0.0079], device='cuda:0')\n",
      "mas tensor([0.1675, 0.2133, 0.4929, 0.0900, 0.2347, 0.2635, 0.0609, 0.0643, 0.1512,\n",
      "        0.1009], device='cuda:0')\n",
      "mdas tensor([0.0245, 0.0247, 0.2126, 0.1775, 0.3475, 1.4139, 0.2628, 0.0106, 2.2747,\n",
      "        0.0126], device='cuda:0')\n",
      "corrected_gradient tensor([-0.1943, -0.0989,  0.6320,  0.2394,  0.1626, -0.6413, -0.1125,  0.0091,\n",
      "        -0.1677,  0.1312], device='cuda:0')\n",
      "lr tensor([0.1961, 0.0354, 0.0378, 0.0112, 0.2021, 0.0168, 0.0854, 0.4253, 0.0063,\n",
      "        0.5732], device='cuda:0')\n",
      "tau tensor([3.1486, 1.5686, 1.7011, 2.0394, 1.6453, 2.0852, 1.9799, 2.9992, 2.1585,\n",
      "        2.9660], device='cuda:0')\n",
      "new delta tensor([ 0.0381,  0.0035, -0.0239, -0.0027, -0.0329,  0.0108,  0.0096, -0.0039,\n",
      "         0.0010, -0.0752], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.2027, -0.2114,  0.2537,  1.0421,  1.2557, -3.8466, -1.6434,  0.1552,\n",
      "        -8.5078, -0.1676], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "Epoch 1/1 - Loss: 14.81264841556549\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23402824fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23402a56850>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1328125\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAArLklEQVR4nO3dd3hUdfr+8feTRq8SihRBivRmRKQEC92CdQX9oWtDVKTp7tq2uLtucd0AKorYdVVERUVFKZYEkBaQXhRQIIIQRemd5/dHhv1mYzAnkGQymft1XbmY8zllns8FzD3nZOY55u6IiEj0iQl3ASIiEh4KABGRKKUAEBGJUgoAEZEopQAQEYlSceEuID+qVavm9evXD3cZIiIRZeHChd+7e2LO8YgKgPr165Oenh7uMkREIoqZbchtPNAlIDPrbWZrzGytmd2Ty/qmZjbHzA6Y2d3Zxkub2XwzW2JmK8zswWzr/mJmS81ssZlNM7NTT2RiIiJyYvIMADOLBcYCfYDmwAAza55js+3AUOCRHOMHgPPdvQ3QFuhtZh1D6/7l7q3dvS3wPvCHE52EiIjkX5AzgA7AWndf7+4HgQlAv+wbuPs2d18AHMox7u6+O7QYH/rx0Lqd2TYtd2xcRESKRpAAqA1syracERoLxMxizWwxsA2Y7u7zsq17yMw2AddynDMAMxtkZulmlp6ZmRn0aUVEJA9BAsByGQv8bt3dj4Qu89QBOphZy2zr7nf3usArwJDj7D/e3ZPcPSkx8We/xBYRkRMUJAAygLrZlusAm/P7RO7+E/AZ0DuX1a8CV+T3mCIicuKCBMACoLGZNTCzBKA/MDnIwc0s0cwqhx6XAboDq0PLjbNtesmxcRERKRp5fg/A3Q+b2RBgKhALPOfuK8xscGj9ODOrCaQDFYGjZjacrE8M1QJeDH2SKAaY6O7vhw79DzM7AzgKbAAGF+zU/s/c9T+wYvNOft2pPrExuV3REhGJPhZJ9wNISkryE/ki2O/fWc7LczfQrl5lHr6iNY1rVCiE6kREiiczW+juSTnHo6IX0J/7tWD01W355vs9XPjoLB79+CsOHj4a7rJERMIqKgLAzLi0XW2mj+xGr5Y1SZn+JZc8PoulGT+FuzQRkbCJigA4plr5Ujw2oB1PX5fEj3sPcunY2fx9yir2HTwS7tJERIpcVAXAMT2a12DaiG5cfVZdnkpbT58xacxd/0O4yxIRKVJRGQAAlcrE8/fLW/PqzWdz1KH/+Lnc//Yydu0/lPfOIiIlQNQGwDGdGlXjo+FdublLA16bv5Geo9L4ZPXWcJclIlLooj4AAMomxPHARc1567ZOVCgdx40vpDN8whds33Mw3KWJiBQaBUA27epV4f07uzLsgsZ8sGwL3VNSmbxkM5H0XQkRkaAUADkkxMUwokcT3ruzC3WrlGHoa19wy0vpfLdjf7hLExEpUAqA42hasyKTbu/M/X2bMWvt9/RISeW1+Rt1NiAiJYYC4BfExhi3JJ/OR8OSaVG7IvdOWsY1T89jww97wl2aiMhJUwAEUL9aOV69uSN/v7wVy7/dQa/RaTwzcz1HjupsQEQilwIgoJgYY0CHekwf2Y0ujarx1w9WcfmTn7Pmu13hLk1E5IQoAPKpZqXSPH1dEo8OaMem7Xu56LGZjJr+pZrLiUjEUQCcADPjkjanMmNkN/q2qsWYj7/iosdmsnjTT+EuTUQkMAXASahaLoEx/dvx7PVJ7Nx3mMufmM1f31+p5nIiEhEUAAXggmY1mDYymf4d6vHMrK/pNTqNz9d9H+6yRER+UaAAMLPeZrbGzNaa2T25rG9qZnPM7ICZ3Z1tvLSZzTezJWa2wswezLbuX2a22syWmtnbx+4dHKkqlo7nb5e14rVbOhJjcM3T87h30lJ2qrmciBRTeQZA6H6+Y4E+ZN3nd4CZNc+x2XZgKPBIjvEDwPnu3gZoC/Q2s46hddOBlu7eGvgSuPdEJ1GcnNPwFD4clsytyafz+oJN9EhJZfpKNZcTkeInyBlAB2Ctu69394PABKBf9g3cfZu7LwAO5Rh3d98dWowP/Xho3TR3PxxaNxeoc+LTKF7KJMRyb99mvHNHZ6qUTeCWl9IZ8uoivt99INyliYj8V5AAqA1syracERoLxMxizWwxsA2Y7u7zctnsRuDD4+w/yMzSzSw9MzMz6NMWC63rVGbykC6M7NGEqSu+o0dKKu988a3aSYhIsRAkACyXscCvYO5+xN3bkvUOv4OZtfyfg5vdDxwGXjnO/uPdPcndkxITE4M+bbGREBfD0Asa88HQrpx2SjmGv76Ym15MZ/NP+8JdmohEuSABkAHUzbZcB9ic3ydy95+Az4Dex8bM7HrgIuBaL+Fvi5vUqMBbt3Xi9xc1Z866H+g5Ko3/zN3AUbWTEJEwCRIAC4DGZtbAzBKA/sDkIAc3s8Rjn+4xszJAd2B1aLk38DvgEnffewK1R5zYGOOmLg2YOjyZNnUr8cA7yxnw9Fy+/l7N5USk6FmQN95m1hcYDcQCz7n7Q2Y2GMDdx5lZTSAdqAgcBXaT9Ymh+sCLof1igInu/ufQMdcCpYBjd2Of6+6Df6mOpKQkT09Pz+cUiyd35430DP7ywUoOHj7KyB5NuKlLA+Ji9dUMESlYZrbQ3ZN+Nh5JV15KUgAcs3Xnfh54ZznTV26lVe1K/POK1jQ/tWK4yxKREuR4AaC3m2FWo2Jpxg88k7HXtGfLjn1c8vgs/j1tDQcOq52EiBQuBUAxYGZc2LoW00d045I2p/LYJ2u58NFZLNzwY7hLE5ESTAFQjFQpl0DK1W15/oaz2HvgMFeO+5wH31vB3oOH895ZRCSfFADF0HlnVGfayG4M7Hgaz8/+hp6j0pj1lZrLiUjBUgAUU+VLxfHnfi2ZeOs5xMfG8P+encdv31zCjr1qLiciBUMBUMx1aFCVD4d15bZzG/LWom/pPiqVj5Z/F+6yRKQEUABEgNLxsfyud1Peub0z1cqXYvB/FnLHK4vI3KXmciJy4hQAEaRVnUpMHtKZ3/Q6g+krt9I9JZW3FmaouZyInBAFQISJj43hjvMaMWVYVxpVL89dbyzh188v4Fs1lxORfFIARKhG1cvzxq3n8KeLm7Pgm+30TEnlpTnfqLmciASmAIhgMTHGrztnNZdrf1oV/vDuCq4eP4d1mbvz3llEop4CoASoW7UsL93YgX9d2Zo13+2iz5iZPPHZWg4dORru0kSkGFMAlBBmxlVJdZlxVzfOP6M6D3+0hkvHzmb5tzvCXZqIFFMKgBKmeoXSjBt4Jk9e256tOw/Qb+xs/jV1NfsPqbmciPwvBUAJ1adVLWaMTOaydrUZ++k6+j46k/Rvtoe7LBEpRhQAJVjlsgk8clUbXrqxAwcOHeWqp+bwp8kr2HNAzeVEJGAAmFlvM1tjZmvN7J5c1jc1szlmdsDM7s42XtrM5pvZEjNbYWYPZlt3VWjsqJn97EYFUnCSmyQybUQy159TnxfnZDWXS/0yM9xliUiY5RkAZhYLjAX6kHWbxwFm1jzHZtuBocAjOcYPAOe7exugLdDbzDqG1i0HLgfSTrh6CaxcqTj+dEkL3rj1HErFx3D9c/O5a+ISftp7MNyliUiYBDkD6ACsdff17n4QmAD0y76Bu29z9wXAoRzj7u7HPpQeH/rx0LpV7r7mZCcg+ZNUvypThnZlyHmNeGfxt3RPSePDZVvCXZaIhEGQAKgNbMq2nBEaC8TMYs1sMbANmO7u8/JToJkNMrN0M0vPzNRli4JQOj6Wu3udweQhnalRsRS3vbKIwS8vZNvO/eEuTUSKUJAAsFzGAvcbcPcj7t4WqAN0MLOWQfcN7T/e3ZPcPSkxMTE/u0oeWpxaiXfv6MzvejflkzXb6J6Syhvpm9RcTiRKBAmADKButuU6wOb8PpG7/wR8BvTO775SeOJiY7jt3IZ8OKwrZ9SswG/eXMp1z81n0/a94S5NRApZkABYADQ2swZmlgD0ByYHObiZJZpZ5dDjMkB3YPUJ1iqFqGFieV4fdA5/6deCRRt+pNfoNJ6f/TVH1FxOpMTKMwDc/TAwBJgKrAImuvsKMxtsZoMBzKymmWUAI4EHzCzDzCoCtYBPzWwpWUEy3d3fD+1zWWifc4APzGxqYUxQgouJMQaeU5+pI5I5q35VHnxvJb96ag5rt+0Kd2kiUggskq73JiUleXp6erjLiAruzttffMuf31/J3gNHGHpBI27t1pD4WH13UCTSmNlCd//Z9630v1lyZWZc3r4O00d0o0eLGjwy7UsueVzN5URKEgWA/KLECqUYe017nhp4Jt/vzmou948P1VxOpCRQAEggvVrUZMaIblzZvg7jUtfRd8xM5n+t5nIikUwBIIFVKhvPP69szX9uOpuDR47yq6fm8Pt3lrNr/6G8dxaRYkcBIPnWpXE1po1I5sbODfjPvA30GpXGp2u2hbssEcknBYCckLIJcfzh4ua8ObgT5UrFccPzCxj5+mJ+3KPmciKRQgEgJ+XM06rw/tAuDD2/EZOXbKZ7SirvL92sdhIiEUABICetVFwsI3uewXt3duHUymUY8uoXDHp5IVvVXE6kWFMASIFpVqsib9/eiXv7NCXty0y6p6Ty+oKNOhsQKaYUAFKg4mJjuLVbQz4ankyzWhX53VvLuPaZeWz8Qc3lRIobBYAUigbVyjHhlo48dFlLlmbsoNfoNJ6dpeZyIsWJAkAKTUyMce3ZpzF9ZDLnNDyFv7y/kiue/Jwvt6q5nEhxoACQQlerUhmevT6JMf3bsuGHPVz46Ewe/fgrDh4+Gu7SRKKaAkCKhJnRr21tZozsRu+WtUiZ/iWXPD6LJZt+CndpIlFLASBF6pTypXhsQDuevi6JH/ce5LInZvO3KavYd1DN5USKmgJAwqJH8xpMH9mNq8+qy/i09fQZk8acdT+EuyyRqBIoAMyst5mtMbO1ZnZPLuubmtkcMztgZndnGy9tZvPNbImZrTCzB7Otq2pm083sq9CfVQpmShIpKpaO5++Xt+bVm8/mqMOAp+dy39vL2KnmciJFIs8AMLNYYCzQB2gODDCz5jk22w4MBR7JMX4AON/d2wBtgd5m1jG07h7gY3dvDHwcWpYo1KlRNaYOT+aWrg2YMH8jPVPS+GT11nCXJVLiBTkD6ACsdff17n4QmAD0y76Bu29z9wXAoRzj7u67Q4vxoZ9jHwTvB7wYevwicOkJzUBKhDIJsdx/YXMm3d6ZSmXiufGFdIZN+IIfdh8Id2kiJVaQAKgNbMq2nBEaC8TMYs1sMbCNrJvCzwutquHuWwBCf1Y/zv6DzCzdzNIzMzODPq1EqLZ1K/PenV0Y3r0xU5ZtoceoNN5d/K3aSYgUgiABYLmMBf7f6O5H3L0tUAfoYGYtg+4b2n+8uye5e1JiYmJ+dpUIlRAXw/DuTXj/zq7UrVqWYRMWc/OL6WzZsS/cpYmUKEECIAOom225DrA5v0/k7j8BnwG9Q0NbzawWQOhP3VFE/scZNSsw6bZOPHBhM2av+56eKWm8Om8jR9VOQqRABAmABUBjM2tgZglAf2BykIObWaKZVQ49LgN0B1aHVk8Grg89vh54Nx91S5SIjTFu7no6U4cn07J2Je57exnXPDOXb77fE+7SRCKeBbm2amZ9gdFALPCcuz9kZoMB3H2cmdUE0oGKwFFgN1mfGKpP1i94Y8kKm4nu/ufQMU8BJgL1gI3AVe7+i3cZT0pK8vT09PzPUkoEd+f1BZt46INVHDp6lLt6nMGNXRoQG5PbVUoROcbMFrp70s/GI+mXawoAAfhux34eeGcZM1Zto02dSjx8ZRvOqFkh3GWJFFvHCwB9E1giTs1KpXn6uiQeG9COjB/3cdFjMxk1/UsOHFY7CZH8UABIRDIzLm5zKtNHduPCVrUY8/FXXPzYLL7Y+GO4SxOJGAoAiWhVyyUwun87nvt1Erv2H+byJz/nL++vZO/Bw+EuTaTYUwBIiXB+0xpMG5HMtWfX49lZX9N79Ew+X/t9uMsSKdYUAFJiVCgdz18vbcWEQR2JMbjmmXnc89ZSduxTczmR3CgApMTpePopfDQ8mVu7nc7E9E30HJXK9JVqLieSkwJASqTS8bHc26cZ79zRmSplE7jlpXSGvLqI79VcTuS/FABSorWuU5nJQ7pwV48mTFuxle4pqbz9RYaay4mgAJAokBAXw50XNOaDoV1oUK0cI15fwo0vLGDzT2ouJ9FNASBRo3GNCrw5uBN/uKg5c9dvp+eoNF6eu0HN5SRqKQAkqsTGGDd2acC0Ecm0rVuZ37+znP5Pz+VrNZeTKKQAkKhUt2pZXr6pAw9f0ZpVW3bSe3Qa41LXcfjI0XCXJlJkFAAStcyMX51Vlxkju9GtSSL/+HA1lz4xm5Wbd4a7NJEioQCQqFejYmmeGngmT1zbnu927OeSx2fx72lr1FxOSjwFgAhZZwN9W9Vi+ohuXNL2VB77ZC0XPjqLhRvUXE5KLgWASDZVyiWQ8qu2vHDDWew7eIQrx33Og++tYM8BNZeTkkcBIJKLc8+oztQRyQzseBrPz/6GXqPTmPlVZrjLEilQgQLAzHqb2RozW2tm9+SyvqmZzTGzA2Z2d7bxumb2qZmtMrMVZjYs27o2oX2Wmdl7ZlaxYKYkUjDKl4rjz/1aMvHWc0iIjWHgs/P57ZtL2LFXzeWkZMgzAMwsFhgL9CHrPr8DzKx5js22A0OBR3KMHwbucvdmQEfgjmz7PgPc4+6tgLeB35zwLEQKUYcGVZkyrCu3nduQtxZ9S/dRqXy0/LtwlyVy0oKcAXQA1rr7enc/CEwA+mXfwN23ufsC4FCO8S3uvij0eBewCqgdWn0GkBZ6PB244oRnIVLISsfH8rveTXn3js4kli/F4P8s5PZXFrJt1/5wlyZywoIEQG1gU7blDP7vRTwwM6sPtAPmhYaWA5eEHl8F1D3OfoPMLN3M0jMzdQ1Wwqtl7Uq8O6Qzv+l1BjNWbaNHShpvLVRzOYlMQQLAchnL1792MysPvAUMd/dj37K5kaxLQguBCsDB3PZ19/HunuTuSYmJifl5WpFCER8bwx3nNWLK0K40ql6eu95YwvXPLyDjx73hLk0kX4IEQAb/++68DrA56BOYWTxZL/6vuPukY+Puvtrde7r7mcBrwLqgxxQpDhpVL88bt57Dg5e0IP2b7fQalcZLc75RczmJGEECYAHQ2MwamFkC0B+YHOTgZmbAs8Aqd0/Jsa566M8Y4AFgXH4KFykOYmKM6zvVZ+rwZNqfVoU/vLuCXz01h3WZu8Ndmkie8gwAdz8MDAGmkvVL3InuvsLMBpvZYAAzq2lmGcBI4AEzywh9rLMzMBA438wWh376hg49wMy+BFaTdUbxfIHPTqSI1K1alpdu7MAjV7Xhq2276TNmJmM/XcshNZeTYswi6ZdXSUlJnp6eHu4yRH7Rtl37+dPkFUxZ9h0tTq3IP69oTcvalcJdlkQxM1vo7kk5x/VNYJECVr1CaZ649kzG/b/2bN15gH5jZ/PwR6vZf0jN5aR4UQCIFJLeLWvx8chuXN6uNk98to6+j84k/Zvt4S5L5L8UACKFqFLZeP51VRteurEDBw4d5aqn5vDHd5ezW83lpBhQAIgUgeQmiUwbkcz159Tnpbkb6DUqjdQv9cVGCS8FgEgRKVcqjj9d0oI3B59D6fgYrn9uPiMnLuanvbl+B1Kk0CkARIrYmadV5YOhXRlyXiMmL95M95RUpizbEu6yJAopAETCoHR8LHf3OoN3h3SmZqXS3P7KIga/vJBtO9VcToqOAkAkjFqcWol3bu/M73o35ZM12+ieksrE9E1qLidFQgEgEmZxsTHcdm5DPhrWlaY1K/LbN5dy3XPz2bRdzeWkcCkARIqJ0xPLM2FQR/5yaUsWbfiRnqPSeH721xxRczkpJAoAkWIkJsYY2PE0po3sxtmnV+XB91Zy1bjPWbttV7hLkxJIASBSDNWuXIbnf30Wo65uw/rv99B3zCwe/+QrNZeTAqUAECmmzIzL2tVhxshu9GhRg0emfcnFj81iWcaOcJcmJYQCQKSYq1a+FGOvac9TA89k+56DXPrEbP7xoZrLyclTAIhEiF4tajJ9ZDeubF+Hcanr6DNmJvPW/xDusiSCKQBEIkilMvH888rWvHLz2Rw+epSrx8/lgXeWsWv/oXCXJhEoUACYWW8zW2Nma83snlzWNzWzOWZ2wMzuzjZe18w+NbNVZrbCzIZlW9fWzOaG7hKWbmYdCmZKIiVf50bVmDo8mZu6NOCVeRvpNSqNT1dvC3dZEmHyDAAziwXGAn2A5mTdyrF5js22A0OBR3KMHwbucvdmQEfgjmz7Pgw86O5tgT+ElkUkoLIJcfz+oua8dVsnypWK44YXFjDi9cVs36PmchJMkDOADsBad1/v7geBCUC/7Bu4+zZ3XwAcyjG+xd0XhR7vIuuewrWPrQYqhh5XIuu+wCKST+3rVeH9oV0YekFj3luymR4pqby/dLPaSUieggRAbWBTtuUM/u9FPDAzqw+0A+aFhoYD/zKzTWSdOdx7nP0GhS4RpWdmqn+6SG5KxcUyskcT3ruzC7WrlGHIq18w6OWFbFVzOfkFQQLAchnL11sLMysPvAUMd/edoeHbgBHuXhcYATyb277uPt7dk9w9KTExMT9PKxJ1mtWqyKTbOnFf36akfZlJ95RUJszfqLMByVWQAMgA6mZbrkM+LteYWTxZL/6vuPukbKuuB44tv0HWpSYROUlxsTEMSm7I1OHJNK9VkXsmLePaZ+ax8Qc1l5P/FSQAFgCNzayBmSUA/YHJQQ5uZkbWO/tV7p6SY/VmoFvo8fnAV8FKFpEg6lcrx2u3dORvl7ViacYOeo5O5ZmZ69VcTv7LgpwamllfYDQQCzzn7g+Z2WAAdx9nZjWBdLJ+qXsU2E3WJ4ZaAzOBZaFxgPvcfYqZdQHGAHHAfuB2d1/4S3UkJSV5enp6vicpEu227NjH/W8v55PV22hbtzIPX9maJjUqhLssKSJmttDdk342HknXBhUAIifO3Zm8ZDMPvreSXfsPMeS8xtx2bkMS4vR90JLueAGgv3mRKGFm9Gtbm+kjkunTshajZmQ1l1uy6adwlyZhogAQiTKnlC/FowPa8cx1SezYd4jLnpjNQx+sZN9BNZeLNgoAkSjVvXkNpo1Mpn+Hejw982t6j0ljzjo1l4smCgCRKFaxdDx/u6wVr95yNgADnp7LvZOWsVPN5aKCAkBE6NSwGh8NS2ZQ8um8vmAjPVPS+HjV1nCXJYVMASAiAJRJiOW+vs2YdHtnKpWJ56YX0xn62hf8sPtAuEuTQqIAEJH/0bZuZd67swsjujfhw+Vb6DEqjXcXf6t2EiWQAkBEfiYhLoZh3RvzwdCu1KtalmETFnPzi+ls2bEv3KVJAVIAiMhxNalRgbdu68QDFzZj9rrv6ZGSxivzNnBU7SRKBAWAiPyi2Bjj5q6nM214N1rXqcT9by/nmmfm8s33e8JdmpwkBYCIBFLvlLK8cvPZ/OPyVqz4die9RqcxPm0dh48czXtnKZYUACISmJnRv0M9po/sRtfGifxtymquePJzVn+3M++dpdhRAIhIvtWsVJqnrzuTx69pR8aP+7jo0VmkTP+SA4fVTiKSKABE5ISYGRe1PpUZI7txcZtTefTjr7jo0Vks2vhjuEuTgBQAInJSqpRLYNTVbXn+12ex+8Bhrnjyc/7y/kr2Hjwc7tIkDwoAESkQ5zWtzrQRyVx7dj2enfU1vUanMXvt9+EuS35BoAAws95mtsbM1prZPbmsb2pmc8zsgJndnW28rpl9amarzGyFmQ3Ltu51M1sc+vnGzBYXyIxEJGwqlI7nr5e24vVBHYmLieHaZ+Zxz1tL2bFPzeWKozwDwMxigbFAH7Ju8zjAzJrn2Gw7MBR4JMf4YeAud28GdATuOLavu1/t7m3dvS1ZN42fhIiUCGeffgofDuvKrd1OZ2L6JnqkpDJtxXfhLktyCHIG0AFY6+7r3f0gMAHol30Dd9/m7guAQznGt7j7otDjXcAqoHb2bUI3jv8V8NoJz0JEip3S8bHc26cZ79zRmarlEhj08kLueHURmbvUXK64CBIAtYFN2ZYzyPEiHoSZ1QfaAfNyrOoKbHX3r46z3yAzSzez9MzMzPw+rYiEWes6Wc3l7u7ZhOkrttJjVCpvf5Gh5nLFQJAAsFzG8vU3Z2blybrMM9zdc35jZAC/8O7f3ce7e5K7JyUmJubnaUWkmIiPjWHI+Y2ZMqwLp1crx4jXl3DDCwv49ic1lwunIAGQAdTNtlwH2Bz0CcwsnqwX/1fcfVKOdXHA5cDrQY8nIpGrUfUKvDG4E3+8uDnz1m+nZ0oqL89Vc7lwCRIAC4DGZtbAzBKA/sDkIAcPXd9/Fljl7im5bNIdWO3uGUELFpHIFhtj3NC5AdNGJNOuXhV+/85y+o+fy/rM3eEuLerkGQDufhgYAkwl65e4E919hZkNNrPBAGZW08wygJHAA2aWYWYVgc7AQOD8bB/57Jvt8P3RL39FolLdqmV5+aYOPHxla1Z/t5M+Y2YyLlXN5YqSRdIvYpKSkjw9PT3cZYhIAdu2cz+/f3c5U1dspWXtijx8RRuan1ox3GWVGGa20N2Tco7rm8AiEnbVK5bmqYFJPHlte77bcYBLHp/FI1PXsP+QmssVJgWAiBQbfVrVYsbIZPq1rc3jn67lwkdnsnDD9nCXVWIpAESkWKlcNoF//6oNL97Ygf2HjnLluDn8afIK9hxQc7mCpgAQkWKpW5NEpo5I5rqOp/HC59/Qa3QaM7/Sl0ELkgJARIqt8qXieLBfS94YfA4JcTEMfHY+v3ljCTv2qrlcQVAAiEixd1b9qkwZ2pXbz23IpC++pfuoVD5aviXcZUU8BYCIRITS8bH8tndT3r2jM4nlSzH4P4u47T8L2bZrf7hLi1gKABGJKC1rV+LdIZ35Ta8z+Hj1NnqkpPHmQjWXOxEKABGJOPGxMdxxXiOmDO1K4+rlufuNJVz//AIyftwb7tIiigJARCJWo+rlmXjrOfy5XwsWfrOdnqPSePHzb9RcLiAFgIhEtJgY47pz6jN1RDJJ9avyx8kr+NVTc1i7Tc3l8qIAEJESoU6Vsrx4w1n8+6o2fLVtN33HzGTsp2s5pOZyx6UAEJESw8y44sw6zBjZje7Nq/OvqWvo9/hsln+7I9ylFUsKABEpcRIrlOKJa89k3P9rT+buA/QbO5t/frRazeVyUACISInVu2UtZozoxuXtavPkZ+voO2YmC75Rc7ljFAAiUqJVKhvPv65qw8s3deDgkaNcNW4Of3h3ObvVXC5YAJhZbzNbY2ZrzeyeXNY3NbM5ZnbAzO7ONl7XzD41s1VmtsLMhuXY787QcVeY2cMnPx0Rkdx1bZzI1OHJ3NC5Pi/P3UCvUWl8tmZbuMsKqzwDwMxigbFAH6A5MMDMmufYbDswFHgkx/hh4C53bwZ0BO44tq+ZnQf0A1q7e4tc9hURKVDlSsXxx4tb8ObgTpRJiOXXzy9g5MTF/LjnYLhLC4sgZwAdgLXuvt7dDwITyHrh/i933+buC4BDOca3uPui0ONdZN1TuHZo9W3AP9z9wLFjnNRMREQCOvO0KnwwtAt3nt+IyYs302NUKlOWbYm6dhJBAqA2sCnbcgb/9yIemJnVB9oB80JDTYCuZjbPzFLN7Kz8HlNE5ESViovlrp5nMHlIF2pVKsPtryxi8H8Wsm1n9DSXCxIAlstYvmLSzMoDbwHD3X1naDgOqELWpaHfABPN7GfPZWaDzCzdzNIzM3UzCBEpWM1Prcjbt3finj5N+WxNJt1TUpmYvikqzgaCBEAGUDfbch1gc9AnMLN4sl78X3H3STmOO8mzzAeOAtVy7u/u4909yd2TEhMTgz6tiEhgcbExDO7WkA+HdaVprYr89s2lDHx2Ppu2l+zmckECYAHQ2MwamFkC0B+YHOTgoXf0zwKr3D0lx+p3gPND2zUBEoDvA9YtIlLgTk8sz4RbOvLXS1uyeNNP9ByVxnOzvuZICW0uZ0FOc8ysLzAaiAWec/eHzGwwgLuPM7OaQDpQkax38rvJ+sRQa2AmsCw0DnCfu08JhclzQFvgIHC3u3/yS3UkJSV5enp6fucoIpJvm3/ax31vL+OzNZm0r1eZf17RmsY1KoS7rBNiZgvdPeln45F0nUsBICJFyd15d/FmHnxvBXsOHOHO8xsx+NyGxMdG1ndojxcAkTULEZEiZGZc2q4200d2o2eLGvx7+pdc/NgslmWUjOZyCgARkTxUK1+Kx69pz/iBZ/Lj3oP0GzuLv3+4KuKbyykAREQC6tmiJtNGdOPqs+ryVOp6+oyZydz1P4S7rBOmABARyYdKZeL5++WtefXmszly1Ok/fi73v72MXfsP5b1zMaMAEBE5AZ0aVeOj4V25uUsDXpu/kZ6j0vh0dWR1tFEAiIicoLIJcTxwUXPeuq0T5UvFccMLCxg+4Qu2R0hzOQWAiMhJalevCu8P7cKwCxrz/tIt9EhJ5b0lm4t9OwkFgIhIASgVF8uIHk14f2gX6lQpw52vfcEtLy3kux3Ft7mcAkBEpAA1rVmRSbd35v6+zZi1NpMeKam8Nn9jsTwbUACIiBSw2BjjluTT+WhYMi1qV+TeScu45ul5bPhhT7hL+x8KABGRQlK/Wjlevbkjf7usFcu/3UGv0Wk8M3N9sWkupwAQESlEMTHGNWfXY9rIZDo3rMZfP1jF5U9+zprvdoW7NAWAiEhRqFWpDM9cn8SjA9qxafteLnpsJqNnfMnBw0fz3rmQKABERIqImXFJm1OZMbIbfVvVYvSMr7j4sVks3vRTWOpRAIiIFLGq5RIY078dz16fxI59h7j8idk89MFK9h0s2uZyCgARkTC5oFkNpo1Mpn+Hejw982t6jU7j83VFd2NEBYCISBhVLB3P3y5rxWu3dMQMrnl6HvdOWsbOImguFygAzKy3ma0xs7Vmdk8u65ua2RwzO2Bmd2cbr2tmn5rZKjNbYWbDsq37k5l9a2aLQz99C2ZKIiKR55yGp/DRsGQGJZ/O6ws20iMllRkrtxbqc+YZAGYWC4wF+pB1n98BZtY8x2bbgaHAIznGDwN3uXszoCNwR459R7l729DPlBOdhIhISVAmIZb7+jbj7ds7U6VsAje/lM7Q177gh90HCuX5gpwBdADWuvt6dz8ITAD6Zd/A3be5+wLgUI7xLe6+KPR4F7AKqF0glYuIlFBt6lZm8pAujOzRhA+Xb6F7Sipz1hX8jWeCBEBtYFO25QxO4EXczOoD7YB52YaHmNlSM3vOzKocZ79BZpZuZumZmZn5fVoRkYiUEBfD0Asa88HQrrSsXYn61coW+HMECQDLZSxf32M2s/LAW8Bwd98ZGn4SaAi0BbYA/85tX3cf7+5J7p6UmJiYn6cVEYl4TWpU4OWbzqZWpTIFfuwgAZAB1M22XAfYHPQJzCyerBf/V9x90rFxd9/q7kfc/SjwNFmXmkREpIgECYAFQGMza2BmCUB/YHKQg5uZAc8Cq9w9Jce6WtkWLwOWBytZREQKQlxeG7j7YTMbAkwFYoHn3H2FmQ0OrR9nZjWBdKAicNTMhpP1iaHWwEBgmZktDh3yvtAnfh42s7ZkXU76Bri1AOclIiJ5sOJ4k4LjSUpK8vT09HCXISISUcxsobsn5RzXN4FFRKKUAkBEJEopAEREopQCQEQkSkXUL4HNLBPYcIK7VwOKrs9q8aA5RwfNOTqczJxPc/effZM2ogLgZJhZem6/BS/JNOfooDlHh8KYsy4BiYhEKQWAiEiUiqYAGB/uAsJAc44OmnN0KPA5R83vAERE5H9F0xmAiIhkowAQEYlSJS4AAtzA3szs0dD6pWbWPhx1FqQAc742NNelZva5mbUJR50FKa85Z9vuLDM7YmZXFmV9BS3IfM3sXDNbbGYrzCy1qGssaAH+XVcys/fMbElozjeEo86CFLo74jYzy7U9foG/frl7ifkhq131OuB0IAFYAjTPsU1f4EOy7nTWEZgX7rqLYM6dgCqhx32iYc7ZtvsEmAJcGe66C/nvuDKwEqgXWq4e7rqLYM73Af8MPU4EtgMJ4a79JOedDLQHlh9nfYG+fpW0M4A8b2AfWn7Js8wFKue4OU2kyXPO7v65u/8YWpxL1l3dIlmQv2eAO8m6G922oiyuEASZ7zXAJHffCODu0TBnByqEbjxVnqwAOFy0ZRYsd08jax7HU6CvXyUtAILcwL5AbnJfjOR3PjeR9Q4ikuU5ZzOrTdad5sYVYV2FJcjfcROgipl9ZmYLzey6IquucASZ8+NAM7JuUbsMGOZZt5gtyQr09SvPO4JFmCA3sD/pm9wXM4HnY2bnkRUAXQq1osIXZM6jgd+5+5GsN4gRLch844AzgQuAMsAcM5vr7l8WdnGFJMicewGLgfOBhsB0M5vp7jsLubZwKtDXr5IWAEFuYH9SN7kvhgLNx8xaA88Afdz9hyKqrbAEmXMSMCH04l8N6Gtmh939nSKpsGAF/Xf9vbvvAfaYWRrQBojUAAgy5xuAf3jWxfG1ZvY10BSYXzQlhkWBvn6VtEtAQW5gPxm4LvTb9I7ADnffUtSFFqA852xm9YBJwMAIfkeYXZ5zdvcG7l7f3esDbwK3R+iLPwT7d/0u0NXM4sysLHA2sKqI6yxIQea8kawzHsysBnAGsL5Iqyx6Bfr6VaLOADzADezJ+kRIX2AtsJesdxERK+Cc/wCcAjwRekd82CO4k2LAOZcYQebr7qvM7CNgKXAUeMbdc/0oYSQI+Hf8F+AFM1tG1qWR37l7RLeINrPXgHOBamaWAfwRiIfCef1SKwgRkShV0i4BiYhIQAoAEZEopQAQEYlSCgARkSilABARiVIKABGRKKUAEBGJUv8fobbM57I6/QcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
