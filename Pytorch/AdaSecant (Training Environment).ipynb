{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2135, -0.1278, -0.2495,  0.0526,  0.1513,  0.0271, -0.1223, -0.0470,\n",
      "         -0.1977, -0.2170],\n",
      "        [-0.1285,  0.1133, -0.2761, -0.2720, -0.3155, -0.2889,  0.1507,  0.1169,\n",
      "         -0.2754,  0.1117],\n",
      "        [-0.0658, -0.0388, -0.0051,  0.1379, -0.2626, -0.0795, -0.2263,  0.0952,\n",
      "         -0.3134,  0.0006],\n",
      "        [ 0.1138, -0.1348, -0.0091, -0.1778,  0.3156, -0.1540, -0.1041, -0.1293,\n",
      "         -0.2746, -0.2961],\n",
      "        [-0.1181, -0.0018, -0.1586,  0.2298, -0.1785, -0.0142, -0.1911, -0.2382,\n",
      "         -0.0088, -0.2588],\n",
      "        [-0.2179, -0.1114, -0.2636, -0.2959, -0.2951, -0.2657,  0.2713, -0.2628,\n",
      "         -0.2472, -0.0278],\n",
      "        [ 0.1604, -0.2458, -0.1888, -0.1883,  0.0228, -0.0344, -0.3145,  0.0466,\n",
      "         -0.1247, -0.1604],\n",
      "        [-0.0928,  0.2854,  0.0482, -0.1393, -0.3014, -0.1819,  0.1672,  0.1146,\n",
      "          0.2684,  0.0408],\n",
      "        [-0.0987, -0.0962,  0.2565,  0.2347, -0.2063, -0.1477, -0.2444,  0.1916,\n",
      "         -0.2592, -0.1246],\n",
      "        [ 0.2826,  0.0474, -0.0058,  0.2317, -0.0085,  0.2795,  0.1993, -0.2094,\n",
      "          0.0906, -0.2608]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0079,  0.2068,  0.1249, -0.1387, -0.0540,  0.0813,  0.0450, -0.1043,\n",
      "        -0.3007, -0.1492], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1283, -0.2297,  0.2553,  0.0189, -0.0868,  0.3051,  0.2352,  0.0940,\n",
      "          0.1934, -0.1281],\n",
      "        [-0.0236, -0.3141,  0.2552, -0.1477,  0.0088,  0.1564, -0.2687,  0.1852,\n",
      "          0.1484, -0.0660],\n",
      "        [ 0.2896,  0.2637, -0.2858,  0.0084, -0.1477,  0.1900, -0.2874, -0.0466,\n",
      "         -0.0983,  0.2456],\n",
      "        [-0.2468, -0.2526,  0.0101,  0.1965, -0.0293,  0.0939,  0.3141,  0.1842,\n",
      "          0.0892, -0.1582],\n",
      "        [-0.1185, -0.1063,  0.0921, -0.2800,  0.3143, -0.2450, -0.1179, -0.3036,\n",
      "          0.3062,  0.2348],\n",
      "        [ 0.0109,  0.1430,  0.0806, -0.2241, -0.0909,  0.0778, -0.3074, -0.2218,\n",
      "         -0.2697,  0.0899],\n",
      "        [-0.0467,  0.1041,  0.0891, -0.2127, -0.1771, -0.1934,  0.0464,  0.3155,\n",
      "         -0.2420,  0.2082],\n",
      "        [ 0.1065, -0.0381,  0.0032, -0.0483, -0.2273, -0.0012, -0.0586,  0.1363,\n",
      "          0.1252,  0.1174],\n",
      "        [ 0.1156, -0.0162, -0.1457,  0.0104, -0.0037,  0.2309,  0.0338,  0.2362,\n",
      "         -0.1770, -0.1395],\n",
      "        [ 0.1008, -0.2410,  0.3015,  0.1351,  0.0552,  0.1495,  0.3036,  0.1409,\n",
      "          0.1830,  0.1766]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0306,  0.1345, -0.1794,  0.2223, -0.0007, -0.2419,  0.0968,  0.2592,\n",
      "         0.2844,  0.0618], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        # here we need to introduce some attribute to save params/gradients from old batch.\n",
    "        self.moving_average_of_g = None\n",
    "        self.delta = None\n",
    "        self.memory_size = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                    # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                    if p.grad is not None:\n",
    "                        d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # all parameters of a model seem to be contained within the same group\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "        return #loss\n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # g[i] corresponds to param[i]    \n",
    "    for i, param in enumerate(params):\n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        if i == 0:\n",
    "            print(param)\n",
    "            print(g)\n",
    "            print(g_next)\n",
    "        \n",
    "        correction_term = None # to be implemented\n",
    "        corrected_gradient = None # to be implemented\n",
    "        \n",
    "        \n",
    "    # update all attributes in optimizer\n",
    "    optimizer.gradients = g\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except:\n",
    "                # whatever happens if there is no next gradient\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2135, -0.1278, -0.2495,  0.0526,  0.1513,  0.0271, -0.1223, -0.0470,\n",
      "         -0.1977, -0.2170],\n",
      "        [-0.1285,  0.1133, -0.2761, -0.2720, -0.3155, -0.2889,  0.1507,  0.1169,\n",
      "         -0.2754,  0.1117],\n",
      "        [-0.0658, -0.0388, -0.0051,  0.1379, -0.2626, -0.0795, -0.2263,  0.0952,\n",
      "         -0.3134,  0.0006],\n",
      "        [ 0.1138, -0.1348, -0.0091, -0.1778,  0.3156, -0.1540, -0.1041, -0.1293,\n",
      "         -0.2746, -0.2961],\n",
      "        [-0.1181, -0.0018, -0.1586,  0.2298, -0.1785, -0.0142, -0.1911, -0.2382,\n",
      "         -0.0088, -0.2588],\n",
      "        [-0.2179, -0.1114, -0.2636, -0.2959, -0.2951, -0.2657,  0.2713, -0.2628,\n",
      "         -0.2472, -0.0278],\n",
      "        [ 0.1604, -0.2458, -0.1888, -0.1883,  0.0228, -0.0344, -0.3145,  0.0466,\n",
      "         -0.1247, -0.1604],\n",
      "        [-0.0928,  0.2854,  0.0482, -0.1393, -0.3014, -0.1819,  0.1672,  0.1146,\n",
      "          0.2684,  0.0408],\n",
      "        [-0.0987, -0.0962,  0.2565,  0.2347, -0.2063, -0.1477, -0.2444,  0.1916,\n",
      "         -0.2592, -0.1246],\n",
      "        [ 0.2826,  0.0474, -0.0058,  0.2317, -0.0085,  0.2795,  0.1993, -0.2094,\n",
      "          0.0906, -0.2608]], device='cuda:0', requires_grad=True)\n",
      "tensor([[-0.0066, -0.0053, -0.0028, -0.0068, -0.0078, -0.0117, -0.0105, -0.0074,\n",
      "         -0.0013, -0.0049],\n",
      "        [ 0.0242,  0.0125,  0.0198,  0.0155,  0.0271,  0.0084,  0.0230,  0.0272,\n",
      "          0.0283,  0.0275],\n",
      "        [-0.0203, -0.0141, -0.0248, -0.0130, -0.0208, -0.0160, -0.0293, -0.0271,\n",
      "         -0.0259, -0.0259],\n",
      "        [-0.0188, -0.0213, -0.0220, -0.0294, -0.0309, -0.0159, -0.0221, -0.0277,\n",
      "         -0.0246, -0.0238],\n",
      "        [-0.0034,  0.0054,  0.0016,  0.0011, -0.0008,  0.0023,  0.0074, -0.0024,\n",
      "          0.0019,  0.0029],\n",
      "        [-0.0247, -0.0269, -0.0218, -0.0316, -0.0329, -0.0232, -0.0281, -0.0343,\n",
      "         -0.0287, -0.0313],\n",
      "        [-0.0376, -0.0384, -0.0443, -0.0446, -0.0439, -0.0332, -0.0457, -0.0421,\n",
      "         -0.0441, -0.0462],\n",
      "        [-0.0136, -0.0042, -0.0044, -0.0047, -0.0212, -0.0043, -0.0149, -0.0062,\n",
      "         -0.0145, -0.0170],\n",
      "        [-0.0163, -0.0033, -0.0125, -0.0048, -0.0144, -0.0030, -0.0158, -0.0163,\n",
      "         -0.0149, -0.0128],\n",
      "        [ 0.0181,  0.0193,  0.0128,  0.0194,  0.0178,  0.0092,  0.0150,  0.0186,\n",
      "          0.0229,  0.0255]], device='cuda:0')\n",
      "tensor([[ 0.0024,  0.0007, -0.0024, -0.0037, -0.0009,  0.0120,  0.0027, -0.0047,\n",
      "          0.0011,  0.0086],\n",
      "        [ 0.0246,  0.0292,  0.0216,  0.0219,  0.0206,  0.0239,  0.0216,  0.0222,\n",
      "          0.0319,  0.0342],\n",
      "        [-0.0152, -0.0113, -0.0206, -0.0206, -0.0075, -0.0199, -0.0149, -0.0106,\n",
      "         -0.0159, -0.0301],\n",
      "        [-0.0049, -0.0176, -0.0139, -0.0097, -0.0113, -0.0020, -0.0140, -0.0079,\n",
      "         -0.0120,  0.0045],\n",
      "        [-0.0305, -0.0230, -0.0098, -0.0232, -0.0218, -0.0223, -0.0161, -0.0286,\n",
      "         -0.0202, -0.0401],\n",
      "        [-0.0126, -0.0230, -0.0260, -0.0208, -0.0261, -0.0055, -0.0205, -0.0178,\n",
      "         -0.0272, -0.0020],\n",
      "        [-0.0192, -0.0261, -0.0250, -0.0273, -0.0183, -0.0214, -0.0310, -0.0129,\n",
      "         -0.0229, -0.0150],\n",
      "        [ 0.0201,  0.0045,  0.0052,  0.0074,  0.0094,  0.0043,  0.0009,  0.0126,\n",
      "         -0.0040,  0.0200],\n",
      "        [-0.0316, -0.0285, -0.0150, -0.0174, -0.0138, -0.0140, -0.0122, -0.0251,\n",
      "         -0.0196, -0.0375],\n",
      "        [ 0.0124,  0.0204,  0.0182,  0.0130,  0.0249,  0.0153,  0.0202,  0.0110,\n",
      "          0.0308,  0.0067]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.2135, -0.1278, -0.2495,  0.0526,  0.1513,  0.0271, -0.1223, -0.0470,\n",
      "         -0.1977, -0.2170],\n",
      "        [-0.1285,  0.1133, -0.2761, -0.2720, -0.3155, -0.2889,  0.1507,  0.1169,\n",
      "         -0.2754,  0.1117],\n",
      "        [-0.0658, -0.0388, -0.0051,  0.1379, -0.2626, -0.0795, -0.2263,  0.0952,\n",
      "         -0.3134,  0.0006],\n",
      "        [ 0.1138, -0.1348, -0.0091, -0.1778,  0.3156, -0.1540, -0.1041, -0.1293,\n",
      "         -0.2746, -0.2961],\n",
      "        [-0.1181, -0.0018, -0.1586,  0.2298, -0.1785, -0.0142, -0.1911, -0.2382,\n",
      "         -0.0088, -0.2588],\n",
      "        [-0.2179, -0.1114, -0.2636, -0.2959, -0.2951, -0.2657,  0.2713, -0.2628,\n",
      "         -0.2472, -0.0278],\n",
      "        [ 0.1604, -0.2458, -0.1888, -0.1883,  0.0228, -0.0344, -0.3145,  0.0466,\n",
      "         -0.1247, -0.1604],\n",
      "        [-0.0928,  0.2854,  0.0482, -0.1393, -0.3014, -0.1819,  0.1672,  0.1146,\n",
      "          0.2684,  0.0408],\n",
      "        [-0.0987, -0.0962,  0.2565,  0.2347, -0.2063, -0.1477, -0.2444,  0.1916,\n",
      "         -0.2592, -0.1246],\n",
      "        [ 0.2826,  0.0474, -0.0058,  0.2317, -0.0085,  0.2795,  0.1993, -0.2094,\n",
      "          0.0906, -0.2608]], device='cuda:0', requires_grad=True)\n",
      "tensor([[ 0.0024,  0.0007, -0.0024, -0.0037, -0.0009,  0.0120,  0.0027, -0.0047,\n",
      "          0.0011,  0.0086],\n",
      "        [ 0.0246,  0.0292,  0.0216,  0.0219,  0.0206,  0.0239,  0.0216,  0.0222,\n",
      "          0.0319,  0.0342],\n",
      "        [-0.0152, -0.0113, -0.0206, -0.0206, -0.0075, -0.0199, -0.0149, -0.0106,\n",
      "         -0.0159, -0.0301],\n",
      "        [-0.0049, -0.0176, -0.0139, -0.0097, -0.0113, -0.0020, -0.0140, -0.0079,\n",
      "         -0.0120,  0.0045],\n",
      "        [-0.0305, -0.0230, -0.0098, -0.0232, -0.0218, -0.0223, -0.0161, -0.0286,\n",
      "         -0.0202, -0.0401],\n",
      "        [-0.0126, -0.0230, -0.0260, -0.0208, -0.0261, -0.0055, -0.0205, -0.0178,\n",
      "         -0.0272, -0.0020],\n",
      "        [-0.0192, -0.0261, -0.0250, -0.0273, -0.0183, -0.0214, -0.0310, -0.0129,\n",
      "         -0.0229, -0.0150],\n",
      "        [ 0.0201,  0.0045,  0.0052,  0.0074,  0.0094,  0.0043,  0.0009,  0.0126,\n",
      "         -0.0040,  0.0200],\n",
      "        [-0.0316, -0.0285, -0.0150, -0.0174, -0.0138, -0.0140, -0.0122, -0.0251,\n",
      "         -0.0196, -0.0375],\n",
      "        [ 0.0124,  0.0204,  0.0182,  0.0130,  0.0249,  0.0153,  0.0202,  0.0110,\n",
      "          0.0308,  0.0067]], device='cuda:0')\n",
      "tensor([[-0.0153, -0.0170, -0.0204, -0.0248, -0.0117, -0.0139, -0.0228, -0.0201,\n",
      "         -0.0198, -0.0173],\n",
      "        [ 0.0166,  0.0223,  0.0080,  0.0180,  0.0228,  0.0112,  0.0147,  0.0215,\n",
      "          0.0225,  0.0183],\n",
      "        [-0.0132, -0.0333, -0.0109, -0.0212, -0.0266, -0.0109, -0.0155, -0.0271,\n",
      "         -0.0210, -0.0099],\n",
      "        [-0.0368, -0.0298, -0.0235, -0.0333, -0.0301, -0.0298, -0.0371, -0.0338,\n",
      "         -0.0333, -0.0304],\n",
      "        [-0.0025, -0.0046,  0.0011,  0.0015, -0.0011,  0.0009, -0.0021,  0.0016,\n",
      "          0.0014, -0.0089],\n",
      "        [-0.0316, -0.0202, -0.0261, -0.0388, -0.0312, -0.0267, -0.0358, -0.0364,\n",
      "         -0.0393, -0.0341],\n",
      "        [-0.0479, -0.0524, -0.0183, -0.0299, -0.0257, -0.0276, -0.0391, -0.0396,\n",
      "         -0.0356, -0.0194],\n",
      "        [-0.0026, -0.0117, -0.0089, -0.0029, -0.0106,  0.0020, -0.0063, -0.0119,\n",
      "         -0.0101, -0.0006],\n",
      "        [-0.0196, -0.0227, -0.0060, -0.0203, -0.0119, -0.0145, -0.0138, -0.0171,\n",
      "         -0.0188, -0.0185],\n",
      "        [ 0.0047, -0.0102, -0.0046, -0.0006,  0.0003, -0.0012, -0.0015,  0.0010,\n",
      "          0.0089,  0.0032]], device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[ 0.2135, -0.1278, -0.2495,  0.0526,  0.1513,  0.0271, -0.1223, -0.0470,\n",
      "         -0.1977, -0.2170],\n",
      "        [-0.1285,  0.1133, -0.2761, -0.2720, -0.3155, -0.2889,  0.1507,  0.1169,\n",
      "         -0.2754,  0.1117],\n",
      "        [-0.0658, -0.0388, -0.0051,  0.1379, -0.2626, -0.0795, -0.2263,  0.0952,\n",
      "         -0.3134,  0.0006],\n",
      "        [ 0.1138, -0.1348, -0.0091, -0.1778,  0.3156, -0.1540, -0.1041, -0.1293,\n",
      "         -0.2746, -0.2961],\n",
      "        [-0.1181, -0.0018, -0.1586,  0.2298, -0.1785, -0.0142, -0.1911, -0.2382,\n",
      "         -0.0088, -0.2588],\n",
      "        [-0.2179, -0.1114, -0.2636, -0.2959, -0.2951, -0.2657,  0.2713, -0.2628,\n",
      "         -0.2472, -0.0278],\n",
      "        [ 0.1604, -0.2458, -0.1888, -0.1883,  0.0228, -0.0344, -0.3145,  0.0466,\n",
      "         -0.1247, -0.1604],\n",
      "        [-0.0928,  0.2854,  0.0482, -0.1393, -0.3014, -0.1819,  0.1672,  0.1146,\n",
      "          0.2684,  0.0408],\n",
      "        [-0.0987, -0.0962,  0.2565,  0.2347, -0.2063, -0.1477, -0.2444,  0.1916,\n",
      "         -0.2592, -0.1246],\n",
      "        [ 0.2826,  0.0474, -0.0058,  0.2317, -0.0085,  0.2795,  0.1993, -0.2094,\n",
      "          0.0906, -0.2608]], device='cuda:0', requires_grad=True)\n",
      "tensor([[-0.0153, -0.0170, -0.0204, -0.0248, -0.0117, -0.0139, -0.0228, -0.0201,\n",
      "         -0.0198, -0.0173],\n",
      "        [ 0.0166,  0.0223,  0.0080,  0.0180,  0.0228,  0.0112,  0.0147,  0.0215,\n",
      "          0.0225,  0.0183],\n",
      "        [-0.0132, -0.0333, -0.0109, -0.0212, -0.0266, -0.0109, -0.0155, -0.0271,\n",
      "         -0.0210, -0.0099],\n",
      "        [-0.0368, -0.0298, -0.0235, -0.0333, -0.0301, -0.0298, -0.0371, -0.0338,\n",
      "         -0.0333, -0.0304],\n",
      "        [-0.0025, -0.0046,  0.0011,  0.0015, -0.0011,  0.0009, -0.0021,  0.0016,\n",
      "          0.0014, -0.0089],\n",
      "        [-0.0316, -0.0202, -0.0261, -0.0388, -0.0312, -0.0267, -0.0358, -0.0364,\n",
      "         -0.0393, -0.0341],\n",
      "        [-0.0479, -0.0524, -0.0183, -0.0299, -0.0257, -0.0276, -0.0391, -0.0396,\n",
      "         -0.0356, -0.0194],\n",
      "        [-0.0026, -0.0117, -0.0089, -0.0029, -0.0106,  0.0020, -0.0063, -0.0119,\n",
      "         -0.0101, -0.0006],\n",
      "        [-0.0196, -0.0227, -0.0060, -0.0203, -0.0119, -0.0145, -0.0138, -0.0171,\n",
      "         -0.0188, -0.0185],\n",
      "        [ 0.0047, -0.0102, -0.0046, -0.0006,  0.0003, -0.0012, -0.0015,  0.0010,\n",
      "          0.0089,  0.0032]], device='cuda:0')\n",
      "tensor([[-1.0704e-02, -9.8529e-03, -7.3038e-03, -7.6136e-03, -1.2496e-02,\n",
      "         -2.7260e-03,  2.7466e-03, -3.3658e-03,  2.5851e-05,  4.4362e-03],\n",
      "        [ 1.0261e-02,  5.8279e-03,  7.4446e-03,  4.1086e-03,  1.0599e-02,\n",
      "          9.3221e-03,  1.0809e-02,  1.0430e-03,  1.4887e-02,  2.0042e-02],\n",
      "        [-9.7336e-03,  1.4957e-03, -3.3469e-03, -2.5329e-03, -6.8349e-03,\n",
      "         -4.9753e-03, -7.4734e-03, -4.1622e-03, -5.8462e-03, -1.0021e-02],\n",
      "        [-1.2928e-02, -1.7334e-02, -1.8946e-02, -1.4251e-02, -7.1933e-03,\n",
      "         -1.4772e-02, -5.6979e-03, -5.9657e-03, -1.5155e-02, -1.1954e-02],\n",
      "        [-5.4476e-03, -1.7526e-03,  2.1209e-03,  3.3145e-03, -5.6635e-03,\n",
      "         -6.5148e-03, -5.8708e-03, -3.6023e-03, -2.2713e-03, -1.1177e-02],\n",
      "        [-2.1937e-02, -2.6529e-02, -3.1918e-02, -1.8621e-02, -2.9620e-02,\n",
      "         -2.0072e-02, -1.5735e-02, -1.5141e-02, -1.8672e-02, -1.5220e-02],\n",
      "        [-2.3594e-02, -1.1174e-02, -1.7151e-02, -1.7168e-02, -1.5247e-02,\n",
      "         -2.1613e-02, -1.4622e-02, -3.8439e-03, -2.0957e-02, -1.8602e-02],\n",
      "        [ 1.9872e-02,  1.5386e-02,  3.2742e-03,  9.4531e-03,  1.4797e-02,\n",
      "          1.7285e-02,  1.6010e-02,  2.4805e-02,  1.4096e-02,  1.1780e-02],\n",
      "        [-2.1561e-02, -9.3141e-03,  3.3686e-04, -8.4892e-03, -1.2440e-02,\n",
      "         -1.3072e-02, -8.5102e-03, -4.9488e-03, -2.1269e-02, -1.9921e-02],\n",
      "        [ 1.0629e-02,  1.3906e-02,  2.0018e-02,  9.3032e-03,  2.3996e-02,\n",
      "          1.3778e-02,  2.0095e-02,  6.9464e-03,  1.7467e-02,  1.7524e-02]],\n",
      "       device='cuda:0')\n",
      "Epoch 1/1 - Loss: 2.391292691230774\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_opt = AdaSecant\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24a97a05dc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24a97b11f70>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10546875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
