{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p.grad))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p.grad))\n",
    "                    self.taus.append(torch.ones_like(p.grad))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # TODO later: build in normalization of gradients\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = 0 for first iteration\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        if torch.is_nonzero(optimizer.gamma_denomenators[i].sum()):\n",
    "            # normal calculation of variance reduction term gamma\n",
    "            gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        else:\n",
    "            # gradient is equal to expected gradient in first epoch\n",
    "            # gamma is therefore zero (equivalent to no variance reduction)\n",
    "            gamma = torch.zeros_like(optimizer.gamma_denomenators[i])\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            delta = copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        \n",
    "        # should I update moving averages for g, gamma, and alpha as well? -> memory size will be set again later\n",
    "        \n",
    "        lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "             - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        \n",
    "        optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / optimizer.mean_delta_squares[i])\n",
    "                             * optimizer.taus[i] + 1)\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model):\n",
    "    for p in model.parameters():\n",
    "        p.grad = torch.zeros_like(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    prepare_model(model)\n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Epoch 1/1 - Loss: nan\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b3342d7190>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b35198c8b0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOHUlEQVR4nO3c34tc533H8fenUkQJSbFdybYsyV011UXVUogYhCG9CPUPJMVYvuiFDYmFcyEMNTi0wVXqf8CJoTGmxkakBpm4mEASIoyCYru5VeqVY8uoiuONSKqNFHuTCyfgCyHy7cUetevNSDu7Z1a76+f9gmHmnPOcmedhwG/NmVmnqpAkteuPVnoCkqSVZQgkqXGGQJIaZwgkqXGGQJIat36lJ7AUGzdurImJiZWehiStKSdPnvx1VW2av39NhmBiYoLJycmVnoYkrSlJfjFsv5eGJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxYwlBkj1J3k4yleTQkONJ8lR3/FSSXfOOr0vy4yQvjWM+kqTR9Q5BknXA08BeYCdwf5Kd84btBXZ0t4PAM/OOPwKc6TsXSdLijeMTwW5gqqrOVtVF4EVg/7wx+4Hna9YJ4LokmwGSbAU+B3xjDHORJC3SOEKwBTg3Z3u62zfqmCeBR4HfX+1FkhxMMplkcmZmpteEJUn/bxwhyJB9NcqYJHcD71XVyYVepKoOV9WgqgabNm1ayjwlSUOMIwTTwLY521uB8yOO+QxwT5KfM3tJ6e+SfHMMc5IkjWgcIXgN2JFke5INwH3A0XljjgIPdL8eug14v6ouVNVXqmprVU105/1nVX1+DHOSJI1ofd8nqKpLSR4GjgPrgOeq6nSSh7rjzwLHgH3AFPAB8GDf15UkjUeq5l/OX/0Gg0FNTk6u9DQkaU1JcrKqBvP3+5fFktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjRtLCJLsSfJ2kqkkh4YcT5KnuuOnkuzq9m9L8sMkZ5KcTvLIOOYjSRpd7xAkWQc8DewFdgL3J9k5b9heYEd3Owg80+2/BPxTVf0lcBvwD0POlSQto3F8ItgNTFXV2aq6CLwI7J83Zj/wfM06AVyXZHNVXaiq1wGq6nfAGWDLGOYkSRrROEKwBTg3Z3uaP/yP+YJjkkwAnwZ+NIY5SZJGNI4QZMi+WsyYJJ8Avg18qap+O/RFkoNJJpNMzszMLHmykqQPG0cIpoFtc7a3AudHHZPkY8xG4IWq+s6VXqSqDlfVoKoGmzZtGsO0JUkwnhC8BuxIsj3JBuA+4Oi8MUeBB7pfD90GvF9VF5IE+HfgTFX96xjmIklapPV9n6CqLiV5GDgOrAOeq6rTSR7qjj8LHAP2AVPAB8CD3emfAb4AvJXkjW7fv1TVsb7zkiSNJlXzL+evfoPBoCYnJ1d6GpK0piQ5WVWD+fv9y2JJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJatxYQpBkT5K3k0wlOTTkeJI81R0/lWTXqOdKkpZX7xAkWQc8DewFdgL3J9k5b9heYEd3Owg8s4hzJUnLaByfCHYDU1V1tqouAi8C++eN2Q88X7NOANcl2TziuZKkZTSOEGwBzs3Znu72jTJmlHMBSHIwyWSSyZmZmd6TliTNGkcIMmRfjThmlHNnd1YdrqpBVQ02bdq0yClKkq5k/RieYxrYNmd7K3B+xDEbRjhXkrSMxvGJ4DVgR5LtSTYA9wFH5405CjzQ/XroNuD9qrow4rmSpGXU+xNBVV1K8jBwHFgHPFdVp5M81B1/FjgG7AOmgA+AB692bt85SZJGl6qhl+RXtcFgUJOTkys9DUlaU5KcrKrB/P3+ZbEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjeoUgyQ1JXk7yTnd//RXG7UnydpKpJIfm7H8iyU+SnEry3STX9ZmPJGnx+n4iOAS8WlU7gFe77Q9Jsg54GtgL7ATuT7KzO/wy8NdV9TfAT4Gv9JyPJGmR+oZgP3Cke3wEuHfImN3AVFWdraqLwIvdeVTVD6rqUjfuBLC153wkSYvUNwQ3VdUFgO7+xiFjtgDn5mxPd/vm+yLw/Z7zkSQt0vqFBiR5Bbh5yKHHRnyNDNlX817jMeAS8MJV5nEQOAhw6623jvjSkqSFLBiCqrrjSseSvJtkc1VdSLIZeG/IsGlg25ztrcD5Oc9xALgbuL2qiiuoqsPAYYDBYHDFcZKkxel7aegocKB7fAD43pAxrwE7kmxPsgG4rzuPJHuAfwbuqaoPes5FkrQEfUPwOHBnkneAO7ttktyS5BhA92Xww8Bx4Azwrao63Z3/b8AngZeTvJHk2Z7zkSQt0oKXhq6mqn4D3D5k/3lg35ztY8CxIeP+os/rS5L68y+LJalxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxvUKQ5IYkLyd5p7u//grj9iR5O8lUkkNDjn85SSXZ2Gc+kqTF6/uJ4BDwalXtAF7ttj8kyTrgaWAvsBO4P8nOOce3AXcC/9NzLpKkJegbgv3Ake7xEeDeIWN2A1NVdbaqLgIvdudd9nXgUaB6zkWStAR9Q3BTVV0A6O5vHDJmC3BuzvZ0t48k9wC/rKo3F3qhJAeTTCaZnJmZ6TltSdJl6xcakOQV4OYhhx4b8TUyZF8l+Xj3HHeN8iRVdRg4DDAYDPz0IEljsmAIquqOKx1L8m6SzVV1Iclm4L0hw6aBbXO2twLngU8B24E3k1ze/3qS3VX1q0WsQZLUQ99LQ0eBA93jA8D3hox5DdiRZHuSDcB9wNGqequqbqyqiaqaYDYYu4yAJF1bfUPwOHBnkneY/eXP4wBJbklyDKCqLgEPA8eBM8C3qup0z9eVJI3JgpeGrqaqfgPcPmT/eWDfnO1jwLEFnmuiz1wkSUvjXxZLUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1LlW10nNYtCQzwC9Weh5LsBH49UpP4hpqbb3gmluxVtf8Z1W1af7ONRmCtSrJZFUNVnoe10pr6wXX3IqP2pq9NCRJjTMEktQ4Q3BtHV7pCVxjra0XXHMrPlJr9jsCSWqcnwgkqXGGQJIaZwjGKMkNSV5O8k53f/0Vxu1J8naSqSSHhhz/cpJKsnH5Z91P3zUneSLJT5KcSvLdJNdds8kv0gjvW5I81R0/lWTXqOeuVktdc5JtSX6Y5EyS00keufazX5o+73N3fF2SHyd56drNuqeq8jamG/A14FD3+BDw1SFj1gE/A/4c2AC8Ceycc3wbcJzZP5jbuNJrWu41A3cB67vHXx12/mq4LfS+dWP2Ad8HAtwG/GjUc1fjreeaNwO7usefBH76UV/znOP/CPwH8NJKr2fUm58Ixms/cKR7fAS4d8iY3cBUVZ2tqovAi915l30deBRYK9/i91pzVf2gqi51404AW5d3uku20PtGt/18zToBXJdk84jnrkZLXnNVXaiq1wGq6nfAGWDLtZz8EvV5n0myFfgc8I1rOem+DMF43VRVFwC6+xuHjNkCnJuzPd3tI8k9wC+r6s3lnugY9VrzPF9k9l9aq9Eoa7jSmFHXv9r0WfP/STIBfBr40finOHZ91/wks/+Q+/0yzW9ZrF/pCaw1SV4Bbh5y6LFRn2LIvkry8e457lrq3JbLcq153ms8BlwCXljc7K6ZBddwlTGjnLsa9Vnz7MHkE8C3gS9V1W/HOLflsuQ1J7kbeK+qTib57LgntpwMwSJV1R1XOpbk3csfi7uPiu8NGTbN7PcAl20FzgOfArYDbya5vP/1JLur6ldjW8ASLOOaLz/HAeBu4PbqLrKuQlddwwJjNoxw7mrUZ80k+RizEXihqr6zjPMcpz5r/nvgniT7gD8G/iTJN6vq88s43/FY6S8pPko34Ak+/MXp14aMWQ+cZfY/+pe/jPqrIeN+ztr4srjXmoE9wH8Dm1Z6LQusc8H3jdlrw3O/RPyvxbznq+3Wc80BngeeXOl1XKs1zxvzWdbQl8UrPoGP0g34U+BV4J3u/oZu/y3AsTnj9jH7K4qfAY9d4bnWSgh6rRmYYvZ66xvd7dmVXtNV1voHawAeAh7qHgd4ujv+FjBYzHu+Gm9LXTPwt8xeUjk1573dt9LrWe73ec5zrKkQ+L+YkKTG+ashSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWrc/wLouA/ZRwywxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1171875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
