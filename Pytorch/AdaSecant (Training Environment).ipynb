{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # TODO later: build in normalization of gradients\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = g for first iteration ==> lr = 0\n",
    "            alpha = copy.deepcopy(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        if torch.is_nonzero(optimizer.gamma_denomenators[i].sum()):\n",
    "            # normal calculation of variance reduction term gamma\n",
    "            gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        else:\n",
    "            # gradient is equal to expected gradient in first epoch\n",
    "            # gamma is therefore zero (equivalent to no variance reduction)\n",
    "            gamma = torch.zeros_like(optimizer.gamma_denomenators[i])\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            delta = copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        \n",
    "        # should I update moving averages for g, gamma, and alpha as well? -> memory size will be set again later\n",
    "        \n",
    "        lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "             - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        print('corrected_gradient', corrected_gradient)\n",
    "        print('lr', lr)\n",
    "        \n",
    "        optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / optimizer.mean_delta_squares[i])\n",
    "                             * optimizer.taus[i] + 1)\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        print('params', params[i])\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    #prepare_model(model)\n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "corrected_gradient tensor([[-0.0163, -0.0114, -0.0059, -0.0156, -0.0313, -0.0329, -0.0217, -0.0150,\n",
      "         -0.0235, -0.0143],\n",
      "        [-0.0234, -0.0236, -0.0077, -0.0068, -0.0367, -0.0355, -0.0261, -0.0192,\n",
      "         -0.0288, -0.0171],\n",
      "        [ 0.0032, -0.0100, -0.0155, -0.0111, -0.0084, -0.0097, -0.0045, -0.0147,\n",
      "         -0.0084, -0.0035],\n",
      "        [ 0.0191,  0.0199,  0.0182,  0.0183,  0.0114,  0.0138,  0.0139,  0.0164,\n",
      "          0.0118,  0.0172],\n",
      "        [-0.0244, -0.0304, -0.0235, -0.0320, -0.0449, -0.0399, -0.0314, -0.0295,\n",
      "         -0.0348, -0.0350],\n",
      "        [ 0.0163,  0.0098,  0.0140,  0.0210,  0.0225,  0.0187,  0.0188,  0.0132,\n",
      "          0.0201,  0.0189],\n",
      "        [ 0.0096,  0.0139,  0.0112,  0.0065, -0.0020, -0.0059,  0.0054,  0.0118,\n",
      "          0.0007, -0.0017],\n",
      "        [-0.0065, -0.0080, -0.0047, -0.0027,  0.0020,  0.0013, -0.0001, -0.0013,\n",
      "         -0.0005, -0.0087],\n",
      "        [-0.0077, -0.0049, -0.0110, -0.0152,  0.0027,  0.0037, -0.0055, -0.0040,\n",
      "         -0.0004, -0.0185],\n",
      "        [ 0.0120,  0.0128, -0.0040, -0.0028,  0.0057,  0.0099, -0.0004, -0.0005,\n",
      "          0.0040,  0.0052]], device='cuda:0')\n",
      "lr tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([[-0.1604, -0.0025, -0.1712, -0.1469, -0.2370,  0.2053, -0.2231, -0.2797,\n",
      "          0.0135,  0.2540],\n",
      "        [-0.0920,  0.0239, -0.1854,  0.1664,  0.0934, -0.0345, -0.2095,  0.2768,\n",
      "         -0.2310,  0.1067],\n",
      "        [-0.1337, -0.1303,  0.1108,  0.0759, -0.2875,  0.0259,  0.2865, -0.1089,\n",
      "         -0.2641, -0.0108],\n",
      "        [ 0.1562,  0.2992, -0.0070, -0.2879,  0.2732,  0.1908,  0.1413,  0.0276,\n",
      "          0.0576,  0.1038],\n",
      "        [ 0.0772,  0.2060,  0.1129, -0.0729, -0.1727, -0.1858, -0.0870,  0.1054,\n",
      "          0.1943,  0.0124],\n",
      "        [-0.1374,  0.2812, -0.0667, -0.0272, -0.0910, -0.0963, -0.0117,  0.2407,\n",
      "         -0.1927,  0.1031],\n",
      "        [-0.0451,  0.0440,  0.0661,  0.0202, -0.1506,  0.0771,  0.2764,  0.0597,\n",
      "         -0.0225,  0.1798],\n",
      "        [-0.0235,  0.2559,  0.0393, -0.0805,  0.0292,  0.1584,  0.2967,  0.3149,\n",
      "         -0.1553, -0.1594],\n",
      "        [ 0.0790,  0.0303, -0.1585,  0.2075, -0.2331, -0.2784, -0.2882,  0.2282,\n",
      "         -0.1492,  0.0819],\n",
      "        [ 0.1008,  0.1049, -0.2381,  0.1829,  0.1500, -0.2936,  0.1626,  0.2951,\n",
      "          0.1624,  0.1440]], device='cuda:0', requires_grad=True)\n",
      "1\n",
      "corrected_gradient tensor([-0.0449, -0.0469, -0.0243,  0.0301, -0.0673,  0.0356,  0.0079, -0.0009,\n",
      "        -0.0024,  0.0088], device='cuda:0')\n",
      "lr tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.2643, -0.2802, -0.2494, -0.2168, -0.1121,  0.1552,  0.0994, -0.1165,\n",
      "         0.0650, -0.2289], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "corrected_gradient tensor([[-1.0263e-02, -2.8388e-02, -4.4451e-02,  2.5853e-02, -3.3291e-03,\n",
      "          1.5711e-02,  3.2585e-02,  2.2401e-02, -1.8714e-02,  1.2162e-02],\n",
      "        [ 1.5407e-02,  1.2934e-02,  1.6087e-02, -1.4563e-02, -9.0011e-03,\n",
      "         -7.7620e-03, -1.1373e-02, -1.3798e-02,  1.0714e-04, -9.0963e-03],\n",
      "        [-6.2585e-03, -2.2428e-02, -3.9655e-02,  2.6889e-02, -3.5635e-03,\n",
      "          9.4573e-03,  2.4694e-02,  1.4283e-02, -1.3305e-02,  2.0262e-02],\n",
      "        [ 4.8083e-03, -1.3216e-03, -6.4448e-03,  7.7566e-03, -3.4299e-03,\n",
      "          4.9012e-04,  2.2859e-03,  1.1859e-03, -1.0958e-03,  9.6627e-04],\n",
      "        [-1.7910e-03,  1.7209e-03,  1.3134e-02, -4.4256e-03, -1.9285e-03,\n",
      "         -1.1439e-02, -1.1387e-02, -3.1672e-03, -3.9746e-03, -5.3090e-03],\n",
      "        [-3.6677e-03,  1.9315e-03,  6.8257e-03, -1.2975e-02,  1.6619e-03,\n",
      "         -3.5229e-03, -4.6730e-03, -7.2579e-03,  5.6104e-03, -5.2568e-03],\n",
      "        [-9.5674e-03,  1.3920e-02,  2.1302e-02, -1.2222e-02,  5.7978e-03,\n",
      "          1.7336e-03, -3.8850e-03, -2.3284e-04,  1.4438e-02,  7.8360e-03],\n",
      "        [ 1.0674e-02,  2.3471e-02,  4.1094e-02, -2.3451e-02,  9.5958e-03,\n",
      "         -1.0948e-02, -2.0662e-02, -2.3931e-02,  2.2643e-02, -9.0538e-03],\n",
      "        [-3.5711e-03, -7.8787e-03, -1.8565e-02,  4.5656e-03,  4.2749e-03,\n",
      "          1.6115e-02,  9.0758e-03,  1.4697e-02,  3.3459e-03,  1.0232e-03],\n",
      "        [ 4.2296e-03,  6.0389e-03,  1.0674e-02,  2.5730e-03, -7.8249e-05,\n",
      "         -9.8356e-03, -1.6659e-02, -4.1802e-03, -9.0538e-03, -1.3534e-02]],\n",
      "       device='cuda:0')\n",
      "lr tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([[-2.0448e-01, -2.2222e-01, -2.5473e-01,  2.0723e-01, -2.4396e-01,\n",
      "          8.4593e-02, -7.8828e-02,  5.5806e-02, -1.7660e-01, -2.2057e-01],\n",
      "        [-1.9488e-01, -2.2160e-02, -1.5829e-01, -2.4292e-01, -1.3938e-01,\n",
      "         -1.7191e-02, -3.0328e-01,  2.2207e-01,  1.2149e-01, -1.7484e-01],\n",
      "        [-3.8739e-02, -3.0334e-01,  1.6032e-01, -1.5036e-01, -2.5044e-01,\n",
      "          9.8126e-02,  5.5374e-02,  7.1658e-03,  5.9453e-02,  1.3002e-01],\n",
      "        [-3.0981e-01, -7.8071e-02,  2.1031e-01, -1.0307e-01,  8.3762e-02,\n",
      "         -1.9463e-01, -2.9404e-01,  2.2195e-01,  1.8604e-01,  1.7275e-01],\n",
      "        [-2.7566e-01, -2.4057e-01,  2.4851e-02, -1.0651e-01,  2.6444e-01,\n",
      "          4.7562e-03, -2.6218e-02,  2.8062e-01,  2.9408e-01, -1.9365e-01],\n",
      "        [-9.1578e-02,  2.8009e-01, -8.9052e-02, -2.0165e-01, -6.9183e-04,\n",
      "          1.2092e-01, -5.8952e-02,  2.6977e-01,  1.4881e-01, -2.1299e-01],\n",
      "        [ 2.5619e-01,  1.9318e-01,  6.0165e-02, -1.3245e-01,  1.5001e-01,\n",
      "         -2.4926e-01,  1.8781e-01,  1.0667e-01, -1.3153e-01, -2.7736e-01],\n",
      "        [ 3.0890e-01,  1.3785e-01,  3.8996e-02,  1.1004e-02,  3.0768e-01,\n",
      "         -1.9191e-01, -2.1338e-05, -1.2286e-01, -1.7020e-01,  2.6177e-02],\n",
      "        [ 1.2299e-01, -2.1906e-02, -2.5826e-01, -4.9090e-02,  1.6650e-01,\n",
      "         -2.9074e-01,  2.3593e-01,  7.5472e-02,  2.8459e-01,  9.1829e-02],\n",
      "        [ 1.7365e-01, -2.2502e-01,  5.9063e-02, -2.4086e-01,  3.6758e-02,\n",
      "         -2.9722e-01, -1.2085e-04,  4.3236e-02,  2.1932e-01,  1.0735e-01]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "3\n",
      "corrected_gradient tensor([ 0.0938, -0.0297,  0.0713,  0.0086, -0.0285, -0.0063, -0.0327, -0.0795,\n",
      "         0.0419, -0.0388], device='cuda:0')\n",
      "lr tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.1027, -0.2131,  0.2177,  0.1322, -0.1113, -0.2857, -0.1777,  0.0016,\n",
      "         0.1436, -0.0887], device='cuda:0', requires_grad=True)\n",
      "enter adasecant\n",
      "0\n",
      "corrected_gradient tensor([[ 2.4832e-03, -5.0278e-03,  2.2565e-03,  6.3196e-05, -5.1226e-03,\n",
      "          7.3533e-03,  8.2865e-03,  1.3682e-03,  5.9803e-03,  2.5901e-03],\n",
      "        [ 7.0923e-03, -6.8639e-03, -6.1168e-04,  2.7459e-03, -7.8023e-03,\n",
      "          1.5718e-03, -4.4916e-03, -5.4190e-03, -7.0897e-03, -2.1062e-03],\n",
      "        [ 1.6262e-02,  1.3227e-02,  1.9194e-02,  1.3954e-02,  1.1754e-03,\n",
      "          1.8174e-02,  2.4910e-02,  1.1993e-02,  1.9044e-02,  1.1275e-02],\n",
      "        [ 6.0107e-03,  1.9770e-02,  1.1362e-02,  6.5671e-03,  1.1231e-02,\n",
      "          9.3588e-03,  1.6203e-03,  1.4124e-02,  1.1952e-02,  9.8506e-03],\n",
      "        [ 7.7311e-03,  3.9697e-03, -1.4390e-03,  8.5752e-03, -1.5450e-02,\n",
      "          1.8139e-02, -7.6753e-03,  2.4103e-03,  1.1600e-03, -7.6462e-03],\n",
      "        [-4.3567e-05,  3.3292e-03,  2.3957e-03,  3.6902e-04,  6.6852e-03,\n",
      "         -4.2468e-03, -1.3144e-03,  2.8968e-03, -2.8146e-03,  8.6932e-04],\n",
      "        [-4.6763e-03, -9.2237e-03, -2.8517e-03, -3.7472e-03, -7.9466e-03,\n",
      "          1.0020e-02, -6.0155e-03, -5.0758e-03, -1.1792e-03, -1.7384e-03],\n",
      "        [-8.7251e-03, -1.3609e-02, -1.2429e-02, -6.7754e-03, -1.1192e-02,\n",
      "         -9.5544e-03, -1.3271e-02, -1.4493e-02, -1.7817e-02, -1.4717e-02],\n",
      "        [-5.3279e-03, -1.8536e-02, -1.7436e-02, -6.4518e-03, -8.0396e-03,\n",
      "         -4.8489e-03, -1.9235e-02, -1.3659e-02, -1.0971e-02, -1.0194e-02],\n",
      "        [ 2.1738e-02,  1.6081e-02,  1.6972e-02,  1.4137e-02,  1.9678e-02,\n",
      "          1.7537e-02,  1.4975e-02,  1.8518e-02,  3.0651e-02,  2.7320e-02]],\n",
      "       device='cuda:0')\n",
      "lr tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([[-0.1604, -0.0025, -0.1712, -0.1469, -0.2370,  0.2053, -0.2231, -0.2797,\n",
      "          0.0135,  0.2540],\n",
      "        [-0.0920,  0.0239, -0.1854,  0.1664,  0.0934, -0.0345, -0.2095,  0.2768,\n",
      "         -0.2310,  0.1067],\n",
      "        [-0.1337, -0.1303,  0.1108,  0.0759, -0.2875,  0.0259,  0.2865, -0.1089,\n",
      "         -0.2641, -0.0108],\n",
      "        [ 0.1562,  0.2992, -0.0070, -0.2879,  0.2732,  0.1908,  0.1413,  0.0276,\n",
      "          0.0576,  0.1038],\n",
      "        [ 0.0772,  0.2060,  0.1129, -0.0729, -0.1727, -0.1858, -0.0870,  0.1054,\n",
      "          0.1943,  0.0124],\n",
      "        [-0.1374,  0.2812, -0.0667, -0.0272, -0.0910, -0.0963, -0.0117,  0.2407,\n",
      "         -0.1927,  0.1031],\n",
      "        [-0.0451,  0.0440,  0.0661,  0.0202, -0.1506,  0.0771,  0.2764,  0.0597,\n",
      "         -0.0225,  0.1798],\n",
      "        [-0.0235,  0.2559,  0.0393, -0.0805,  0.0292,  0.1584,  0.2967,  0.3149,\n",
      "         -0.1553, -0.1594],\n",
      "        [ 0.0790,  0.0303, -0.1585,  0.2075, -0.2331, -0.2784, -0.2882,  0.2282,\n",
      "         -0.1492,  0.0819],\n",
      "        [ 0.1008,  0.1049, -0.2381,  0.1829,  0.1500, -0.2936,  0.1626,  0.2951,\n",
      "          0.1624,  0.1440]], device='cuda:0', requires_grad=True)\n",
      "1\n",
      "corrected_gradient tensor([ 0.0042, -0.0033,  0.0266,  0.0194, -0.0042,  0.0030, -0.0101, -0.0246,\n",
      "        -0.0255,  0.0370], device='cuda:0')\n",
      "lr tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.2643, -0.2802, -0.2494, -0.2168, -0.1121,  0.1552,  0.0994, -0.1165,\n",
      "         0.0650, -0.2289], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "corrected_gradient tensor([[-5.8014e-03,  9.0765e-04, -1.2976e-02,  7.2484e-03,  4.0708e-03,\n",
      "          1.0906e-02, -8.3228e-04,  6.0913e-03, -3.7658e-05,  5.9919e-03],\n",
      "        [ 9.5682e-04,  2.0626e-02,  2.5982e-02, -1.6008e-02, -4.0835e-03,\n",
      "         -3.8109e-03, -1.7237e-02, -1.2183e-02,  1.1913e-02,  3.9132e-04],\n",
      "        [-1.0937e-03, -1.9721e-02, -1.8055e-02,  1.4276e-02, -7.2034e-04,\n",
      "          3.5513e-03,  2.0503e-02,  6.4872e-03, -1.5305e-02,  7.5387e-03],\n",
      "        [-3.9180e-03, -2.2638e-02, -2.8220e-02,  1.6422e-02,  4.9270e-03,\n",
      "          7.8845e-03,  2.0742e-02,  9.4749e-03, -9.1367e-03,  8.8413e-03],\n",
      "        [ 3.4658e-03,  1.7637e-02,  2.1041e-02, -1.0171e-02,  6.7338e-04,\n",
      "         -3.3929e-03, -1.3491e-02,  2.4079e-03,  9.2223e-03, -1.1499e-02],\n",
      "        [-1.0070e-03,  4.6926e-03,  7.2595e-04, -6.3321e-03, -3.0902e-03,\n",
      "         -7.2280e-03, -9.2588e-03, -9.5082e-03,  6.1805e-03, -3.7676e-03],\n",
      "        [ 4.8543e-03,  3.6604e-04,  3.4903e-02, -1.2082e-02, -4.3439e-03,\n",
      "         -8.1176e-03, -6.3854e-04,  1.2380e-04, -4.7147e-03, -1.5985e-02],\n",
      "        [-6.0110e-03, -1.8434e-02, -3.2872e-02,  1.6633e-02,  7.5448e-03,\n",
      "          1.4761e-02,  2.1571e-02,  1.7170e-02, -3.5048e-03,  1.2089e-02],\n",
      "        [ 1.3714e-02,  1.3746e-02, -3.9903e-03, -1.2979e-03, -2.9765e-03,\n",
      "         -7.3867e-03, -1.2124e-02, -1.5919e-02,  5.5077e-03, -3.9142e-03],\n",
      "        [-5.1594e-03,  2.8182e-03,  1.3460e-02, -8.6880e-03, -2.0015e-03,\n",
      "         -7.1665e-03, -9.2344e-03, -4.1454e-03, -1.2498e-04,  3.1331e-04]],\n",
      "       device='cuda:0')\n",
      "lr tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([[-2.0448e-01, -2.2222e-01, -2.5473e-01,  2.0723e-01, -2.4396e-01,\n",
      "          8.4593e-02, -7.8828e-02,  5.5806e-02, -1.7660e-01, -2.2057e-01],\n",
      "        [-1.9488e-01, -2.2160e-02, -1.5829e-01, -2.4292e-01, -1.3938e-01,\n",
      "         -1.7191e-02, -3.0328e-01,  2.2207e-01,  1.2149e-01, -1.7484e-01],\n",
      "        [-3.8739e-02, -3.0334e-01,  1.6032e-01, -1.5036e-01, -2.5044e-01,\n",
      "          9.8126e-02,  5.5374e-02,  7.1658e-03,  5.9453e-02,  1.3002e-01],\n",
      "        [-3.0981e-01, -7.8071e-02,  2.1031e-01, -1.0307e-01,  8.3762e-02,\n",
      "         -1.9463e-01, -2.9404e-01,  2.2195e-01,  1.8604e-01,  1.7275e-01],\n",
      "        [-2.7566e-01, -2.4057e-01,  2.4851e-02, -1.0651e-01,  2.6444e-01,\n",
      "          4.7562e-03, -2.6218e-02,  2.8062e-01,  2.9408e-01, -1.9365e-01],\n",
      "        [-9.1578e-02,  2.8009e-01, -8.9052e-02, -2.0165e-01, -6.9183e-04,\n",
      "          1.2092e-01, -5.8952e-02,  2.6977e-01,  1.4881e-01, -2.1299e-01],\n",
      "        [ 2.5619e-01,  1.9318e-01,  6.0165e-02, -1.3245e-01,  1.5001e-01,\n",
      "         -2.4926e-01,  1.8781e-01,  1.0667e-01, -1.3153e-01, -2.7736e-01],\n",
      "        [ 3.0890e-01,  1.3785e-01,  3.8996e-02,  1.1004e-02,  3.0768e-01,\n",
      "         -1.9191e-01, -2.1338e-05, -1.2286e-01, -1.7020e-01,  2.6177e-02],\n",
      "        [ 1.2299e-01, -2.1906e-02, -2.5826e-01, -4.9090e-02,  1.6650e-01,\n",
      "         -2.9074e-01,  2.3593e-01,  7.5472e-02,  2.8459e-01,  9.1829e-02],\n",
      "        [ 1.7365e-01, -2.2502e-01,  5.9063e-02, -2.4086e-01,  3.6758e-02,\n",
      "         -2.9722e-01, -1.2085e-04,  4.3236e-02,  2.1932e-01,  1.0735e-01]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "3\n",
      "corrected_gradient tensor([ 0.0131, -0.0472,  0.0426,  0.0560, -0.0431, -0.0074, -0.0329,  0.0612,\n",
      "        -0.0200, -0.0222], device='cuda:0')\n",
      "lr tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.1027, -0.2131,  0.2177,  0.1322, -0.1113, -0.2857, -0.1777,  0.0016,\n",
      "         0.1436, -0.0887], device='cuda:0', requires_grad=True)\n",
      "enter adasecant\n",
      "0\n",
      "corrected_gradient tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "lr tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "1\n",
      "corrected_gradient tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
      "lr tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "2\n",
      "corrected_gradient tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0')\n",
      "lr tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params Parameter containing:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "3\n",
      "corrected_gradient tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
      "lr tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Epoch 1/1 - Loss: nan\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2260158ed00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x226017cfac0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11328125\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
