{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1689, -0.1860,  0.0812,  0.1872, -0.2180,  0.2179,  0.0343, -0.0496,\n",
      "         -0.0268, -0.0857],\n",
      "        [-0.1922,  0.1191,  0.2860, -0.2826,  0.0625,  0.2469, -0.0329, -0.2295,\n",
      "         -0.0391, -0.0941],\n",
      "        [-0.1550,  0.3015, -0.2341, -0.2573, -0.1454,  0.2853,  0.2870,  0.2963,\n",
      "          0.0691,  0.2268],\n",
      "        [ 0.3047, -0.2551,  0.2024, -0.3028,  0.0185,  0.0188,  0.0362,  0.0913,\n",
      "         -0.2760,  0.2242],\n",
      "        [-0.2149, -0.1047, -0.1039,  0.1232,  0.0444, -0.0077,  0.3138, -0.1337,\n",
      "          0.1189, -0.0587],\n",
      "        [-0.2164, -0.2320,  0.1837,  0.1021,  0.1257, -0.2711,  0.1361, -0.0784,\n",
      "          0.2676, -0.0169],\n",
      "        [ 0.0811, -0.3050,  0.1567,  0.0945,  0.2700, -0.0111,  0.0227, -0.2766,\n",
      "          0.1121, -0.3076],\n",
      "        [-0.2394,  0.3038, -0.0920,  0.1568, -0.2277, -0.1026,  0.2486, -0.0506,\n",
      "          0.2353, -0.2618],\n",
      "        [-0.0379,  0.2757,  0.1425, -0.0496, -0.2598, -0.2098,  0.1460,  0.1277,\n",
      "         -0.1266,  0.1363],\n",
      "        [ 0.2886,  0.2110,  0.2598, -0.1338,  0.2841,  0.1299, -0.2416,  0.1243,\n",
      "         -0.1593, -0.0216]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1251, -0.2282,  0.0168,  0.2672, -0.0436,  0.2787,  0.2163, -0.2339,\n",
      "         0.0312,  0.1434], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0993,  0.0942,  0.0144, -0.0142, -0.0502,  0.1336,  0.0446, -0.2270,\n",
      "          0.0273,  0.0999],\n",
      "        [-0.0423,  0.1188,  0.2280,  0.1479,  0.0330,  0.0554,  0.0576, -0.0135,\n",
      "          0.0337, -0.0637],\n",
      "        [-0.2344,  0.0184,  0.1817, -0.0697, -0.1531, -0.1829, -0.0239,  0.2393,\n",
      "         -0.0019,  0.0669],\n",
      "        [-0.2697,  0.0268, -0.2657,  0.0067,  0.0401, -0.2655,  0.1315,  0.0266,\n",
      "          0.2706,  0.1203],\n",
      "        [ 0.1453,  0.1360,  0.1747, -0.2186, -0.1803,  0.2341, -0.1723, -0.2892,\n",
      "         -0.0492, -0.1519],\n",
      "        [ 0.2117, -0.0592, -0.0136,  0.0248,  0.0754,  0.0733, -0.0346,  0.2738,\n",
      "          0.2277,  0.2046],\n",
      "        [ 0.2010, -0.1580,  0.0096,  0.0251,  0.0446,  0.1510, -0.0706,  0.1152,\n",
      "          0.0045, -0.3024],\n",
      "        [-0.2452, -0.2335, -0.2363, -0.0261, -0.1865, -0.3053,  0.1014, -0.1536,\n",
      "         -0.2210,  0.0890],\n",
      "        [-0.0676,  0.2608, -0.1587, -0.2420, -0.0048, -0.1587,  0.0686, -0.2034,\n",
      "          0.1895,  0.0775],\n",
      "        [-0.2575,  0.0781, -0.2295,  0.3025,  0.2661,  0.0739,  0.2173,  0.0131,\n",
      "          0.2342, -0.0909]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3127, -0.0607,  0.1668,  0.2609,  0.2004,  0.2675,  0.0451,  0.2304,\n",
      "        -0.0852,  0.1302], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p.grad))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p.grad))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p.grad))\n",
    "                    self.old_gradients.append(torch.zeros_like(p.grad))\n",
    "                    self.taus.append(torch.ones_like(p.grad))\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            # all parameters of a model seem to be contained within the same group\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                    # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                    if p.grad is not None:\n",
    "                        d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # all parameters of a model seem to be contained within the same group\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "        return #loss\n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    for i, param in enumerate(params):\n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, taus[i])\n",
    "        # later: build in normalization of gradients\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)(g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])(g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "    pass\n",
    "\n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "Epoch 1/10 - Loss: 2.3132859766483307\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "Epoch 2/10 - Loss: 2.313285991549492\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "Epoch 3/10 - Loss: 2.3132860213518143\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "Epoch 4/10 - Loss: 2.3132860213518143\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "Epoch 5/10 - Loss: 2.3132860362529755\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "Epoch 6/10 - Loss: 2.3132860958576202\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "Epoch 7/10 - Loss: 2.3132860958576202\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "Epoch 8/10 - Loss: 2.313286066055298\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "Epoch 9/10 - Loss: 2.313285991549492\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "Epoch 10/10 - Loss: 2.313286006450653\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17816e271c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17817031190>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08984375\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
