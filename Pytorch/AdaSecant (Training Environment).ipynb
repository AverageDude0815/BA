{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2501,  0.2601,  0.2433,  0.3059,  0.0878, -0.1158,  0.2993,  0.1901,\n",
      "          0.1981, -0.0084],\n",
      "        [ 0.0015, -0.0187,  0.0443,  0.0505, -0.2748,  0.1861, -0.0744,  0.1864,\n",
      "         -0.1253, -0.2782],\n",
      "        [-0.2717,  0.2392, -0.1123, -0.0765,  0.1025,  0.0858, -0.2340, -0.1811,\n",
      "         -0.2292, -0.0406],\n",
      "        [-0.0878, -0.2477, -0.1294, -0.0092,  0.1917,  0.1506, -0.2453, -0.0894,\n",
      "          0.0102, -0.0823],\n",
      "        [-0.2604, -0.3037, -0.2291, -0.1036, -0.3102,  0.0853, -0.2506,  0.1597,\n",
      "         -0.1367, -0.1329],\n",
      "        [ 0.1402, -0.0906,  0.2494, -0.0398,  0.1609,  0.1632, -0.1216,  0.1547,\n",
      "         -0.1449,  0.0746],\n",
      "        [ 0.2133, -0.0401, -0.1163, -0.0687,  0.0681, -0.3097,  0.1416, -0.3011,\n",
      "          0.3024, -0.0509],\n",
      "        [-0.0734,  0.2640, -0.2592, -0.2089, -0.0567, -0.0114, -0.1685, -0.0092,\n",
      "          0.1734, -0.2641],\n",
      "        [ 0.1854, -0.2834, -0.0970, -0.1077,  0.1404, -0.2849, -0.0240, -0.2419,\n",
      "         -0.0978,  0.1583],\n",
      "        [-0.1613,  0.0534,  0.1218,  0.3021, -0.2712,  0.0122,  0.1436,  0.0898,\n",
      "         -0.2194, -0.0744]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1846,  0.0135, -0.2736, -0.1920,  0.0783,  0.3049, -0.1824, -0.1927,\n",
      "        -0.2606,  0.2629], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0579,  0.0420,  0.1767,  0.1534,  0.2056, -0.1717, -0.1602,  0.3024,\n",
      "          0.1049,  0.3049],\n",
      "        [-0.1784, -0.2311,  0.1864, -0.1910, -0.0903,  0.0658,  0.2129,  0.0807,\n",
      "          0.1700, -0.1757],\n",
      "        [ 0.1800,  0.2567, -0.0014, -0.2794,  0.2835, -0.2770, -0.0621, -0.1317,\n",
      "         -0.0608,  0.0567],\n",
      "        [-0.0806,  0.0583, -0.1772,  0.1698, -0.2235, -0.1304, -0.1570, -0.0020,\n",
      "          0.0205,  0.1999],\n",
      "        [-0.2587, -0.1731, -0.0044,  0.2156,  0.0266,  0.2475, -0.2228,  0.2401,\n",
      "          0.3098,  0.1161],\n",
      "        [-0.3004,  0.2762, -0.1855, -0.1590, -0.0596, -0.1922,  0.1594,  0.0803,\n",
      "         -0.2019,  0.2791],\n",
      "        [-0.1357, -0.2807, -0.1393,  0.0926,  0.2035,  0.3069,  0.0360, -0.2394,\n",
      "          0.1869, -0.1829],\n",
      "        [-0.1533, -0.1835,  0.1140, -0.1379,  0.1658,  0.2522, -0.0752, -0.2961,\n",
      "         -0.2627, -0.1193],\n",
      "        [-0.1648,  0.2060, -0.1267,  0.1082, -0.2655, -0.0840, -0.1323,  0.2090,\n",
      "          0.0294, -0.0293],\n",
      "        [ 0.1773,  0.1608, -0.1043, -0.0231, -0.0241, -0.2144,  0.3087, -0.2653,\n",
      "          0.2506,  0.1948]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1258,  0.0773,  0.0936,  0.1453, -0.0318, -0.2104,  0.2476, -0.2734,\n",
      "         0.0328, -0.2257], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        # here we need to introduce some attribute to save params/gradients from old batch.\n",
    "        self.moving_average_of_g = None\n",
    "        self.delta = None\n",
    "        self.memory_size = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                    # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                    if p.grad is not None:\n",
    "                        d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # all parameters of a model seem to be contained within the same group\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "        return #loss\n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # g[i] corresponds to param[i]    \n",
    "    for i, param in enumerate(params):\n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        \n",
    "        correction_term = None # to be implemented\n",
    "        corrected_gradient = None # to be implemented\n",
    "        \n",
    "        \n",
    "    # update all attributes in optimizer\n",
    "    optimizer.gradients = g\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except:\n",
    "                # whatever happens if there is no next gradient\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Loss: 2.3263102769851685\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_opt = AdaSecant\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25107224e50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25107314430>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14453125\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
