{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # TODO: fix IF(torch.is_nonzero(...sum())) with element-wise where\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # normalization of gradients\n",
    "        g = g / torch.linalg.norm(g)\n",
    "        g_next = g_next / torch.linalg.norm(g_next)\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('g', g)\n",
    "            print('mg', optimizer.mean_gradients[i])\n",
    "            print('g_next', g_next)\n",
    "            print('mgs', optimizer.mean_gradient_squares[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = 0 for first iteration because no second derivative can be made yet\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        gamma[gamma != gamma] = 0.0\n",
    "        \n",
    "        #gamma = torch.where(optimizer.gamma_denomenators[i] == 0.0,\n",
    "         #                  optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i],\n",
    "          #                 torch.full_like(g, 0.0))\n",
    "        \n",
    "        if i == 1:\n",
    "            print('gamma', gamma)\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            # delta = -lr * corrected_gradient, lr = 1 as initialization\n",
    "            delta = -copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('md', optimizer.mean_deltas[i])\n",
    "            print('mds', optimizer.mean_delta_squares[i])\n",
    "            print('ma', optimizer.mean_alphas[i])\n",
    "            print('mas', optimizer.mean_alpha_squares[i])\n",
    "            print('mdas', optimizer.mean_delta_times_alphas[i])\n",
    "        \n",
    "        \n",
    "        # should I update moving averages for g, gamma as well? -> memory size will be set again later\n",
    "        \n",
    "        lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "              - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        lr[lr != lr] = 0.0\n",
    "        #lr = torch.where(torch.logical_or(optimizer.mean_alphas[i] == 0,\n",
    "         #                                 optimizer.mean_alpha_squares[i] == 0),\n",
    "          #              (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "           #              - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i]),\n",
    "            #            torch.full_like(g, 0.0))\n",
    "        \n",
    "        if i == 1:\n",
    "            print('corrected_gradient', corrected_gradient)\n",
    "            print('lr', lr)\n",
    "        \n",
    "        optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2/ optimizer.mean_delta_squares[i])\n",
    "                             * optimizer.taus[i] + 1)\n",
    "        optimizer.taus[i][optimizer.taus[i] != optimizer.taus[i]] = 1.0\n",
    "        #optimizer.taus[i] = torch.where(optimizer.mean_delta_squares[i] == 0,\n",
    "         #                              ((1 - optimizer.mean_deltas[i] ** 2\n",
    "          #                               / optimizer.mean_delta_squares[i])\n",
    "           #                             * optimizer.taus[i] + 1),\n",
    "            #                           torch.full_like(g, 1.0))\n",
    "        \n",
    "        if i == 1:\n",
    "            print('tau', optimizer.taus[i])\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        if i == 1:\n",
    "            print('new delta', new_delta)\n",
    "        \n",
    "        if i == 1:\n",
    "            print('params', params[i])\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "g tensor([ 0.2637, -0.3324,  0.0202, -0.3351,  0.2593,  0.1188, -0.2514, -0.6167,\n",
      "         0.1055, -0.4138], device='cuda:0')\n",
      "mg tensor([ 0.2637, -0.3324,  0.0202, -0.3351,  0.2593,  0.1188, -0.2514, -0.6167,\n",
      "         0.1055, -0.4138], device='cuda:0')\n",
      "g_next tensor([ 0.3905,  0.1203,  0.2923, -0.4389,  0.3191,  0.3074,  0.0720, -0.0989,\n",
      "         0.5809, -0.0793], device='cuda:0')\n",
      "mgs tensor([0.0695, 0.1105, 0.0004, 0.1123, 0.0672, 0.0141, 0.0632, 0.3804, 0.0111,\n",
      "        0.1713], device='cuda:0')\n",
      "gamma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "md tensor([-0.2637,  0.3324, -0.0202,  0.3351, -0.2593, -0.1188,  0.2514,  0.6167,\n",
      "        -0.1055,  0.4138], device='cuda:0')\n",
      "mds tensor([0.0695, 0.1105, 0.0004, 0.1123, 0.0672, 0.0141, 0.0632, 0.3804, 0.0111,\n",
      "        0.1713], device='cuda:0')\n",
      "ma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mdas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "corrected_gradient tensor([ 0.2637, -0.3324,  0.0202, -0.3351,  0.2593,  0.1188, -0.2514, -0.6167,\n",
      "         0.1055, -0.4138], device='cuda:0')\n",
      "lr tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([-0., 0., -0., 0., -0., -0., 0., 0., -0., 0.], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.0649,  0.0716,  0.0751,  0.0526, -0.1844,  0.1197,  0.2339, -0.1154,\n",
      "         0.1659, -0.0707], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "g tensor([ 0.3905,  0.1203,  0.2923, -0.4389,  0.3191,  0.3074,  0.0720, -0.0989,\n",
      "         0.5809, -0.0793], device='cuda:0')\n",
      "mg tensor([ 0.3905,  0.1203,  0.2923, -0.4389,  0.3191,  0.3074,  0.0720, -0.0989,\n",
      "         0.5809, -0.0793], device='cuda:0')\n",
      "g_next tensor([ 0.1048, -0.0422,  0.3646,  0.0416,  0.1261,  0.4948, -0.4247, -0.3072,\n",
      "        -0.0146, -0.5629], device='cuda:0')\n",
      "mgs tensor([0.1525, 0.0145, 0.0854, 0.1926, 0.1019, 0.0945, 0.0052, 0.0098, 0.3374,\n",
      "        0.0063], device='cuda:0')\n",
      "gamma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "md tensor([-0.1438,  0.1813, -0.0110,  0.1828, -0.1414, -0.0648,  0.1372,  0.3364,\n",
      "        -0.0575,  0.2257], device='cuda:0')\n",
      "mds tensor([0.0379, 0.0603, 0.0002, 0.0612, 0.0367, 0.0077, 0.0345, 0.2075, 0.0061,\n",
      "        0.0934], device='cuda:0')\n",
      "ma tensor([ 0.0576,  0.2058,  0.1237, -0.0472,  0.0272,  0.0858,  0.1470,  0.2354,\n",
      "         0.2161,  0.1521], device='cuda:0')\n",
      "mas tensor([0.0073, 0.0931, 0.0336, 0.0049, 0.0016, 0.0162, 0.0476, 0.1219, 0.1027,\n",
      "        0.0509], device='cuda:0')\n",
      "mdas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "corrected_gradient tensor([ 0.3905,  0.1203,  0.2923, -0.4389,  0.3191,  0.3074,  0.0720, -0.0989,\n",
      "         0.5809, -0.0793], device='cuda:0')\n",
      "lr tensor([2.2774, 0.8044, 0.0815, 3.5357, 4.7448, 0.6898, 0.8516, 1.3047, 0.2430,\n",
      "        1.3552], device='cuda:0')\n",
      "tau tensor([2.0000, 2.0000, 2.0000, 2.0000, 2.0000, 2.0000, 2.0000, 2.0000, 2.0000,\n",
      "        2.0000], device='cuda:0')\n",
      "new delta tensor([-0.8893, -0.0967, -0.0238,  1.5517, -1.5143, -0.2121, -0.0613,  0.1290,\n",
      "        -0.1412,  0.1075], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.9542, -0.0251,  0.0513,  1.6043, -1.6987, -0.0924,  0.1725,  0.0136,\n",
      "         0.0247,  0.0368], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "g tensor([-0.0202,  0.0666,  0.0320, -0.0329, -0.0871,  0.0070,  0.9916, -0.0277,\n",
      "        -0.0324, -0.0160], device='cuda:0')\n",
      "mg tensor([ 0.1852,  0.0935,  0.1622, -0.2359,  0.1160,  0.1572,  0.5318, -0.0633,\n",
      "         0.2742, -0.0477], device='cuda:0')\n",
      "g_next tensor([-0.0216,  0.0714,  0.0251, -0.0359, -0.0478, -0.0048,  0.9948, -0.0141,\n",
      "        -0.0200,  0.0083], device='cuda:0')\n",
      "mgs tensor([0.0764, 0.0095, 0.0432, 0.0968, 0.0547, 0.0473, 0.4943, 0.0053, 0.1692,\n",
      "        0.0033], device='cuda:0')\n",
      "gamma tensor([-0.0067,  0.2155, -0.0506,  0.0152,  0.2400, -0.0725, -0.0067, -0.2760,\n",
      "         0.0419, -0.4350], device='cuda:0')\n",
      "md tensor([-0.4827,  0.0423, -0.0168,  0.8050, -0.7654, -0.1317,  0.0469,  0.2327,\n",
      "        -0.0956,  0.1666], device='cuda:0')\n",
      "mds tensor([3.8016e-01, 3.4814e-02, 3.7947e-04, 1.1278e+00, 1.0623e+00, 2.4640e-02,\n",
      "        2.0520e-02, 1.1206e-01, 1.2369e-02, 5.2486e-02], device='cuda:0')\n",
      "ma tensor([-0.1552,  0.0761, -0.0508,  0.1588, -0.1698, -0.0898,  0.4982,  0.1533,\n",
      "        -0.1609,  0.1077], device='cuda:0')\n",
      "mas tensor([0.0806, 0.0480, 0.0491, 0.0776, 0.0759, 0.0499, 0.4104, 0.0635, 0.2270,\n",
      "        0.0274], device='cuda:0')\n",
      "mdas tensor([ 0.1660,  0.0026,  0.0028,  0.2864,  0.2796,  0.0290, -0.0256,  0.0046,\n",
      "         0.0394,  0.0034], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0216,  0.0714,  0.0251, -0.0359, -0.0478, -0.0048,  0.9948, -0.0141,\n",
      "        -0.0200,  0.0083], device='cuda:0')\n",
      "lr tensor([0.1128, 0.7976, 0.0306, 0.1221, 0.0571, 0.1221, 0.2861, 1.2562, 0.0601,\n",
      "        1.2592], device='cuda:0')\n",
      "tau tensor([1.8518, 2.8973, 1.5555, 1.9359, 1.9866, 1.6505, 2.9638, 2.0334, 1.5761,\n",
      "        1.9422], device='cuda:0')\n",
      "new delta tensor([ 0.0024, -0.0569, -0.0008,  0.0044,  0.0027,  0.0006, -0.2846,  0.0177,\n",
      "         0.0012, -0.0105], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.9517, -0.0821,  0.0505,  1.6087, -1.6960, -0.0918, -0.1120,  0.0313,\n",
      "         0.0259,  0.0263], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "Epoch 1/1 - Loss: 5.548641860485077\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x15195264160>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x151957d1580>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12890625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
