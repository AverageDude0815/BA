{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # TODO later: build in normalization of gradients\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = 0 for first iteration\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        if torch.is_nonzero(optimizer.gamma_denomenators[i].sum()):\n",
    "            # normal calculation of variance reduction term gamma\n",
    "            gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        else:\n",
    "            # gradient is equal to expected gradient in first epoch\n",
    "            # gamma is therefore zero (equivalent to no variance reduction)\n",
    "            gamma = torch.zeros_like(optimizer.gamma_denomenators[i])\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            delta = copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        \n",
    "        # should I update moving averages for g, gamma, and alpha as well? -> memory size will be set again later\n",
    "        \n",
    "        lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "             - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        \n",
    "        optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / optimizer.mean_delta_squares[i])\n",
    "                             * optimizer.taus[i] + 1)\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    #prepare_model(model)\n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Epoch 1/1 - Loss: nan\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1242b9572b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmAUlEQVR4nO3de5xVdb3/8ddbLhI3uRuKNHhLBQcYJw8dDCTLFEq8pZR3K9LjUSz1iHrykqeTx4iIU+nx+rDClB9KehJJJJT8ncRmCLmXqagEKtABQfECfM4fa824HWbPrM3MnhmY9/Px2I+99vf7XWt/vm4ffGat71rfryICMzOzrPZq7gDMzGz34sRhZmYFceIwM7OCOHGYmVlBnDjMzKwgbZs7gKbQq1evKCkpae4wzMx2K5WVlesjonfN8laROEpKSqioqGjuMMzMdiuSXqmt3JeqzMysIE4cZmZWECcOMzMrSKsY4zCzpvfBBx+wevVq3n333eYOxerRoUMH+vXrR7t27TK1d+Iws6JYvXo1Xbp0oaSkBEnNHY7lERFs2LCB1atXM2DAgEz7+FKVmRXFu+++S8+ePZ00WjhJ9OzZs6AzQycOMysaJ43dQ6G/kxOHmZkVpGiJQ9IBkuZJWiFpmaQJtbQZK2mxpEWSKiQdk1PXTdIMSSvTY3y6xr5XSgpJvYrVBzPbfW3cuJGf/exnu7Tv6NGj2bhxY51trr/+ep588sldOn5NJSUlrF+/vlGO1RSKOTi+DbgiIhZK6gJUSpoTEctz2swFHo2IkFQKTAcOS+t+DMyOiNMltQc6Vu0k6QDg88CrRYzfzHZjVYnjn/7pn3aq2759O23atMm776xZs+o9/ne/+90Gxbc7K9oZR0SsjYiF6fZmYAWwf402W+LDJQg7AQEgqSswArg7bfd+RGzM2fVHwL9UtTczq2nixIm8+OKLDBkyhKuuuoqnnnqKUaNG8dWvfpUjjzwSgJNPPpmjjjqKgQMHcscdd1TvW3UGsGrVKg4//HC+8Y1vMHDgQI4//ni2bt0KwPnnn8+MGTOq299www2UlZVx5JFHsnLlSgDWrVvH5z//ecrKyvjmN7/JJz7xiXrPLCZPnsygQYMYNGgQU6ZMAeDtt99mzJgxDB48mEGDBvHggw9W9/GII46gtLSUK6+8slH/+9WlSW7HlVQCDAUW1FJ3CvB9oA8wJi0+EFgH3CtpMFAJTIiItyWdBPwtIp6va0BH0nhgPED//v0brzNmVrCb/nsZy9e81ajHPGK/rtzwpYF562+55RaWLl3KokWLAHjqqad47rnnWLp0afVtp/fccw89evRg69atfOpTn+K0006jZ8+eHznOCy+8wK9+9SvuvPNOzjjjDB566CHOPvvsnb6vV69eLFy4kJ/97GdMmjSJu+66i5tuuonPfvazXHPNNcyePfsjyak2lZWV3HvvvSxYsICI4B/+4R8YOXIkL730Evvttx+PPfYYAJs2beLvf/87M2fOZOXKlUiq99JaYyr64LikzsBDwOURsdP/ORExMyIOA04Gbk6L2wJlwG0RMRR4G5goqSNwHXB9fd8bEXdERHlElPfuvdPkjmbWCh199NEfeVZh6tSpDB48mGHDhvHaa6/xwgsv7LTPgAEDGDJkCABHHXUUq1atqvXYp5566k5tnnnmGcaNGwfACSecQPfu3euM75lnnuGUU06hU6dOdO7cmVNPPZXf//73HHnkkTz55JNcffXV/P73v2efffaha9eudOjQga9//es8/PDDdOzYsc5jN6ainnFIakeSNKZFxMN1tY2I+ZIOSge7VwOrI6LqDGUGMBE4CBgAVJ1t9AMWSjo6Il4vVj/MrGHqOjNoSp06darefuqpp3jyySf5wx/+QMeOHTn22GNrfZZh7733rt5u06ZN9aWqfO3atGnDtm3bgOThukLka3/ooYdSWVnJrFmzuOaaazj++OO5/vrree6555g7dy4PPPAAP/nJT/jd735X0PftqmLeVSWSMYoVETE5T5uD03ZIKgPaAxvSJPCapE+mTY8DlkfEkojoExElEVFCkmDKnDTMrKYuXbqwefPmvPWbNm2ie/fudOzYkZUrV/Lss882egzHHHMM06dPB+CJJ57gf//3f+tsP2LECH7961/zzjvv8PbbbzNz5kw+85nPsGbNGjp27MjZZ5/NlVdeycKFC9myZQubNm1i9OjRTJkypfqSXFMo5hnHcOAcYImkRWnZtUB/gIi4HTgNOFfSB8BW4MycwfJLgWnpHVUvARcUMVYz28P07NmT4cOHM2jQIE488UTGjBnzkfoTTjiB22+/ndLSUj75yU8ybNiwRo/hhhtu4Ctf+QoPPvggI0eOpG/fvnTp0iVv+7KyMs4//3yOPvpoAL7+9a8zdOhQfvvb33LVVVex11570a5dO2677TY2b97M2LFjeffdd4kIfvSjHzV6/Pmo0FOp3VF5eXl4ISezprVixQoOP/zw5g6jWb333nu0adOGtm3b8oc//IGLL764Sc8MClHb7yWpMiLKa7b1JIdmZkXy6quvcsYZZ7Bjxw7at2/PnXfe2dwhNQonDjOzIjnkkEP405/+1NxhNDrPVWVmZgVx4jAzs4I4cZiZWUGcOMzMrCBOHGZmqc6dOwOwZs0aTj/99FrbHHvssdR3e/+UKVN45513qj9nmaY9ixtvvJFJkyY1+DgN5cRhZlbDfvvtVz3z7a6omThmzZpFt27dGiGylsGJw8z2SFdfffVHFnK68cYb+eEPf8iWLVs47rjjqqdAf+SRR3bad9WqVQwaNAiArVu3Mm7cOEpLSznzzDM/MlfVxRdfTHl5OQMHDuSGG24AkokT16xZw6hRoxg1ahTw0YWaaps2va7p2/NZtGgRw4YNo7S0lFNOOaV6OpOpU6dWT7VeNcHi008/zZAhQxgyZAhDhw6tcyqWLPwch5kV3+MT4fUljXvMjx8JJ96St3rcuHFcfvnl1Qs5TZ8+ndmzZ9OhQwdmzpxJ165dWb9+PcOGDeOkk07Ku+72bbfdRseOHVm8eDGLFy+mrKysuu573/sePXr0YPv27Rx33HEsXryYyy67jMmTJzNv3jx69froAqX5pk3v3r175unbq5x77rn853/+JyNHjuT666/npptuYsqUKdxyyy28/PLL7L333tWXxyZNmsRPf/pThg8fzpYtW+jQoUPW/8q18hmHme2Rhg4dyptvvsmaNWt4/vnn6d69O/379yciuPbaayktLeVzn/scf/vb33jjjTfyHmf+/PnV/4CXlpZSWlpaXTd9+nTKysoYOnQoy5YtY/ny5fkOA+SfNh2yT98OyQSNGzduZOTIkQCcd955zJ8/vzrGs846i1/+8pe0bZucGwwfPpxvf/vbTJ06lY0bN1aX7yqfcZhZ8dVxZlBMp59+OjNmzOD111+vvmwzbdo01q1bR2VlJe3ataOkpKTW6dRz1XY28vLLLzNp0iT++Mc/0r17d84///x6j1PX3IBZp2+vz2OPPcb8+fN59NFHufnmm1m2bBkTJ05kzJgxzJo1i2HDhvHkk09y2GGH1X+wPHzGYWZ7rHHjxvHAAw8wY8aM6rukNm3aRJ8+fWjXrh3z5s3jlVdeqfMYI0aMYNq0aQAsXbqUxYsXA/DWW2/RqVMn9tlnH9544w0ef/zx6n3yTemeb9r0Qu2zzz507969+mzlF7/4BSNHjmTHjh289tprjBo1iltvvZWNGzeyZcsWXnzxRY488kiuvvpqysvLq5e23VU+4zCzPdbAgQPZvHkz+++/P3379gXgrLPO4ktf+hLl5eUMGTKk3r+8L774Yi644AJKS0sZMmRI9ZTngwcPZujQoQwcOJADDzyQ4cOHV+8zfvx4TjzxRPr27cu8efOqy/NNm17XZal87rvvPi666CLeeecdDjzwQO699162b9/O2WefzaZNm4gIvvWtb9GtWze+853vMG/ePNq0acMRRxzBiSeeWPD35fK06mZWFJ5WffdSyLTqvlRlZmYFceIwM7OCFHPN8QMkzZO0QtIySRNqaTNW0mJJiyRVSDomp66bpBmSVqbH+HRa/oO0bLGkmZK6FasPZtYwreFS+J6g0N+pmGcc24ArIuJwYBhwiaQjarSZCwyOiCHAhcBdOXU/BmZHxGHAYGBFWj4HGBQRpcBfgGuK1wUz21UdOnRgw4YNTh4tXESwYcOGgh4KLNpdVRGxFlibbm+WtALYH1ie02ZLzi6dgACQ1BUYAZyftnsfeD/dfiJnn2eB2mciM7Nm1a9fP1avXs26deuaOxSrR4cOHejXr1/m9k1yO66kEmAosKCWulOA7wN9gDFp8YHAOuBeSYOBSmBCRLxdY/cLgQfzfOd4YDxA//79G94JMytIu3btGDBgQHOHYUVQ9MFxSZ2Bh4DLI+KtmvURMTO9HHUycHNa3BYoA26LiKHA28DEGse9juRy2LTavjci7oiI8ogo7927d2N1x8ys1Stq4pDUjiRpTIuIh+tqGxHzgYMk9QJWA6sjouoMZQZJIqk67nnAF4GzwhdQzcyaVDHvqhJwN7AiIibnaXNw2g5JZUB7YENEvA68JumTadPjSMdGJJ0AXA2cFBHv1HJYMzMromKOcQwHzgGWSFqUll0L9AeIiNuB04BzJX0AbAXOzDmDuBSYJqk98BJwQVr+E2BvYE6ac56NiIuK2A8zM8vhKUfMzKxWnnLEzMwahROHmZkVxInDzMwK4sRhZmYFqTdxSPqypC7p9r9Keji9ddbMzFqhLGcc30nnmjoG+AJwH3BbccMyM7OWKkvi2J6+jyGZAuQRkgf1zMysFcqSOP4m6b+AM4BZkvbOuJ+Zme2BsiSAM4DfAidExEagB3BVMYMyM7OWK8uUI32BxyLiPUnHAqXAz4sZlJmZtVxZzjgeArZLOphk0sIBwP1FjcrMzFqsLIljR0RsA04FpkTEt0jOQszMrBXKkjg+kPQV4FzgN2lZu+KFZGZmLVmWxHEB8GngexHxsqQBwC+LG5aZmbVU9SaOiFgOXEmyrsYgkpX5bil6ZGZm1iLVe1dVeifVfcAqQMABks5Ll3o1M7NWJsvtuD8Ejo+IPwNIOhT4FXBUMQMzM7OWKcsYR7uqpAEQEX8hw+C4pAMkzZO0QtIySRNqaTNW0mJJiyRVpPNhVdV1kzRD0sr0GJ9Oy3tImiPphfS9e7aumplZY8iSOCok3S3p2PR1J1CZYb9twBURcTgwDLhE0hE12swFBkfEEOBC4K6cuh8DsyPiMGAwsCItnwjMjYhD0v0nZojFzMwaSZbEcTGwDLgMmAAsB75Z304RsTYiFqbbm0n+4d+/Rpst8eGi552AAJDUFRhB8sAhEfF+Ot0JwFiSMRfS95Mz9MHMzBpJvWMcEfEeMDl9ASDp/wPDs36JpBJgKLCglrpTgO8DfUhm4AU4EFgH3CtpMMkZzoSIeBvYNyLWprGtldQnz3eOB8YD9O/fP2uoZmZWj12d5Tbzv8SSOpNMW3J5RLxVsz4iZqaXo04Gbk6L2wJlJNO4DwXepsBLUhFxR0SUR0R57969C9nVzMzqsKuJI+pvApLakSSNaRHxcJ0HTG7vPUhSL2A1yfMiVWcoM0gSCcAbkvqmx+8LvLkL8ZuZ2S7Ke6lK0qn5qoCP1XdgSSIZo1gREZPztDkYeDEiIl2Otj2wIf38mqRPpnd0HUcytgLwKHAecEv6/kh9sZiZWeOpa4zjS3XU/aaOuirDgXNInjhflJZdS3qZKyJuB04DzpX0AbAVODNnsPxSYJqk9sBLJFOfQJIwpkv6GvAq8OUMsZiZWSPRh/9O77nKy8ujoqKiucMwM9utSKqMiPKa5V4C1szMCuLEYWZmBXHiMDOzgtSbONI5pC7xnFBmZgbZzjjGAfsBf5T0gKQvpLfamplZK5RlIae/RsR1wKHA/cA9wKuSbpLUo9gBmplZy5JpjENSKcm6HD8geRL8dOAt4HfFC83MzFqiLCsAVgIbSZ4Cn5hOegiwQFLmiQ7NzGzPkGUFwC9HxEu1VUREvmlJzMxsD5XlUtUmSVMlLZRUKenHknoWPTIzM2uRsiSOB0jWxjiNZGxjHfBgMYMyM7OWK8ulqh4RcXPO53+TdHKR4jEzsxYuyxnHPEnjJO2Vvs4AHit2YGZm1jJlSRzfJHl+4/309QDwbUmbJe20op+Zme3Zsqw53qUpAjEzs91DljEOJJ0EjEg/PhURWRZyMjOzPVCWSQ5vASaQLN26HJiQlpmZWSuUZYxjNPD5iLgnIu4BTkjL6iTpAEnzJK2QtEzShFrajJW0WNKidBbeY3LqVklaUlWXUz5E0rM5+xydratmZtYYMl2qAroBf0+398m4zzbgiohYKKkLUClpTkQsz2kzF3g0IiKdD2s6cFhO/aiIWF/juLcCN0XE45JGp5+PzRiTmZk1UJbE8e/AnyTNA0Qy1nFNfTtFxFpgbbq9WdIKYH+Sy11Vbbbk7NIJyLIAegBd0+19gDUZ9jEzs0ZSZ+KQtBewAxgGfIokcVwdEa8X8iWSSoChwIJa6k4Bvg/0AcbkVAXwhKQA/isi7kjLLwd+K2kSyaW2f8zzneOB8QD9+/cvJFwzM6uDIur+I1/S/IgYUWejuvfvDDwNfC8iHq6j3Qjg+oj4XPp5v4hYI6kPMAe4NCLmS5oKPB0RD6UPI46v2ief8vLyqKioqKuJmZnVIKkyIsprlmcZHJ8j6cp0sLtH1Svjl7YjWb9jWl1JAyAi5gMHSeqVfl6Tvr8JzASqBsHPA6qO9f9yys3MrAlkSRwXApcA84HK9FXvn+/p8rJ3AysiYnKeNgdXLUMrqQxoD2yQ1CkdUEdSJ+B4YGm62xpgZLr9WeCFDH0wM7NGkmVw/PCIeDe3QFKHDPsNB84BlkhalJZdC/QHiIjbSWbcPVfSB8BW4Mz0Dqt9gZlpTmkL3B8Rs9NjfAP4saS2wLuk4xhmZtY0soxxLIyIsvrKWjKPcZiZFS7fGEfeMw5JHye5ffZjkoaS3FEFya2wHYsSpZmZtXh1Xar6AnA+0A/IHaPYTHLJyczMWqG8iSMi7gPuk3RaRDzUhDGZmVkLlmVw/DeSvgqU5LaPiO8WKygzM2u5siSOR4BNJLfhvlfccMzMrKXLkjj6RcQJRY/EzMx2C1keAPwfSUcWPRIzM9stZDnjOAY4X9LLJJeqBERElBY1MjMza5GyJI4Tix6FmZntNvJeqpL0WYCIeAXYKyJeqXoBRzVVgGZm1rLUNcYxKWe75nMc/1qEWMzMbDdQV+JQnu3aPpuZWStRV+KIPNu1fTYzs1airsHxAyU9SnJ2UbVN+nlA0SMzM7MWqa7EMTZne1KNupqfzcyslahrksOnmzIQMzPbPWR5ctzMzKxa0RKHpAMkzZO0QtIySRNqaTNW0mJJiyRVSDomp26VpCVVdTX2u1TSn9Pj3lqsPpiZ2c6yPDleTdJeQOeIeCtD823AFRGxUFIXoFLSnIhYntNmLvBous54KTAdOCynflRErK8RwyiS8ZfSiHhPUp9C+mBmZg1T7xmHpPsldZXUCVgO/FnSVfXtFxFrI2Jhur0ZWEGyFG1umy3x4aLnnch2m+/FwC0R8V56jDcz7GNmZo0ky6WqI9IzjJOBWUB/4JxCvkRSCTAUWFBL3SmSVgKPARfmVAXwhKRKSeNzyg8FPiNpgaSnJX0qz3eOTy9/Vaxbt66QcM3MrA5ZEkc7Se1IEscjEfEBBTwAKKkzyZQll9d2iSsiZkbEYenxb86pGh4RZSSTLF4iaURa3hboDgwDrgKmS9rpSfaIuCMiyiOivHfv3lnDNTOzemRJHP8FrCK5lDRf0ieALGMcpAnnIWBaRDxcV9uImA8cJKlX+nlN+v4mMBM4Om26Gng4Es8BO4BeWeIxM7OGqzdxRMTUiNg/Ikan/1i/Aoyqb7/0LOBuYEVETM7T5uCqswVJZUB7YIOkTumAOunYyvHA0nS3XwOfTesOTfdZj5mZNYl676pKb6O9F9gM3EUyVjEReKKeXYeTjIUskbQoLbuWZIyEiLgdOA04V9IHwFbgzPQOq32BmWlOaQvcHxGz02PcA9wjaSnwPnBezgC7mZkVmer7N1fS8xExWNIXgEuA7wD3puMPu4Xy8vKoqKiov6GZmVWTVBkR5TXLs4xxVA08jyZJGM/jadXNzFqtLImjUtITJInjt+nYw47ihmVmZi1VlifHvwYMAV6KiHck9QQuKGpUZmbWYtWbOCJih6R+wFfTweqnI+K/ix6ZmZm1SFmmHLkFmEAy3chy4DJJ3y92YGZm1jJluVQ1GhgSETsAJN0H/Am4ppiBmZlZy5R1WvVuOdv7FCEOMzPbTWQ54/h34E+S5pHchjsCn22YmbVadSaOdP2NHSQTCn6KJHFcHRGvN0FsZmbWAtWZONI7qv45IqYDjzZRTGZm1oJlGeOYI+nKdCnYHlWvokdmZmYtUpYxjqrFlS7JKQvgwMYPx8zMWrosDwAOaIpAzMxs95D3UpWksyXttESspG9I+mpxwzIzs5aqrjGOK0gWTarpwbTOzMxaoboSR5uI2FyzMF03vF3xQjIzs5asrsTRLl229SPSadXbFy8kMzNryepKHHcDMySVVBWk2w+kdXVKb9+dJ2mFpGXpErQ124yVtFjSIkkVko7JqVslaUlVXS37XikpJPWqLxYzM2s8ee+qiohJkrYAT0vqTHIL7tvALRFxW4ZjbwOuiIiF6VlKpaQ5EbE8p81c4NF0nfFSYDpwWE79qIhYX/PAkg4APg+8miEOMzNrRPU9OX47cHuaOFTbmEcd+64F1qbbmyWtAPYnmZq9qs2WnF06kSSnLH4E/AvwSNZ4zMyscWSaHTcithSSNGpKL3ENBRbUUneKpJXAY3z4sCEkSeQJSZWSxue0Pwn4W7r2eV3fOT69/FWxbt26XQ3dzMxqyDqt+i5Lz1YeAi5P78j6iIiYGRGHAScDN+dUDY+IMuBE4BJJIyR1BK4Drq/veyPijogoj4jy3r17N0ZXzMyMIicOSe1Iksa0iHi4rrYRMR84qGqwOyLWpO9vAjOBo4GDgAHA85JWAf2AhZI+XrROmJnZR2SZqwpJ/wiU5LaPiJ/Xs49I7r5aERGT87Q5GHgxHRwvI7nNd0N6G/Be6dhIJ+B44LsRsQTok7P/KqC8tgF0MzMrjnoTh6RfkPylvwjYnhYHUGfiAIYD5wBLJC1Ky64F+kP1wPtpwLmSPgC2AmemSWRfYGaSe2gL3B8Rs7N3y8zMikURdd/IlN4NdUTU17AFKy8vj4qKnR4FMTOzOkiqjIjymuVZxjiWAh5DMDMzINsYRy9guaTngPeqCiPipKJFZWZmLVaWxHFjsYMwM7PdR5aFnJ5uikDMzGz3UO8Yh6Rhkv4oaYuk9yVtl7TTg3xmZtY6ZBkc/wnwFeAF4GPA19MyMzNrhTI9ABgRf5XUJiK2A/dK+p8ix2VmZi1UlsTxjqT2wCJJt5LMeLvTAk9mZtY6ZLlUdU7a7p9J1uM4gOSJbzMza4Wy3FX1iqSPAX0j4qYmiMnMzFqwLHdVfYlknqrZ6echkh4tclxmZtZCZblUdSPJlOYbASJiEclMuWZm1gplSRzbImJT0SMxM7PdQpa7qpZK+irQRtIhwGWAb8c1M2ulspxxXAoMJJng8FfAW8DlRYzJzMxasCx3Vb1Dss73dcUPx8zMWrq8iaO+O6c8rbqZWetU1xnHp4HXSC5PLQBUyIElHUCyvOzHgR3AHRHx4xptxgI3p/XbgMsj4pm0bhWwmWS52m1Vq1BJ+gHwJeB94EXggojYWEhsZma26+oa4/g4yRrhg4AfA58H1kfE0xmnWt8GXBERhwPDgEskHVGjzVxgcEQMAS4E7qpRPyoihtRYunAOMCgiSoG/ANdkiMXMzBpJ3sQREdsjYnZEnEfyD/9fgackXZrlwBGxNiIWptubgRXA/jXabMlZy7wTUO+65hHxRERsSz8+C/TLEo+ZmTWOOu+qkrS3pFOBXwKXAFOBhwv9EkklwFCSS141606RtBJ4jOSso0oAT0iqlDQ+z6EvBB7P853jJVVIqli3bl2hIZuZWR768A/+GhXSfSSXqR4HHoiIpbv0BVJn4GngexGRN+lIGgFcHxGfSz/vFxFrJPUhuTx1aUTMz2l/HVAOnBr5OpEqLy+PioqKXQnfzKzVklRZY6gAqHtw/ByS2XAPBS6TqsfGBUREdM3wpe2Ah4BpdSUNkgPOl3SQpF4RsT4i1qTlb0qaSTLtyfz0uOcBXwSOqy9pmJlZ48qbOCIiy8OBeSnJNHcDKyJicp42BwMvRkRIKgPaAxskdQL2iojN6fbxwHfTfU4ArgZGps+YmJlZE8q0AuAuGk5y1rJE0qK07FqgP0BE3E6yrse5kj4AtgJnpklkX2BmepbTFrg/Imanx/gJsDcwJ61/NiIuKmI/zMwsR94xjj2JxzjMzAqXb4yjQZejzMys9XHiMDOzgjhxmJlZQZw4zMysIE4cZmZWECcOMzMriBOHmZkVxInDzMwK4sRhZmYFceIwM7OCOHGYmVlBnDjMzKwgThxmZlYQJw4zMyuIE4eZmRXEicPMzArixGFmZgUpWuKQdICkeZJWSFomaUItbcZKWixpkaQKScfk1K2StKSqLqe8h6Q5kl5I37sXqw9mZrazYp5xbAOuiIjDgWHAJZKOqNFmLjA4IoYAFwJ31agfFRFDaixdOBGYGxGHpPtPLEr0ZmZWq6IljohYGxEL0+3NwApg/xpttsSHi553ArIsgD4WuC/dvg84uVECNjOzTJpkjENSCTAUWFBL3SmSVgKPkZx1VAngCUmVksbnlO8bEWshSU5AnzzfOT69/FWxbt26RuqJmZkVPXFI6gw8BFweEW/VrI+ImRFxGMmZw805VcMjogw4keQy14hCvjci7oiI8ogo79279653wMzMPqKoiUNSO5KkMS0iHq6rbUTMBw6S1Cv9vCZ9fxOYCRydNn1DUt/0+H2BN4sUvpmZ1aKYd1UJuBtYERGT87Q5OG2HpDKgPbBBUidJXdLyTsDxwNJ0t0eB89Lt84BHitUHMzPbWdsiHns4cA6wRNKitOxaoD9ARNwOnAacK+kDYCtwZkSEpH2BmWlOaQvcHxGz02PcAkyX9DXgVeDLReyDmZnVoA9vatpzlZeXR0VFRf0NzcysmqTKGo9DAH5y3MzMCuTEYWZmBXHiMDOzgjhxmJlZQZw4zMysIE4cZmZWECcOMzMriBOHmZkVxInDzMwK4sRhZmYFceIwM7OCOHGYmVlBnDjMzKwgThxmZlYQJw4zMyuIE4eZmRXEicPMzArSKlYAlLQOeKW549gFvYD1zR1EE2pt/QX3ubXYXfv8iYjoXbOwVSSO3ZWkitqWbdxTtbb+gvvcWuxpffalKjMzK4gTh5mZFcSJo2W7o7kDaGKtrb/gPrcWe1SfPcZhZmYF8RmHmZkVxInDzMwK4sTRjCT1kDRH0gvpe/c87U6Q9GdJf5U0sZb6KyWFpF7Fj7phGtpnST+QtFLSYkkzJXVrsuALlOF3k6Spaf1iSWVZ922pdrXPkg6QNE/SCknLJE1o+uh3TUN+57S+jaQ/SfpN00XdQBHhVzO9gFuBien2ROA/amnTBngROBBoDzwPHJFTfwDwW5IHHHs1d5+K3WfgeKBtuv0fte3fEl71/W5pm9HA44CAYcCCrPu2xFcD+9wXKEu3uwB/2dP7nFP/beB+4DfN3Z+sL59xNK+xwH3p9n3AybW0ORr4a0S8FBHvAw+k+1X5EfAvwO5yl0OD+hwRT0TEtrTds0C/4oa7y+r73Ug//zwSzwLdJPXNuG9LtMt9joi1EbEQICI2AyuA/Zsy+F3UkN8ZSf2AMcBdTRl0QzlxNK99I2ItQPrep5Y2+wOv5XxenZYh6STgbxHxfLEDbUQN6nMNF5L8JdcSZelDvjZZ+9/SNKTP1SSVAEOBBY0fYqNraJ+nkPzht6NI8RVF2+YOYE8n6Ung47VUXZf1ELWUhaSO6TGO39XYiqVYfa7xHdcB24BphUXXZOrtQx1tsuzbEjWkz0ml1Bl4CLg8It5qxNiKZZf7LOmLwJsRUSnp2MYOrJicOIosIj6Xr07SG1Wn6emp65u1NFtNMo5RpR+wBjgIGAA8L6mqfKGkoyPi9UbrwC4oYp+rjnEe8EXguEgvErdAdfahnjbtM+zbEjWkz0hqR5I0pkXEw0WMszE1pM+nAydJGg10ALpK+mVEnF3EeBtHcw+ytOYX8AM+OlB8ay1t2gIvkSSJqsG3gbW0W8XuMTjeoD4DJwDLgd7N3Zd6+lnv70ZybTt30PS5Qn7zlvZqYJ8F/ByY0tz9aKo+12hzLLvR4HizB9CaX0BPYC7wQvreIy3fD5iV0240yV0mLwLX5TnW7pI4GtRn4K8k14sXpa/bm7tPdfR1pz4AFwEXpdsCfprWLwHKC/nNW+JrV/sMHENyiWdxzm87urn7U+zfOecYu1Xi8JQjZmZWEN9VZWZmBXHiMDOzgjhxmJlZQZw4zMysIE4cZmZWECcOswaQtF3SopxXo81kK6lE0tLGOp5ZY/GT42YNszUihjR3EGZNyWccZkUgaZWk/5D0XPo6OC3/hKS56boMcyX1T8v3TdcXeT59/WN6qDaS7kzXqHhC0sfS9pdJWp4e54Fm6qa1Uk4cZg3zsRqXqs7MqXsrIo4GfkIyCyrp9s8jopRkgsapaflU4OmIGAyUAcvS8kOAn0bEQGAjcFpaPhEYmh7nouJ0zax2fnLcrAEkbYmIzrWUrwI+GxEvpZP3vR4RPSWtB/pGxAdp+dqI6CVpHdAvIt7LOUYJMCciDkk/Xw20i4h/kzQb2AL8Gvh1RGwpclfNqvmMw6x4Is92vja1eS9nezsfjkuOIZn/6CigUpLHK63JOHGYFc+ZOe9/SLf/BxiXbp8FPJNuzwUuhuo1qLvmO6ikvYADImIeySJA3YCdznrMisV/pZg1zMckLcr5PDsiqm7J3VvSApI/0L6Sll0G3CPpKmAdcEFaPgG4Q9LXSM4sLgbW5vnONsAvJe1DMvPqjyJiYyP1x6xeHuMwK4J0jKM8ItY3dyxmjc2XqszMrCA+4zAzs4L4jMPMzArixGFmZgVx4jAzs4I4cZiZWUGcOMzMrCD/B32fqcQMHOV9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1242bfa35b0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11328125\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
