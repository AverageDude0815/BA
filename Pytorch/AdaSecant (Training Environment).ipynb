{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # TODO: fix IF(torch.is_nonzero(...sum())) with element-wise where\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # normalization of gradients\n",
    "        g = g / torch.linalg.norm(g)\n",
    "        g_next = g_next / torch.linalg.norm(g_next)\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('g', g)\n",
    "            print('mg', optimizer.mean_gradients[i])\n",
    "            print('g_next', g_next)\n",
    "            print('mgs', optimizer.mean_gradient_squares[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = g for first iteration ==> lr = 0\n",
    "            #alpha = copy.deepcopy(g)\n",
    "            \n",
    "            # alpha = 0 for first iteration because no second derivative can be made yet\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        if torch.is_nonzero(optimizer.gamma_denomenators[i].sum()):\n",
    "            # normal calculation of variance reduction term gamma\n",
    "            gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        else:\n",
    "            # gradient is equal to expected gradient in first epoch\n",
    "            # gamma is therefore zero (equivalent to no variance reduction)\n",
    "            gamma = torch.zeros_like(optimizer.gamma_denomenators[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('gamma', gamma)\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            # delta = -lr * corrected_gradient, lr = 1 as initialization\n",
    "            delta = -copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('md', optimizer.mean_deltas[i])\n",
    "            print('mds', optimizer.mean_delta_squares[i])\n",
    "            print('ma', optimizer.mean_alphas[i])\n",
    "            print('mas', optimizer.mean_alpha_squares[i])\n",
    "            print('mdas', optimizer.mean_delta_times_alphas[i])\n",
    "        \n",
    "        \n",
    "        # should I update moving averages for g, gamma, and alpha as well? -> memory size will be set again later\n",
    "        \n",
    "        if torch.is_nonzero(optimizer.mean_alphas[i].sum()) or torch.is_nonzero(optimizer.mean_alpha_squares[i].sum()):\n",
    "            # normal calculation of lr\n",
    "            lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "                 - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        else:\n",
    "            # Catch initial exception by setting lr = 1. Because: lr = 0 ==> delta = 0 ==> optimization stops \n",
    "            lr = torch.ones_like(g)\n",
    "        \n",
    "        if i == 1:\n",
    "            print('corrected_gradient', corrected_gradient)\n",
    "            print('lr', lr)\n",
    "        \n",
    "        if torch.is_nonzero(optimizer.mean_delta_squares[i].sum()):\n",
    "            optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / optimizer.mean_delta_squares[i])\n",
    "                                 * optimizer.taus[i] + 1)\n",
    "        else:\n",
    "            optimizer.taus[i] = torch.ones_like(optimizer.taus[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('tau', optimizer.taus[i])\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        if i == 1:\n",
    "            print('new delta', new_delta)\n",
    "        \n",
    "        if i == 1:\n",
    "            print('params', params[i])\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "g tensor([-0.0287,  0.0124, -0.0969,  0.5299,  0.0427, -0.6046,  0.1519, -0.4763,\n",
      "         0.2922, -0.0784], device='cuda:0')\n",
      "mg tensor([-0.0287,  0.0124, -0.0969,  0.5299,  0.0427, -0.6046,  0.1519, -0.4763,\n",
      "         0.2922, -0.0784], device='cuda:0')\n",
      "g_next tensor([ 0.1727,  0.3257,  0.0115,  0.5608, -0.0808, -0.1404, -0.2605, -0.5027,\n",
      "         0.3223, -0.3142], device='cuda:0')\n",
      "mgs tensor([8.2519e-04, 1.5494e-04, 9.3937e-03, 2.8079e-01, 1.8265e-03, 3.6555e-01,\n",
      "        2.3078e-02, 2.2683e-01, 8.5406e-02, 6.1496e-03], device='cuda:0')\n",
      "gamma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "md tensor([ 0.0287, -0.0124,  0.0969, -0.5299, -0.0427,  0.6046, -0.1519,  0.4763,\n",
      "        -0.2922,  0.0784], device='cuda:0')\n",
      "mds tensor([8.2519e-04, 1.5494e-04, 9.3937e-03, 2.8079e-01, 1.8265e-03, 3.6555e-01,\n",
      "        2.3078e-02, 2.2683e-01, 8.5406e-02, 6.1496e-03], device='cuda:0')\n",
      "ma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mdas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0287,  0.0124, -0.0969,  0.5299,  0.0427, -0.6046,  0.1519, -0.4763,\n",
      "         0.2922, -0.0784], device='cuda:0')\n",
      "lr tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([ 0.0287, -0.0124,  0.0969, -0.5299, -0.0427,  0.6046, -0.1519,  0.4763,\n",
      "        -0.2922,  0.0784], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-1.8331e-01,  7.0933e-02, -8.2210e-05, -8.4449e-01, -1.3825e-01,\n",
      "         5.4058e-01, -1.4939e-01,  2.2276e-01, -5.3264e-01,  1.5501e-01],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "g tensor([ 0.2200,  0.5666,  0.3000, -0.2725, -0.1375,  0.5065, -0.3213, -0.1726,\n",
      "        -0.2341,  0.0559], device='cuda:0')\n",
      "mg tensor([ 0.2200,  0.5666,  0.3000, -0.2725, -0.1375,  0.5065, -0.3213, -0.1726,\n",
      "        -0.2341,  0.0559], device='cuda:0')\n",
      "g_next tensor([ 0.1963,  0.5629,  0.3275, -0.3547, -0.2029,  0.4254, -0.3759, -0.1437,\n",
      "        -0.1520,  0.0663], device='cuda:0')\n",
      "mgs tensor([0.0484, 0.3210, 0.0900, 0.0742, 0.0189, 0.2566, 0.1032, 0.0298, 0.0548,\n",
      "        0.0031], device='cuda:0')\n",
      "gamma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "md tensor([ 0.0287, -0.0124,  0.0969, -0.5299, -0.0427,  0.6046, -0.1519,  0.4763,\n",
      "        -0.2922,  0.0784], device='cuda:0')\n",
      "mds tensor([8.2519e-04, 1.5494e-04, 9.3937e-03, 2.8079e-01, 1.8265e-03, 3.6555e-01,\n",
      "        2.3078e-02, 2.2683e-01, 8.5406e-02, 6.1496e-03], device='cuda:0')\n",
      "ma tensor([ 0.1130,  0.2519,  0.1804, -0.3647, -0.0819,  0.5051, -0.2151,  0.1380,\n",
      "        -0.2392,  0.0611], device='cuda:0')\n",
      "mas tensor([0.0281, 0.1396, 0.0716, 0.2926, 0.0148, 0.5612, 0.1018, 0.0419, 0.1259,\n",
      "        0.0082], device='cuda:0')\n",
      "mdas tensor([ 0.0032, -0.0031,  0.0175,  0.1933,  0.0035,  0.3054,  0.0327,  0.0657,\n",
      "         0.0699,  0.0048], device='cuda:0')\n",
      "corrected_gradient tensor([ 0.2200,  0.5666,  0.3000, -0.2725, -0.1375,  0.5065, -0.3213, -0.1726,\n",
      "        -0.2341,  0.0559], device='cuda:0')\n",
      "lr tensor([0.0558, 0.0558, 0.1180, 0.3191, 0.1146, 0.2629, 0.1551, 0.7579, 0.2683,\n",
      "        0.2820], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([-0.0123, -0.0316, -0.0354,  0.0870,  0.0158, -0.1332,  0.0498,  0.1308,\n",
      "         0.0628, -0.0158], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.1956,  0.0393, -0.0355, -0.7575, -0.1225,  0.4074, -0.0996,  0.3536,\n",
      "        -0.4698,  0.1392], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "g tensor([-0.0746, -0.1571, -0.1934, -0.9499,  0.1153, -0.0336,  0.0816, -0.0402,\n",
      "         0.0149, -0.0847], device='cuda:0')\n",
      "mg tensor([-0.0746, -0.1571, -0.1934, -0.9499,  0.1153, -0.0336,  0.0816, -0.0402,\n",
      "         0.0149, -0.0847], device='cuda:0')\n",
      "g_next tensor([-0.0864, -0.1527, -0.1778, -0.9332,  0.1007, -0.0094,  0.0695, -0.0789,\n",
      "        -0.1966, -0.0829], device='cuda:0')\n",
      "mgs tensor([5.5656e-03, 2.4671e-02, 3.7421e-02, 9.0223e-01, 1.3305e-02, 1.1299e-03,\n",
      "        6.6549e-03, 1.6163e-03, 2.2294e-04, 7.1808e-03], device='cuda:0')\n",
      "gamma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "md tensor([ 0.0101, -0.0212,  0.0368,  0.0870, -0.0162, -0.1332, -0.0602,  0.1308,\n",
      "         0.0628,  0.0356], device='cuda:0')\n",
      "mds tensor([0.0005, 0.0005, 0.0057, 0.0076, 0.0011, 0.0177, 0.0137, 0.0171, 0.0039,\n",
      "        0.0035], device='cuda:0')\n",
      "ma tensor([-0.0722, -0.1915, -0.1259, -0.6774,  0.0702, -0.5401,  0.0658,  0.1324,\n",
      "         0.2490, -0.0306], device='cuda:0')\n",
      "mas tensor([0.0548, 0.3142, 0.1497, 0.4589, 0.0371, 0.2917, 0.1293, 0.0175, 0.0620,\n",
      "        0.0135], device='cuda:0')\n",
      "mdas tensor([ 0.0034,  0.0087,  0.0175, -0.0589,  0.0037,  0.0719,  0.0269,  0.0173,\n",
      "         0.0156,  0.0036], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0746, -0.1571, -0.1934, -0.9499,  0.1153, -0.0336,  0.0816, -0.0402,\n",
      "         0.0149, -0.0847], device='cuda:0')\n",
      "lr tensor([ 3.4957e-02,  1.3756e-02,  7.8281e-02,  2.5672e-01,  7.2628e-02,\n",
      "        -1.4901e-08,  1.1728e-01, -5.9605e-08,  0.0000e+00,  2.3852e-01],\n",
      "       device='cuda:0')\n",
      "tau tensor([2.7683, 1.3717, 2.6774, 1.0000, 2.6826, 1.0000, 2.6186, 1.0000, 1.0000,\n",
      "        2.3958], device='cuda:0')\n",
      "new delta tensor([ 2.6079e-03,  2.1606e-03,  1.5143e-02,  2.4385e-01, -8.3775e-03,\n",
      "        -5.0090e-10, -9.5677e-03, -2.3963e-09, -0.0000e+00,  2.0213e-02],\n",
      "       device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.1930,  0.0415, -0.0203, -0.5137, -0.1309,  0.4074, -0.1091,  0.3536,\n",
      "        -0.4698,  0.1594], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "Epoch 1/1 - Loss: nan\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2a28d81de20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a28da48f40>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1015625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
