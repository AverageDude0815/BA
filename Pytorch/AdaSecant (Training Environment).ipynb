{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3081, -0.1087,  0.3043,  0.1041,  0.2293,  0.0750,  0.0326, -0.0675,\n",
      "         -0.0793, -0.2276],\n",
      "        [ 0.1159,  0.1856, -0.2462,  0.1416, -0.2609,  0.1936, -0.2962,  0.1680,\n",
      "         -0.2507,  0.2440],\n",
      "        [-0.3053,  0.0838, -0.3122, -0.3136, -0.0055,  0.0481,  0.2192, -0.1685,\n",
      "         -0.1876,  0.3061],\n",
      "        [-0.1905,  0.0355, -0.0823,  0.0642,  0.2965, -0.1505,  0.0556,  0.2126,\n",
      "         -0.0331, -0.2114],\n",
      "        [ 0.2961,  0.1053,  0.2811,  0.2284, -0.2608,  0.3040,  0.0059, -0.2007,\n",
      "          0.0551, -0.2997],\n",
      "        [ 0.2021, -0.1502,  0.1903,  0.1113,  0.2819, -0.0266, -0.0167,  0.3124,\n",
      "          0.3095,  0.0867],\n",
      "        [-0.0486,  0.2712, -0.0696, -0.2742,  0.1535,  0.0174, -0.0621,  0.1685,\n",
      "         -0.1153,  0.3161],\n",
      "        [-0.1620,  0.1844, -0.2491,  0.1941, -0.1138,  0.0800, -0.2089, -0.1813,\n",
      "         -0.0189,  0.1637],\n",
      "        [ 0.2841,  0.2738,  0.1812,  0.3000,  0.0425,  0.1462, -0.1272,  0.0324,\n",
      "         -0.2271,  0.2893],\n",
      "        [-0.3059,  0.2775, -0.2649, -0.2565, -0.2835, -0.1936,  0.2606,  0.0716,\n",
      "          0.2803, -0.1177]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1585,  0.1824,  0.1929,  0.2371,  0.2605, -0.2081,  0.1706, -0.0931,\n",
      "         0.0096,  0.1385], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2181, -0.0774,  0.0672, -0.0149,  0.0958,  0.2830, -0.2591, -0.1754,\n",
      "         -0.1793, -0.0219],\n",
      "        [ 0.1282,  0.2968, -0.2960, -0.0014,  0.1139,  0.3032,  0.1584,  0.1975,\n",
      "          0.2100, -0.0535],\n",
      "        [ 0.0874, -0.0147,  0.0527,  0.3162, -0.2206,  0.2873, -0.1586, -0.1853,\n",
      "          0.2525, -0.0820],\n",
      "        [-0.0885, -0.2141,  0.2339, -0.2258,  0.0044, -0.0444,  0.2095, -0.0346,\n",
      "          0.2196, -0.0574],\n",
      "        [-0.0737, -0.1201, -0.1635, -0.0794, -0.0051, -0.0139,  0.2133, -0.1252,\n",
      "         -0.0321, -0.2060],\n",
      "        [ 0.3093,  0.2024, -0.2221, -0.2743, -0.2863,  0.0114, -0.1361, -0.1367,\n",
      "          0.2159, -0.0490],\n",
      "        [-0.1811, -0.2165, -0.0424,  0.3004,  0.1018, -0.2280,  0.0487, -0.2544,\n",
      "          0.0832, -0.2640],\n",
      "        [ 0.1887, -0.0458, -0.0390, -0.0824, -0.1584, -0.2617,  0.2923, -0.1361,\n",
      "          0.1935,  0.3031],\n",
      "        [ 0.1496, -0.0746, -0.2041, -0.2771, -0.1693,  0.1788,  0.0211,  0.2148,\n",
      "          0.3140,  0.1276],\n",
      "        [-0.1765, -0.2120,  0.0219,  0.2213,  0.2853, -0.3023,  0.2437,  0.1091,\n",
      "         -0.2399, -0.2242]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1723,  0.3010, -0.1192, -0.0066, -0.0830, -0.1037, -0.3014,  0.1294,\n",
      "        -0.0961,  0.1157], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        # here we need to introduce some attribute to save params/gradients from old batch.\n",
    "        self.moving_average_of_g = None\n",
    "        self.delta = None\n",
    "        self.memory_size = None\n",
    "        self.gradients = None\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            # all parameters of a model seem to be contained within the same group\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "        \n",
    "        for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    d_p_list.append(p.grad)\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # all parameters of a model seem to be contained within the same group\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "            \n",
    "            # ToDo: update old params/gradients\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "        return #loss\n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # d_p_list[i] corresponds to param[i]    \n",
    "    for i, param in enumerate(params):\n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        if i == 0:\n",
    "            print(param)\n",
    "            print(g)\n",
    "            print(g_next)\n",
    "        \n",
    "        correction_term = None # to be implemented\n",
    "        corrected_gradient = None # to be implemented\n",
    "        \n",
    "        \n",
    "    # update all attributes in optimizer\n",
    "    optimizer.gradients = g\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except:\n",
    "                # whatever happens if there is no next gradient\n",
    "                pass\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3081, -0.1087,  0.3043,  0.1041,  0.2293,  0.0750,  0.0326, -0.0675,\n",
      "         -0.0793, -0.2276],\n",
      "        [ 0.1159,  0.1856, -0.2462,  0.1416, -0.2609,  0.1936, -0.2962,  0.1680,\n",
      "         -0.2507,  0.2440],\n",
      "        [-0.3053,  0.0838, -0.3122, -0.3136, -0.0055,  0.0481,  0.2192, -0.1685,\n",
      "         -0.1876,  0.3061],\n",
      "        [-0.1905,  0.0355, -0.0823,  0.0642,  0.2965, -0.1505,  0.0556,  0.2126,\n",
      "         -0.0331, -0.2114],\n",
      "        [ 0.2961,  0.1053,  0.2811,  0.2284, -0.2608,  0.3040,  0.0059, -0.2007,\n",
      "          0.0551, -0.2997],\n",
      "        [ 0.2021, -0.1502,  0.1903,  0.1113,  0.2819, -0.0266, -0.0167,  0.3124,\n",
      "          0.3095,  0.0867],\n",
      "        [-0.0486,  0.2712, -0.0696, -0.2742,  0.1535,  0.0174, -0.0621,  0.1685,\n",
      "         -0.1153,  0.3161],\n",
      "        [-0.1620,  0.1844, -0.2491,  0.1941, -0.1138,  0.0800, -0.2089, -0.1813,\n",
      "         -0.0189,  0.1637],\n",
      "        [ 0.2841,  0.2738,  0.1812,  0.3000,  0.0425,  0.1462, -0.1272,  0.0324,\n",
      "         -0.2271,  0.2893],\n",
      "        [-0.3059,  0.2775, -0.2649, -0.2565, -0.2835, -0.1936,  0.2606,  0.0716,\n",
      "          0.2803, -0.1177]], device='cuda:0', requires_grad=True)\n",
      "tensor([[ 5.5123e-03,  2.0025e-02,  1.4683e-02,  1.3095e-02,  5.0104e-03,\n",
      "          5.6988e-03,  8.4272e-03,  1.1095e-02,  8.5129e-03,  1.3628e-02],\n",
      "        [ 1.7535e-02,  2.7343e-02,  2.3222e-02,  1.8354e-02,  1.1171e-02,\n",
      "          1.5708e-02,  1.6861e-02,  1.5218e-02,  2.1589e-02,  2.3371e-02],\n",
      "        [-1.0739e-02, -1.0136e-02, -1.3498e-02, -2.6347e-03, -3.5984e-03,\n",
      "         -3.4348e-03, -4.4570e-03, -3.1833e-03, -7.3681e-03, -1.2697e-02],\n",
      "        [-4.6905e-03, -3.9763e-03, -1.2713e-02, -1.6619e-02, -6.7746e-05,\n",
      "         -4.3992e-03,  1.9418e-03,  2.2459e-03, -3.1231e-03, -1.0427e-02],\n",
      "        [-1.2941e-03, -7.3214e-03, -6.2358e-03, -4.9500e-03, -5.6066e-04,\n",
      "          3.1470e-03, -2.9004e-03, -3.3879e-03, -9.3959e-03, -8.0587e-03],\n",
      "        [ 1.5062e-02,  2.3913e-02,  1.6211e-02,  1.9455e-02,  9.2456e-03,\n",
      "          1.5106e-02,  1.7805e-02,  2.1232e-02,  2.0479e-02,  1.6522e-02],\n",
      "        [ 6.5849e-03, -5.6043e-04,  6.3778e-03,  4.7065e-03,  8.6142e-03,\n",
      "          1.1157e-02,  7.5987e-03, -3.7218e-03,  6.3311e-03,  9.7816e-03],\n",
      "        [ 1.8427e-02,  1.4975e-02,  2.1616e-02,  1.7788e-02,  1.3087e-02,\n",
      "          1.9439e-02,  1.2062e-02,  1.2598e-02,  1.5961e-02,  2.1495e-02],\n",
      "        [ 2.5391e-02,  2.8600e-02,  2.8197e-02,  2.7977e-02,  1.5887e-02,\n",
      "          1.9140e-02,  2.2295e-02,  2.1084e-02,  3.2884e-02,  3.3013e-02],\n",
      "        [ 1.0210e-02,  2.3233e-02,  1.9786e-02,  1.9717e-02,  1.2892e-02,\n",
      "          1.3979e-02,  1.6336e-02,  1.4929e-02,  1.3206e-02,  2.2122e-02]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 5.5123e-03,  2.0025e-02,  1.4683e-02,  1.3095e-02,  5.0104e-03,\n",
      "          5.6988e-03,  8.4272e-03,  1.1095e-02,  8.5129e-03,  1.3628e-02],\n",
      "        [ 1.7535e-02,  2.7343e-02,  2.3222e-02,  1.8354e-02,  1.1171e-02,\n",
      "          1.5708e-02,  1.6861e-02,  1.5218e-02,  2.1589e-02,  2.3371e-02],\n",
      "        [-1.0739e-02, -1.0136e-02, -1.3498e-02, -2.6347e-03, -3.5984e-03,\n",
      "         -3.4348e-03, -4.4570e-03, -3.1833e-03, -7.3681e-03, -1.2697e-02],\n",
      "        [-4.6905e-03, -3.9763e-03, -1.2713e-02, -1.6619e-02, -6.7746e-05,\n",
      "         -4.3992e-03,  1.9418e-03,  2.2459e-03, -3.1231e-03, -1.0427e-02],\n",
      "        [-1.2941e-03, -7.3214e-03, -6.2358e-03, -4.9500e-03, -5.6066e-04,\n",
      "          3.1470e-03, -2.9004e-03, -3.3879e-03, -9.3959e-03, -8.0587e-03],\n",
      "        [ 1.5062e-02,  2.3913e-02,  1.6211e-02,  1.9455e-02,  9.2456e-03,\n",
      "          1.5106e-02,  1.7805e-02,  2.1232e-02,  2.0479e-02,  1.6522e-02],\n",
      "        [ 6.5849e-03, -5.6043e-04,  6.3778e-03,  4.7065e-03,  8.6142e-03,\n",
      "          1.1157e-02,  7.5987e-03, -3.7218e-03,  6.3311e-03,  9.7816e-03],\n",
      "        [ 1.8427e-02,  1.4975e-02,  2.1616e-02,  1.7788e-02,  1.3087e-02,\n",
      "          1.9439e-02,  1.2062e-02,  1.2598e-02,  1.5961e-02,  2.1495e-02],\n",
      "        [ 2.5391e-02,  2.8600e-02,  2.8197e-02,  2.7977e-02,  1.5887e-02,\n",
      "          1.9140e-02,  2.2295e-02,  2.1084e-02,  3.2884e-02,  3.3013e-02],\n",
      "        [ 1.0210e-02,  2.3233e-02,  1.9786e-02,  1.9717e-02,  1.2892e-02,\n",
      "          1.3979e-02,  1.6336e-02,  1.4929e-02,  1.3206e-02,  2.2122e-02]],\n",
      "       device='cuda:0')\n",
      "Epoch 1/1 - Loss: 2.3498122692108154\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "f_opt = AdaSecant\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20717704100>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x207192046d0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOHUlEQVR4nO3c34tc533H8fenUkQJSbFdybYsyV011UXVUogYhCG9CPUPJMVYvuiFDYmFcyEMNTi0wVXqf8CJoTGmxkakBpm4mEASIoyCYru5VeqVY8uoiuONSKqNFHuTCyfgCyHy7cUetevNSDu7Z1a76+f9gmHmnPOcmedhwG/NmVmnqpAkteuPVnoCkqSVZQgkqXGGQJIaZwgkqXGGQJIat36lJ7AUGzdurImJiZWehiStKSdPnvx1VW2av39NhmBiYoLJycmVnoYkrSlJfjFsv5eGJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxYwlBkj1J3k4yleTQkONJ8lR3/FSSXfOOr0vy4yQvjWM+kqTR9Q5BknXA08BeYCdwf5Kd84btBXZ0t4PAM/OOPwKc6TsXSdLijeMTwW5gqqrOVtVF4EVg/7wx+4Hna9YJ4LokmwGSbAU+B3xjDHORJC3SOEKwBTg3Z3u62zfqmCeBR4HfX+1FkhxMMplkcmZmpteEJUn/bxwhyJB9NcqYJHcD71XVyYVepKoOV9WgqgabNm1ayjwlSUOMIwTTwLY521uB8yOO+QxwT5KfM3tJ6e+SfHMMc5IkjWgcIXgN2JFke5INwH3A0XljjgIPdL8eug14v6ouVNVXqmprVU105/1nVX1+DHOSJI1ofd8nqKpLSR4GjgPrgOeq6nSSh7rjzwLHgH3AFPAB8GDf15UkjUeq5l/OX/0Gg0FNTk6u9DQkaU1JcrKqBvP3+5fFktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjTMEktQ4QyBJjRtLCJLsSfJ2kqkkh4YcT5KnuuOnkuzq9m9L8sMkZ5KcTvLIOOYjSRpd7xAkWQc8DewFdgL3J9k5b9heYEd3Owg80+2/BPxTVf0lcBvwD0POlSQto3F8ItgNTFXV2aq6CLwI7J83Zj/wfM06AVyXZHNVXaiq1wGq6nfAGWDLGOYkSRrROEKwBTg3Z3uaP/yP+YJjkkwAnwZ+NIY5SZJGNI4QZMi+WsyYJJ8Avg18qap+O/RFkoNJJpNMzszMLHmykqQPG0cIpoFtc7a3AudHHZPkY8xG4IWq+s6VXqSqDlfVoKoGmzZtGsO0JUkwnhC8BuxIsj3JBuA+4Oi8MUeBB7pfD90GvF9VF5IE+HfgTFX96xjmIklapPV9n6CqLiV5GDgOrAOeq6rTSR7qjj8LHAP2AVPAB8CD3emfAb4AvJXkjW7fv1TVsb7zkiSNJlXzL+evfoPBoCYnJ1d6GpK0piQ5WVWD+fv9y2JJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJapwhkKTGGQJJatxYQpBkT5K3k0wlOTTkeJI81R0/lWTXqOdKkpZX7xAkWQc8DewFdgL3J9k5b9heYEd3Owg8s4hzJUnLaByfCHYDU1V1tqouAi8C++eN2Q88X7NOANcl2TziuZKkZTSOEGwBzs3Znu72jTJmlHMBSHIwyWSSyZmZmd6TliTNGkcIMmRfjThmlHNnd1YdrqpBVQ02bdq0yClKkq5k/RieYxrYNmd7K3B+xDEbRjhXkrSMxvGJ4DVgR5LtSTYA9wFH5405CjzQ/XroNuD9qrow4rmSpGXU+xNBVV1K8jBwHFgHPFdVp5M81B1/FjgG7AOmgA+AB692bt85SZJGl6qhl+RXtcFgUJOTkys9DUlaU5KcrKrB/P3+ZbEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjeoUgyQ1JXk7yTnd//RXG7UnydpKpJIfm7H8iyU+SnEry3STX9ZmPJGnx+n4iOAS8WlU7gFe77Q9Jsg54GtgL7ATuT7KzO/wy8NdV9TfAT4Gv9JyPJGmR+oZgP3Cke3wEuHfImN3AVFWdraqLwIvdeVTVD6rqUjfuBLC153wkSYvUNwQ3VdUFgO7+xiFjtgDn5mxPd/vm+yLw/Z7zkSQt0vqFBiR5Bbh5yKHHRnyNDNlX817jMeAS8MJV5nEQOAhw6623jvjSkqSFLBiCqrrjSseSvJtkc1VdSLIZeG/IsGlg25ztrcD5Oc9xALgbuL2qiiuoqsPAYYDBYHDFcZKkxel7aegocKB7fAD43pAxrwE7kmxPsgG4rzuPJHuAfwbuqaoPes5FkrQEfUPwOHBnkneAO7ttktyS5BhA92Xww8Bx4Azwrao63Z3/b8AngZeTvJHk2Z7zkSQt0oKXhq6mqn4D3D5k/3lg35ztY8CxIeP+os/rS5L68y+LJalxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxhkCSGmcIJKlxvUKQ5IYkLyd5p7u//grj9iR5O8lUkkNDjn85SSXZ2Gc+kqTF6/uJ4BDwalXtAF7ttj8kyTrgaWAvsBO4P8nOOce3AXcC/9NzLpKkJegbgv3Ake7xEeDeIWN2A1NVdbaqLgIvdudd9nXgUaB6zkWStAR9Q3BTVV0A6O5vHDJmC3BuzvZ0t48k9wC/rKo3F3qhJAeTTCaZnJmZ6TltSdJl6xcakOQV4OYhhx4b8TUyZF8l+Xj3HHeN8iRVdRg4DDAYDPz0IEljsmAIquqOKx1L8m6SzVV1Iclm4L0hw6aBbXO2twLngU8B24E3k1ze/3qS3VX1q0WsQZLUQ99LQ0eBA93jA8D3hox5DdiRZHuSDcB9wNGqequqbqyqiaqaYDYYu4yAJF1bfUPwOHBnkneY/eXP4wBJbklyDKCqLgEPA8eBM8C3qup0z9eVJI3JgpeGrqaqfgPcPmT/eWDfnO1jwLEFnmuiz1wkSUvjXxZLUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1LlW10nNYtCQzwC9Weh5LsBH49UpP4hpqbb3gmluxVtf8Z1W1af7ONRmCtSrJZFUNVnoe10pr6wXX3IqP2pq9NCRJjTMEktQ4Q3BtHV7pCVxjra0XXHMrPlJr9jsCSWqcnwgkqXGGQJIaZwjGKMkNSV5O8k53f/0Vxu1J8naSqSSHhhz/cpJKsnH5Z91P3zUneSLJT5KcSvLdJNdds8kv0gjvW5I81R0/lWTXqOeuVktdc5JtSX6Y5EyS00keufazX5o+73N3fF2SHyd56drNuqeq8jamG/A14FD3+BDw1SFj1gE/A/4c2AC8Ceycc3wbcJzZP5jbuNJrWu41A3cB67vHXx12/mq4LfS+dWP2Ad8HAtwG/GjUc1fjreeaNwO7usefBH76UV/znOP/CPwH8NJKr2fUm58Ixms/cKR7fAS4d8iY3cBUVZ2tqovAi915l30deBRYK9/i91pzVf2gqi51404AW5d3uku20PtGt/18zToBXJdk84jnrkZLXnNVXaiq1wGq6nfAGWDLtZz8EvV5n0myFfgc8I1rOem+DMF43VRVFwC6+xuHjNkCnJuzPd3tI8k9wC+r6s3lnugY9VrzPF9k9l9aq9Eoa7jSmFHXv9r0WfP/STIBfBr40finOHZ91/wks/+Q+/0yzW9ZrF/pCaw1SV4Bbh5y6LFRn2LIvkry8e457lrq3JbLcq153ms8BlwCXljc7K6ZBddwlTGjnLsa9Vnz7MHkE8C3gS9V1W/HOLflsuQ1J7kbeK+qTib57LgntpwMwSJV1R1XOpbk3csfi7uPiu8NGTbN7PcAl20FzgOfArYDbya5vP/1JLur6ldjW8ASLOOaLz/HAeBu4PbqLrKuQlddwwJjNoxw7mrUZ80k+RizEXihqr6zjPMcpz5r/nvgniT7gD8G/iTJN6vq88s43/FY6S8pPko34Ak+/MXp14aMWQ+cZfY/+pe/jPqrIeN+ztr4srjXmoE9wH8Dm1Z6LQusc8H3jdlrw3O/RPyvxbznq+3Wc80BngeeXOl1XKs1zxvzWdbQl8UrPoGP0g34U+BV4J3u/oZu/y3AsTnj9jH7K4qfAY9d4bnWSgh6rRmYYvZ66xvd7dmVXtNV1voHawAeAh7qHgd4ujv+FjBYzHu+Gm9LXTPwt8xeUjk1573dt9LrWe73ec5zrKkQ+L+YkKTG+ashSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWrc/wLouA/ZRwywxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13671875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
