{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # TODO: fix IF(torch.is_nonzero(...sum())) with element-wise where\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # normalization of gradients\n",
    "        g = g / torch.linalg.norm(g)\n",
    "        g_next = g_next / torch.linalg.norm(g_next)\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = g for first iteration ==> lr = 0\n",
    "            #alpha = copy.deepcopy(g)\n",
    "            \n",
    "            # alpha = 0 for first iteration because no second derivative can be made yet\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        if torch.is_nonzero(optimizer.gamma_denomenators[i].sum()):\n",
    "            # normal calculation of variance reduction term gamma\n",
    "            gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        else:\n",
    "            # gradient is equal to expected gradient in first epoch\n",
    "            # gamma is therefore zero (equivalent to no variance reduction)\n",
    "            gamma = torch.zeros_like(optimizer.gamma_denomenators[i])\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            # delta = lr * corrected_gradient, lr = 1 as initialization\n",
    "            delta = copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        if i == 1:\n",
    "            print('md', optimizer.mean_deltas[i])\n",
    "            print('mds', optimizer.mean_delta_squares[i])\n",
    "            print('ma', optimizer.mean_alphas[i])\n",
    "            print('mas', optimizer.mean_alpha_squares[i])\n",
    "            print('mdas', optimizer.mean_delta_times_alphas[i])\n",
    "        \n",
    "        \n",
    "        # should I update moving averages for g, gamma, and alpha as well? -> memory size will be set again later\n",
    "        \n",
    "        if torch.is_nonzero(optimizer.mean_alphas[i].sum()) or torch.is_nonzero(optimizer.mean_alpha_squares[i].sum()):\n",
    "            # normal calculation of lr\n",
    "            lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "                 - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        else:\n",
    "            # Catch initial exception by setting lr = 1. Because: lr = 0 ==> delta = 0 ==> optimization stops \n",
    "            lr = torch.ones_like(g)\n",
    "        \n",
    "        if i == 1:\n",
    "            print('corrected_gradient', corrected_gradient)\n",
    "            print('lr', lr)\n",
    "        \n",
    "        if torch.is_nonzero(optimizer.mean_delta_squares[i].sum()):\n",
    "            optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / optimizer.mean_delta_squares[i])\n",
    "                                 * optimizer.taus[i] + 1)\n",
    "        else:\n",
    "            optimizer.taus[i] = torch.ones_like(optimizer.taus[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('tau', optimizer.taus[i])\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        if i == 1:\n",
    "            print('new delta', new_delta)\n",
    "        \n",
    "        if i == 1:\n",
    "            print('params', params[i])\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([-0.1317,  0.3444, -0.3229,  0.4781,  0.3327, -0.0833, -0.1049,  0.6159,\n",
      "         0.0468,  0.1449], device='cuda:0')\n",
      "mds tensor([0.0174, 0.1186, 0.1043, 0.2286, 0.1107, 0.0069, 0.0110, 0.3793, 0.0022,\n",
      "        0.0210], device='cuda:0')\n",
      "ma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mdas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "corrected_gradient tensor([-0.1317,  0.3444, -0.3229,  0.4781,  0.3327, -0.0833, -0.1049,  0.6159,\n",
      "         0.0468,  0.1449], device='cuda:0')\n",
      "lr tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([ 0.1317, -0.3444,  0.3229, -0.4781, -0.3327,  0.0833,  0.1049, -0.6159,\n",
      "        -0.0468, -0.1449], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.1982, -0.1553,  0.4025, -0.1740, -0.2139, -0.2044,  0.0443, -0.6995,\n",
      "         0.0248, -0.3683], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([ 0.1317, -0.3444,  0.3229, -0.4781, -0.3327,  0.0833,  0.1049, -0.6159,\n",
      "        -0.0468, -0.1449], device='cuda:0')\n",
      "mds tensor([0.0174, 0.1186, 0.1043, 0.2286, 0.1107, 0.0069, 0.0110, 0.3793, 0.0022,\n",
      "        0.0210], device='cuda:0')\n",
      "ma tensor([-0.0473, -0.5099,  0.8967, -0.5286, -0.7700, -0.2674,  0.0819, -0.8784,\n",
      "         0.2597,  0.2173], device='cuda:0')\n",
      "mas tensor([0.0022, 0.2600, 0.8040, 0.2794, 0.5929, 0.0715, 0.0067, 0.7716, 0.0675,\n",
      "        0.0472], device='cuda:0')\n",
      "mdas tensor([-0.0062,  0.1756,  0.2895,  0.2527,  0.2562, -0.0223,  0.0086,  0.5410,\n",
      "        -0.0122, -0.0315], device='cuda:0')\n",
      "corrected_gradient tensor([-0.1790, -0.1655,  0.5738, -0.0505, -0.4373, -0.3507, -0.0230, -0.2625,\n",
      "         0.3065,  0.3622], device='cuda:0')\n",
      "lr tensor([5.5729e+00, 0.0000e+00, 0.0000e+00, 5.9605e-08, 0.0000e+00, 6.2323e-01,\n",
      "        1.1921e-07, 5.9605e-08, 3.6019e-01, 1.3333e+00], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([ 9.9761e-01,  0.0000e+00, -0.0000e+00,  3.0110e-09,  0.0000e+00,\n",
      "         2.1859e-01,  2.7416e-09,  1.5648e-08, -1.1041e-01, -4.8291e-01],\n",
      "       device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 1.1958, -0.1553,  0.4025, -0.1740, -0.2139,  0.0142,  0.0443, -0.6995,\n",
      "        -0.0856, -0.8512], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([ 9.9761e-01,  0.0000e+00,  0.0000e+00,  3.0110e-09,  0.0000e+00,\n",
      "         2.1859e-01,  2.7416e-09,  1.5648e-08, -1.1041e-01, -4.8291e-01],\n",
      "       device='cuda:0')\n",
      "mds tensor([9.9523e-01, 0.0000e+00, 0.0000e+00, 9.0659e-18, 0.0000e+00, 4.7781e-02,\n",
      "        7.5162e-18, 2.4486e-16, 1.2190e-02, 2.3320e-01], device='cuda:0')\n",
      "ma tensor([ 0.1668,  0.3857, -0.4765,  0.0849,  0.1747,  1.0939,  0.1241,  0.1786,\n",
      "        -0.8481, -0.2685], device='cuda:0')\n",
      "mas tensor([0.0278, 0.1488, 0.2270, 0.0072, 0.0305, 1.1967, 0.0154, 0.0319, 0.7193,\n",
      "        0.0721], device='cuda:0')\n",
      "mdas tensor([1.6637e-01, 0.0000e+00, 0.0000e+00, 2.5561e-10, 0.0000e+00, 2.3912e-01,\n",
      "        3.4029e-10, 2.7954e-09, 9.3640e-02, 1.2967e-01], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0122,  0.2203,  0.0973,  0.0344, -0.2626,  0.7432,  0.1011, -0.0839,\n",
      "        -0.5416,  0.0937], device='cuda:0')\n",
      "lr tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.4901e-08, -1.7764e-15, -7.1054e-15,  0.0000e+00,  0.0000e+00],\n",
      "       device='cuda:0')\n",
      "tau tensor([1., nan, nan, 1., nan, 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([ 0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,\n",
      "         1.1075e-08,  1.7963e-16, -5.9604e-16,  0.0000e+00, -0.0000e+00],\n",
      "       device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 1.1958, -0.1553,  0.4025, -0.1740, -0.2139,  0.0142,  0.0443, -0.6995,\n",
      "        -0.0856, -0.8512], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "Epoch 1/1 - Loss: 2.8918381333351135\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2030c9fae20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2030e706d90>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.109375\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
