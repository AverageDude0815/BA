{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # TODO later: build in normalization of gradients\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = g for first iteration ==> lr = 0\n",
    "            alpha = copy.deepcopy(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        if torch.is_nonzero(optimizer.gamma_denomenators[i].sum()):\n",
    "            # normal calculation of variance reduction term gamma\n",
    "            gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        else:\n",
    "            # gradient is equal to expected gradient in first epoch\n",
    "            # gamma is therefore zero (equivalent to no variance reduction)\n",
    "            gamma = torch.zeros_like(optimizer.gamma_denomenators[i])\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            delta = copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        if i == 1:\n",
    "            print('md', optimizer.mean_deltas[i])\n",
    "            print('mds', optimizer.mean_delta_squares[i])\n",
    "            print('ma', optimizer.mean_alphas[i])\n",
    "            print('mas', optimizer.mean_alpha_squares[i])\n",
    "        \n",
    "        \n",
    "        # should I update moving averages for g, gamma, and alpha as well? -> memory size will be set again later\n",
    "        \n",
    "        lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "             - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('corrected_gradient', corrected_gradient)\n",
    "            print('lr', lr)\n",
    "        \n",
    "        optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / optimizer.mean_delta_squares[i])\n",
    "                             * optimizer.taus[i] + 1)\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        if i == 1:\n",
    "            print('params', params[i])\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    #prepare_model(model)\n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([ 0.0142, -0.0181, -0.0033, -0.0600,  0.0045,  0.0077, -0.0173, -0.0493,\n",
      "         0.0603, -0.0460], device='cuda:0')\n",
      "mds tensor([2.0124e-04, 3.2663e-04, 1.1072e-05, 3.5974e-03, 2.0297e-05, 5.8989e-05,\n",
      "        3.0025e-04, 2.4306e-03, 3.6400e-03, 2.1121e-03], device='cuda:0')\n",
      "ma tensor([ 0.0142, -0.0181, -0.0033, -0.0600,  0.0045,  0.0077, -0.0173, -0.0493,\n",
      "         0.0603, -0.0460], device='cuda:0')\n",
      "mas tensor([2.0124e-04, 3.2663e-04, 1.1072e-05, 3.5974e-03, 2.0297e-05, 5.8989e-05,\n",
      "        3.0025e-04, 2.4306e-03, 3.6400e-03, 2.1121e-03], device='cuda:0')\n",
      "corrected_gradient tensor([ 0.0142, -0.0181, -0.0033, -0.0600,  0.0045,  0.0077, -0.0173, -0.0493,\n",
      "         0.0603, -0.0460], device='cuda:0')\n",
      "lr tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.2156,  0.1211, -0.0035, -0.1595,  0.1343,  0.0470,  0.2654, -0.2692,\n",
      "         0.1060, -0.0193], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mds tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "ma tensor([-0.0590,  0.0200, -0.0697,  0.0027, -0.0507,  0.0196, -0.0066, -0.0119,\n",
      "        -0.0047, -0.0077], device='cuda:0')\n",
      "mas tensor([3.4828e-03, 4.0008e-04, 4.8563e-03, 7.4184e-06, 2.5664e-03, 3.8467e-04,\n",
      "        4.4109e-05, 1.4121e-04, 2.1847e-05, 5.9133e-05], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0448,  0.0019, -0.0730, -0.0573, -0.0462,  0.0273, -0.0240, -0.0612,\n",
      "         0.0557, -0.0536], device='cuda:0')\n",
      "lr tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.2156,  0.1211, -0.0035, -0.1595,  0.1343,  0.0470,  0.2654, -0.2692,\n",
      "         0.1060, -0.0193], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
      "mds tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
      "ma tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
      "mas tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
      "corrected_gradient tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
      "lr tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "2\n",
      "3\n",
      "Epoch 1/1 - Loss: nan\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x291070c1e50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x291071d8160>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11328125\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
