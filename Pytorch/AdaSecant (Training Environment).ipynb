{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1019, -0.2096,  0.2387,  0.0221, -0.1699, -0.2565,  0.0515,  0.0049,\n",
      "          0.0977,  0.0101],\n",
      "        [ 0.0609, -0.2976,  0.3077,  0.0428,  0.2950, -0.2029, -0.1542,  0.2809,\n",
      "          0.2241,  0.0883],\n",
      "        [ 0.1602, -0.2710,  0.0379, -0.2156,  0.0612, -0.0176, -0.1572, -0.2604,\n",
      "          0.2353,  0.0274],\n",
      "        [-0.0295,  0.2863,  0.0900,  0.1534, -0.0436,  0.0571, -0.2137, -0.2722,\n",
      "         -0.2351,  0.0856],\n",
      "        [ 0.0831, -0.0030,  0.1813, -0.1638, -0.1063,  0.0943,  0.2320, -0.1975,\n",
      "          0.2014,  0.2423],\n",
      "        [ 0.1700,  0.2467,  0.0558, -0.1215,  0.2615, -0.1175, -0.1780,  0.2268,\n",
      "          0.0432, -0.2874],\n",
      "        [ 0.2810,  0.0059,  0.1897, -0.2780, -0.1360,  0.2281,  0.0259,  0.2023,\n",
      "          0.0980,  0.1981],\n",
      "        [-0.1903,  0.1915, -0.1181,  0.2009, -0.0194,  0.1523,  0.1199, -0.2610,\n",
      "         -0.2811,  0.2504],\n",
      "        [ 0.3116, -0.1907,  0.2275, -0.2519,  0.2126,  0.0518,  0.2920,  0.2773,\n",
      "          0.2199,  0.1840],\n",
      "        [-0.3127, -0.0845, -0.0942,  0.3060,  0.2503, -0.2886, -0.2092,  0.0716,\n",
      "          0.1367, -0.0563]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1946,  0.1839, -0.1217,  0.1781,  0.2366,  0.0013,  0.2318, -0.3162,\n",
      "        -0.2854, -0.0258], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2634, -0.0896, -0.0468, -0.2627, -0.0461,  0.0379,  0.2936,  0.1618,\n",
      "         -0.2424,  0.1975],\n",
      "        [-0.2565,  0.1705,  0.1068, -0.1754, -0.0420, -0.0250, -0.0169, -0.2048,\n",
      "          0.2833,  0.0027],\n",
      "        [ 0.1436,  0.0760,  0.2298,  0.0185,  0.2342, -0.2599,  0.2214,  0.2418,\n",
      "         -0.2872, -0.2545],\n",
      "        [ 0.1725, -0.1316,  0.0551, -0.0731,  0.0621,  0.1187,  0.0803, -0.1935,\n",
      "          0.0820,  0.0769],\n",
      "        [-0.1130, -0.1115, -0.2404, -0.0274, -0.2715, -0.1501, -0.2163, -0.0665,\n",
      "         -0.1354,  0.0200],\n",
      "        [ 0.1550,  0.2771, -0.0437, -0.2201,  0.2879,  0.1003, -0.0850,  0.1026,\n",
      "         -0.1940, -0.0926],\n",
      "        [-0.0113,  0.3077,  0.1159,  0.2856, -0.0752,  0.0965,  0.2653,  0.0575,\n",
      "          0.0279, -0.0850],\n",
      "        [ 0.2185, -0.1623,  0.3111, -0.1645, -0.2880,  0.0157,  0.1222, -0.2186,\n",
      "          0.1240,  0.1952],\n",
      "        [ 0.0755,  0.1640,  0.0462, -0.2594, -0.1028,  0.1955,  0.0983,  0.0146,\n",
      "          0.0802,  0.1242],\n",
      "        [ 0.3055, -0.2541, -0.2838, -0.0688, -0.2367,  0.2482, -0.0926, -0.1030,\n",
      "          0.1689, -0.0037]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0831,  0.2100,  0.2586,  0.2465,  0.0091, -0.0069,  0.0747, -0.0697,\n",
      "         0.2667, -0.1742], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        # here we need to introduce some attribute to save params/gradients from old batch.\n",
    "        self.moving_average_of_g = None\n",
    "        self.delta = None\n",
    "        self.memory_size = None\n",
    "        self.gradients = None\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                    # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                    if p.grad is not None:\n",
    "                        d_p_list.append(p.grad)\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # all parameters of a model seem to be contained within the same group\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            print('current')\n",
    "            print(self.current_gradients)\n",
    "            print('next')\n",
    "            print(next_gradients)\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "        return #loss\n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # g[i] corresponds to param[i]    \n",
    "    for i, param in enumerate(params):\n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        if i == 0:\n",
    "            print(param)\n",
    "            #print(g)\n",
    "            #print(g_next)\n",
    "        \n",
    "        correction_term = None # to be implemented\n",
    "        corrected_gradient = None # to be implemented\n",
    "        \n",
    "        \n",
    "    # update all attributes in optimizer\n",
    "    optimizer.gradients = g\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            for param in model.parameters():\n",
    "                print(param.grad[0])\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                for param in model.parameters():\n",
    "                    print(param.grad[0])\n",
    "                optimizer.step()\n",
    "            except:\n",
    "                # whatever happens if there is no next gradient\n",
    "                pass\n",
    "            return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0131, -0.0083, -0.0117, -0.0160, -0.0050, -0.0126, -0.0009, -0.0135,\n",
      "        -0.0117, -0.0107], device='cuda:0')\n",
      "tensor(-0.0231, device='cuda:0')\n",
      "tensor([ 0.0031,  0.0030,  0.0049, -0.0035,  0.0040,  0.0013,  0.0031, -0.0037,\n",
      "         0.0078, -0.0028], device='cuda:0')\n",
      "tensor(-0.0017, device='cuda:0')\n",
      "tensor([-0.0113, -0.0172, -0.0152, -0.0065, -0.0074, -0.0097, -0.0160, -0.0116,\n",
      "        -0.0161, -0.0121], device='cuda:0')\n",
      "tensor(-0.0266, device='cuda:0')\n",
      "tensor([ 0.0062, -0.0181,  0.0076, -0.0024, -0.0161, -0.0079, -0.0282,  0.0158,\n",
      "        -0.0148,  0.0109], device='cuda:0')\n",
      "tensor(-0.0328, device='cuda:0')\n",
      "current\n",
      "[tensor([[-0.0113, -0.0172, -0.0152, -0.0065, -0.0074, -0.0097, -0.0160, -0.0116,\n",
      "         -0.0161, -0.0121],\n",
      "        [ 0.0007, -0.0051, -0.0033, -0.0104, -0.0051, -0.0069,  0.0081, -0.0011,\n",
      "          0.0011, -0.0026],\n",
      "        [ 0.0008, -0.0007, -0.0079, -0.0045, -0.0116, -0.0044, -0.0026,  0.0045,\n",
      "         -0.0036, -0.0008],\n",
      "        [-0.0038,  0.0014, -0.0045, -0.0092, -0.0227, -0.0085,  0.0058, -0.0005,\n",
      "         -0.0018, -0.0030],\n",
      "        [ 0.0097, -0.0054,  0.0012, -0.0007,  0.0010,  0.0008,  0.0064,  0.0068,\n",
      "          0.0039,  0.0032],\n",
      "        [-0.0086, -0.0077, -0.0012, -0.0061,  0.0034, -0.0095, -0.0002, -0.0093,\n",
      "         -0.0073, -0.0059],\n",
      "        [-0.0103, -0.0064, -0.0108, -0.0082, -0.0195, -0.0139, -0.0006, -0.0074,\n",
      "         -0.0087, -0.0080],\n",
      "        [-0.0047, -0.0109, -0.0093, -0.0066, -0.0100, -0.0058, -0.0042, -0.0069,\n",
      "         -0.0054, -0.0069],\n",
      "        [ 0.0018,  0.0121,  0.0152,  0.0017,  0.0099,  0.0030,  0.0097,  0.0073,\n",
      "          0.0064,  0.0084],\n",
      "        [-0.0064, -0.0011, -0.0037,  0.0034,  0.0050, -0.0033, -0.0050, -0.0078,\n",
      "         -0.0050, -0.0043]], device='cuda:0'), tensor([-0.0266, -0.0006, -0.0074, -0.0085,  0.0059, -0.0097, -0.0209, -0.0127,\n",
      "         0.0157, -0.0082], device='cuda:0'), tensor([[ 6.1515e-03, -1.8083e-02,  7.6065e-03, -2.3581e-03, -1.6059e-02,\n",
      "         -7.8958e-03, -2.8193e-02,  1.5779e-02, -1.4846e-02,  1.0873e-02],\n",
      "        [-1.0673e-02,  2.7180e-02, -1.4343e-02,  3.7955e-03,  2.9865e-02,\n",
      "          9.9732e-03,  3.3953e-02, -1.7893e-02,  2.2784e-02, -9.4706e-03],\n",
      "        [-1.8770e-03,  8.2972e-03, -5.8722e-03,  7.2327e-04,  9.3291e-03,\n",
      "          1.2328e-03,  2.1132e-02, -9.3539e-03,  8.2681e-03, -9.7916e-03],\n",
      "        [-3.1597e-05,  1.4003e-02, -1.1909e-02,  3.0041e-03,  1.5229e-02,\n",
      "          2.7207e-03,  1.7554e-02, -6.2767e-03,  1.3792e-02, -6.1311e-03],\n",
      "        [-8.8122e-03,  5.8008e-03, -5.9949e-03,  8.4783e-03,  7.9752e-03,\n",
      "          3.5579e-03,  1.0723e-02, -2.9457e-03, -5.8814e-04, -2.1088e-03],\n",
      "        [ 1.9328e-03, -3.5615e-03,  1.4871e-02, -9.6259e-03, -1.5191e-02,\n",
      "          1.4099e-04, -1.4697e-02, -4.4477e-04, -1.9497e-03,  6.5656e-03],\n",
      "        [ 1.6854e-02, -3.2809e-02,  7.3178e-03, -1.1150e-02, -1.1414e-02,\n",
      "         -1.3320e-02, -2.2667e-02,  1.2487e-02, -1.3168e-02, -3.7185e-03],\n",
      "        [ 1.0329e-03, -2.0859e-02,  8.4666e-03, -1.3108e-03, -2.8135e-02,\n",
      "         -1.4021e-03, -2.4858e-02,  1.3378e-02, -1.9658e-02,  9.6706e-03],\n",
      "        [-4.3386e-03,  1.6154e-02, -2.3573e-03,  6.9387e-03,  8.5540e-03,\n",
      "          4.5172e-03,  8.1755e-03, -4.8494e-03,  5.5064e-03,  1.9101e-03],\n",
      "        [-2.3902e-04,  3.8777e-03,  2.2144e-03,  1.5051e-03, -1.5384e-04,\n",
      "          4.7514e-04, -1.1233e-03,  1.1995e-04, -1.3966e-04,  2.2009e-03]],\n",
      "       device='cuda:0'), tensor([-0.0328,  0.0497,  0.0172,  0.0230,  0.0202, -0.0190, -0.0404, -0.0380,\n",
      "         0.0207, -0.0005], device='cuda:0')]\n",
      "next\n",
      "[tensor([[-0.0113, -0.0172, -0.0152, -0.0065, -0.0074, -0.0097, -0.0160, -0.0116,\n",
      "         -0.0161, -0.0121],\n",
      "        [ 0.0007, -0.0051, -0.0033, -0.0104, -0.0051, -0.0069,  0.0081, -0.0011,\n",
      "          0.0011, -0.0026],\n",
      "        [ 0.0008, -0.0007, -0.0079, -0.0045, -0.0116, -0.0044, -0.0026,  0.0045,\n",
      "         -0.0036, -0.0008],\n",
      "        [-0.0038,  0.0014, -0.0045, -0.0092, -0.0227, -0.0085,  0.0058, -0.0005,\n",
      "         -0.0018, -0.0030],\n",
      "        [ 0.0097, -0.0054,  0.0012, -0.0007,  0.0010,  0.0008,  0.0064,  0.0068,\n",
      "          0.0039,  0.0032],\n",
      "        [-0.0086, -0.0077, -0.0012, -0.0061,  0.0034, -0.0095, -0.0002, -0.0093,\n",
      "         -0.0073, -0.0059],\n",
      "        [-0.0103, -0.0064, -0.0108, -0.0082, -0.0195, -0.0139, -0.0006, -0.0074,\n",
      "         -0.0087, -0.0080],\n",
      "        [-0.0047, -0.0109, -0.0093, -0.0066, -0.0100, -0.0058, -0.0042, -0.0069,\n",
      "         -0.0054, -0.0069],\n",
      "        [ 0.0018,  0.0121,  0.0152,  0.0017,  0.0099,  0.0030,  0.0097,  0.0073,\n",
      "          0.0064,  0.0084],\n",
      "        [-0.0064, -0.0011, -0.0037,  0.0034,  0.0050, -0.0033, -0.0050, -0.0078,\n",
      "         -0.0050, -0.0043]], device='cuda:0'), tensor([-0.0266, -0.0006, -0.0074, -0.0085,  0.0059, -0.0097, -0.0209, -0.0127,\n",
      "         0.0157, -0.0082], device='cuda:0'), tensor([[ 6.1515e-03, -1.8083e-02,  7.6065e-03, -2.3581e-03, -1.6059e-02,\n",
      "         -7.8958e-03, -2.8193e-02,  1.5779e-02, -1.4846e-02,  1.0873e-02],\n",
      "        [-1.0673e-02,  2.7180e-02, -1.4343e-02,  3.7955e-03,  2.9865e-02,\n",
      "          9.9732e-03,  3.3953e-02, -1.7893e-02,  2.2784e-02, -9.4706e-03],\n",
      "        [-1.8770e-03,  8.2972e-03, -5.8722e-03,  7.2327e-04,  9.3291e-03,\n",
      "          1.2328e-03,  2.1132e-02, -9.3539e-03,  8.2681e-03, -9.7916e-03],\n",
      "        [-3.1597e-05,  1.4003e-02, -1.1909e-02,  3.0041e-03,  1.5229e-02,\n",
      "          2.7207e-03,  1.7554e-02, -6.2767e-03,  1.3792e-02, -6.1311e-03],\n",
      "        [-8.8122e-03,  5.8008e-03, -5.9949e-03,  8.4783e-03,  7.9752e-03,\n",
      "          3.5579e-03,  1.0723e-02, -2.9457e-03, -5.8814e-04, -2.1088e-03],\n",
      "        [ 1.9328e-03, -3.5615e-03,  1.4871e-02, -9.6259e-03, -1.5191e-02,\n",
      "          1.4099e-04, -1.4697e-02, -4.4477e-04, -1.9497e-03,  6.5656e-03],\n",
      "        [ 1.6854e-02, -3.2809e-02,  7.3178e-03, -1.1150e-02, -1.1414e-02,\n",
      "         -1.3320e-02, -2.2667e-02,  1.2487e-02, -1.3168e-02, -3.7185e-03],\n",
      "        [ 1.0329e-03, -2.0859e-02,  8.4666e-03, -1.3108e-03, -2.8135e-02,\n",
      "         -1.4021e-03, -2.4858e-02,  1.3378e-02, -1.9658e-02,  9.6706e-03],\n",
      "        [-4.3386e-03,  1.6154e-02, -2.3573e-03,  6.9387e-03,  8.5540e-03,\n",
      "          4.5172e-03,  8.1755e-03, -4.8494e-03,  5.5064e-03,  1.9101e-03],\n",
      "        [-2.3902e-04,  3.8777e-03,  2.2144e-03,  1.5051e-03, -1.5384e-04,\n",
      "          4.7514e-04, -1.1233e-03,  1.1995e-04, -1.3966e-04,  2.2009e-03]],\n",
      "       device='cuda:0'), tensor([-0.0328,  0.0497,  0.0172,  0.0230,  0.0202, -0.0190, -0.0404, -0.0380,\n",
      "         0.0207, -0.0005], device='cuda:0')]\n",
      "Parameter containing:\n",
      "tensor([[ 0.1019, -0.2096,  0.2387,  0.0221, -0.1699, -0.2565,  0.0515,  0.0049,\n",
      "          0.0977,  0.0101],\n",
      "        [ 0.0609, -0.2976,  0.3077,  0.0428,  0.2950, -0.2029, -0.1542,  0.2809,\n",
      "          0.2241,  0.0883],\n",
      "        [ 0.1602, -0.2710,  0.0379, -0.2156,  0.0612, -0.0176, -0.1572, -0.2604,\n",
      "          0.2353,  0.0274],\n",
      "        [-0.0295,  0.2863,  0.0900,  0.1534, -0.0436,  0.0571, -0.2137, -0.2722,\n",
      "         -0.2351,  0.0856],\n",
      "        [ 0.0831, -0.0030,  0.1813, -0.1638, -0.1063,  0.0943,  0.2320, -0.1975,\n",
      "          0.2014,  0.2423],\n",
      "        [ 0.1700,  0.2467,  0.0558, -0.1215,  0.2615, -0.1175, -0.1780,  0.2268,\n",
      "          0.0432, -0.2874],\n",
      "        [ 0.2810,  0.0059,  0.1897, -0.2780, -0.1360,  0.2281,  0.0259,  0.2023,\n",
      "          0.0980,  0.1981],\n",
      "        [-0.1903,  0.1915, -0.1181,  0.2009, -0.0194,  0.1523,  0.1199, -0.2610,\n",
      "         -0.2811,  0.2504],\n",
      "        [ 0.3116, -0.1907,  0.2275, -0.2519,  0.2126,  0.0518,  0.2920,  0.2773,\n",
      "          0.2199,  0.1840],\n",
      "        [-0.3127, -0.0845, -0.0942,  0.3060,  0.2503, -0.2886, -0.2092,  0.0716,\n",
      "          0.1367, -0.0563]], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-01dedd327eac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcycle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n\u001b[0m\u001b[0;32m     10\u001b[0m                                                                               \u001b[0mdataset_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                                                               \u001b[0mdataset_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "f_opt = AdaSecant\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
