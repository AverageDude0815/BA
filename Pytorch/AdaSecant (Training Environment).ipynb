{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # TODO: fix IF(torch.is_nonzero(...sum())) with element-wise where\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # normalization of gradients\n",
    "        g = g / torch.linalg.norm(g)\n",
    "        g_next = g_next / torch.linalg.norm(g_next)\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = g for first iteration ==> lr = 0\n",
    "            #alpha = copy.deepcopy(g)\n",
    "            \n",
    "            # alpha = 0 for first iteration because no second derivative can be made yet\n",
    "            alpha = torch.zeros_like(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        if torch.is_nonzero(optimizer.gamma_denomenators[i].sum()):\n",
    "            # normal calculation of variance reduction term gamma\n",
    "            gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        else:\n",
    "            # gradient is equal to expected gradient in first epoch\n",
    "            # gamma is therefore zero (equivalent to no variance reduction)\n",
    "            gamma = torch.zeros_like(optimizer.gamma_denomenators[i])\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            # delta = -lr * corrected_gradient, lr = 1 as initialization\n",
    "            delta = -copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('md', optimizer.mean_deltas[i])\n",
    "            print('mds', optimizer.mean_delta_squares[i])\n",
    "            print('ma', optimizer.mean_alphas[i])\n",
    "            print('mas', optimizer.mean_alpha_squares[i])\n",
    "            print('mdas', optimizer.mean_delta_times_alphas[i])\n",
    "        \n",
    "        \n",
    "        # should I update moving averages for g, gamma, and alpha as well? -> memory size will be set again later\n",
    "        \n",
    "        if torch.is_nonzero(optimizer.mean_alphas[i].sum()) or torch.is_nonzero(optimizer.mean_alpha_squares[i].sum()):\n",
    "            # normal calculation of lr\n",
    "            lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "                 - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        else:\n",
    "            # Catch initial exception by setting lr = 1. Because: lr = 0 ==> delta = 0 ==> optimization stops \n",
    "            lr = torch.ones_like(g)\n",
    "        \n",
    "        if i == 1:\n",
    "            print('corrected_gradient', corrected_gradient)\n",
    "            print('lr', lr)\n",
    "        \n",
    "        if torch.is_nonzero(optimizer.mean_delta_squares[i].sum()):\n",
    "            optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / optimizer.mean_delta_squares[i])\n",
    "                                 * optimizer.taus[i] + 1)\n",
    "        else:\n",
    "            optimizer.taus[i] = torch.ones_like(optimizer.taus[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('tau', optimizer.taus[i])\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        if i == 1:\n",
    "            print('new delta', new_delta)\n",
    "        \n",
    "        if i == 1:\n",
    "            print('params', params[i])\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([ 0.0110, -0.4836,  0.1840, -0.2934, -0.4982,  0.1015, -0.2918,  0.5351,\n",
      "         0.0517, -0.1155], device='cuda:0')\n",
      "mds tensor([1.2200e-04, 2.3382e-01, 3.3871e-02, 8.6109e-02, 2.4822e-01, 1.0306e-02,\n",
      "        8.5159e-02, 2.8638e-01, 2.6748e-03, 1.3337e-02], device='cuda:0')\n",
      "ma tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mdas tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0110,  0.4836, -0.1840,  0.2934,  0.4982, -0.1015,  0.2918, -0.5351,\n",
      "        -0.0517,  0.1155], device='cuda:0')\n",
      "lr tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "new delta tensor([ 0.0110, -0.4836,  0.1840, -0.2934, -0.4982,  0.1015, -0.2918,  0.5351,\n",
      "         0.0517, -0.1155], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.1287, -0.2505, -0.0414, -0.5650, -0.3118,  0.2925, -0.5972,  0.3201,\n",
      "         0.3301, -0.0224], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([ 0.0110, -0.4836,  0.1840, -0.2934, -0.4982,  0.1015, -0.2918,  0.5351,\n",
      "         0.0517, -0.1155], device='cuda:0')\n",
      "mds tensor([1.2200e-04, 2.3382e-01, 3.3871e-02, 8.6109e-02, 2.4822e-01, 1.0306e-02,\n",
      "        8.5159e-02, 2.8638e-01, 2.6748e-03, 1.3337e-02], device='cuda:0')\n",
      "ma tensor([ 0.1250, -0.2707,  0.2064, -0.4512, -0.1613, -0.0980, -0.1853,  0.2919,\n",
      "         0.0974,  0.1423], device='cuda:0')\n",
      "mas tensor([0.0344, 0.1612, 0.0937, 0.4478, 0.0572, 0.0211, 0.0755, 0.1875, 0.0209,\n",
      "        0.0445], device='cuda:0')\n",
      "mdas tensor([ 0.0014,  0.1309,  0.0380,  0.1324,  0.0804, -0.0100,  0.0541,  0.1562,\n",
      "         0.0050, -0.0164], device='cuda:0')\n",
      "corrected_gradient tensor([ 0.2641, -0.1119,  0.2700, -0.6991,  0.1434, -0.3172, -0.1158,  0.1071,\n",
      "         0.1626,  0.4284], device='cuda:0')\n",
      "lr tensor([0.0194, 0.3924, 0.1959, 0.1429, 0.6785, 1.1691, 0.3459, 0.4026, 0.1166,\n",
      "        0.9164], device='cuda:0')\n",
      "tau tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0')\n",
      "new delta tensor([-0.0051,  0.0439, -0.0529,  0.0999, -0.0973,  0.3708,  0.0401, -0.0431,\n",
      "        -0.0190, -0.3926], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.1338, -0.2066, -0.0943, -0.4651, -0.4090,  0.6633, -0.5571,  0.2769,\n",
      "         0.3112, -0.4150], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([ 0.0037,  0.0439, -0.0529, -0.1147, -0.0973,  0.2239, -0.1410, -0.0431,\n",
      "         0.0196, -0.2414], device='cuda:0')\n",
      "mds tensor([7.8477e-05, 1.9292e-03, 2.7973e-03, 5.1503e-02, 9.4621e-03, 6.8109e-02,\n",
      "        4.7180e-02, 1.8606e-03, 1.6224e-03, 7.7336e-02], device='cuda:0')\n",
      "ma tensor([-0.0677,  0.0459, -0.2202,  0.0519, -0.1256,  0.1312, -0.4798,  0.1760,\n",
      "        -0.0162, -0.1048], device='cuda:0')\n",
      "mas tensor([0.0594, 0.0021, 0.0485, 0.4396, 0.0158, 0.0865, 0.3568, 0.0310, 0.0220,\n",
      "        0.0975], device='cuda:0')\n",
      "mdas tensor([ 0.0014,  0.0020,  0.0116,  0.1020,  0.0122,  0.0630,  0.0143, -0.0076,\n",
      "         0.0041,  0.0626], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0179, -0.0577,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan], device='cuda:0')\n",
      "lr tensor([1.1942e-02, 7.6890e-06, 0.0000e+00, 1.1031e-01, 0.0000e+00, 1.5876e-01,\n",
      "        3.2351e-01, 4.9006e-01, 8.6783e-02, 2.4806e-01], device='cuda:0')\n",
      "tau tensor([2.8170, 1.0000, 1.0000, 2.6384, 1.0000, 1.5806, 2.2735, 1.0000, 2.6796,\n",
      "        1.5416], device='cuda:0')\n",
      "new delta tensor([2.1325e-04, 4.4360e-07,        nan,        nan,        nan,        nan,\n",
      "               nan,        nan,        nan,        nan], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([-0.1336, -0.2066,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "Epoch 1/1 - Loss: nan\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b690964130>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b690a308e0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12890625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
