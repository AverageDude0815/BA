{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "Parameter containing:\n",
      "tensor([[-0.2623,  0.2628,  0.1553,  0.2313,  0.2811, -0.2024,  0.0886,  0.1973,\n",
      "          0.1966, -0.3160],\n",
      "        [-0.1550,  0.1647,  0.0949,  0.0962,  0.2050, -0.2306, -0.0858,  0.2056,\n",
      "         -0.1080, -0.2492],\n",
      "        [-0.1384,  0.2282, -0.2572, -0.3113, -0.0016,  0.1864, -0.0217, -0.2469,\n",
      "         -0.2357, -0.0393],\n",
      "        [ 0.3011,  0.2069,  0.0117, -0.1039,  0.2057,  0.0415,  0.1787, -0.1525,\n",
      "         -0.2384,  0.0140],\n",
      "        [ 0.2393,  0.0739, -0.0293,  0.1555, -0.0989, -0.1281,  0.0726, -0.2141,\n",
      "         -0.2731,  0.2803],\n",
      "        [-0.2862, -0.0710,  0.1642, -0.0646,  0.1532,  0.2103, -0.3158,  0.1760,\n",
      "         -0.0242,  0.1615],\n",
      "        [ 0.2635, -0.2931,  0.2193,  0.1610,  0.1634,  0.2670, -0.1961,  0.0385,\n",
      "         -0.1649,  0.2150],\n",
      "        [ 0.0489, -0.1446, -0.1261, -0.1599,  0.0826,  0.0774,  0.1499,  0.1003,\n",
      "         -0.0182, -0.1993],\n",
      "        [ 0.0345,  0.1907,  0.2640,  0.1186,  0.2713,  0.0633, -0.2910, -0.0925,\n",
      "         -0.1255,  0.1823],\n",
      "        [ 0.0129, -0.0088, -0.0415, -0.0050, -0.1445, -0.2624,  0.2355, -0.0849,\n",
      "         -0.1777,  0.1406]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0828, -0.2873, -0.2152, -0.1359, -0.0599,  0.2607, -0.2044, -0.1147,\n",
      "        -0.1302,  0.2339], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0527,  0.1748, -0.1143,  0.0428,  0.1492,  0.0460,  0.1695, -0.0494,\n",
      "         -0.2902, -0.1318],\n",
      "        [-0.0889, -0.1470,  0.0397,  0.1366, -0.0277, -0.3124, -0.0561, -0.2877,\n",
      "         -0.0715, -0.0738],\n",
      "        [-0.1157,  0.1828,  0.0996, -0.0082, -0.0495, -0.1815, -0.2363, -0.1771,\n",
      "         -0.0819, -0.1016],\n",
      "        [-0.2289, -0.0356, -0.0183, -0.2429, -0.1163,  0.3160, -0.1439,  0.1262,\n",
      "          0.2023,  0.2341],\n",
      "        [ 0.0825, -0.2776, -0.0611,  0.0524,  0.1189,  0.0342, -0.1108,  0.0678,\n",
      "          0.1845,  0.2206],\n",
      "        [ 0.2923, -0.2876,  0.2734, -0.1959, -0.0317, -0.2883, -0.1200, -0.1304,\n",
      "         -0.2138,  0.1965],\n",
      "        [-0.0720,  0.0344, -0.2147,  0.0811, -0.3064, -0.1775,  0.0574, -0.0992,\n",
      "         -0.0442, -0.3137],\n",
      "        [-0.0059,  0.0186, -0.0340, -0.1428, -0.1464,  0.1435,  0.2764, -0.1011,\n",
      "          0.2621,  0.1259],\n",
      "        [-0.0197, -0.2223, -0.2531,  0.2159, -0.0225, -0.2956,  0.3097, -0.1006,\n",
      "          0.1421, -0.1673],\n",
      "        [ 0.1988,  0.1399,  0.1688,  0.0807, -0.1619,  0.0241,  0.0688, -0.1034,\n",
      "          0.1970,  0.2289]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2016, -0.3086,  0.1636,  0.2099,  0.1569, -0.1912, -0.2825,  0.0109,\n",
      "         0.2192,  0.2571], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        # here we need to introduce some attribute to save params/gradients from old batch.\n",
    "        self.moving_average_of_g = None\n",
    "        self.delta = None\n",
    "        self.memory_size = None\n",
    "        self.gradients = None\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            # all parameters of a model seem to be contained within the same group\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "        \n",
    "        for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    d_p_list.append(p.grad)\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            # all parameters of a model seem to be contained within the same group\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "            \n",
    "            # ToDo: update old params/gradients\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "        return #loss\n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    # d_p_list[i] corresponds to param[i]    \n",
    "    for i, param in enumerate(params):\n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        if i == 0:\n",
    "            print(param)\n",
    "            print(g)\n",
    "            print(g_next)\n",
    "        \n",
    "        correction_term = None # to be implemented\n",
    "        corrected_gradient = None # to be implemented\n",
    "        \n",
    "        \n",
    "    # update all attributes in optimizer\n",
    "    optimizer.gradients = g\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except:\n",
    "                # whatever happens if there is no next gradient\n",
    "                pass\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2623,  0.2628,  0.1553,  0.2313,  0.2811, -0.2024,  0.0886,  0.1973,\n",
      "          0.1966, -0.3160],\n",
      "        [-0.1550,  0.1647,  0.0949,  0.0962,  0.2050, -0.2306, -0.0858,  0.2056,\n",
      "         -0.1080, -0.2492],\n",
      "        [-0.1384,  0.2282, -0.2572, -0.3113, -0.0016,  0.1864, -0.0217, -0.2469,\n",
      "         -0.2357, -0.0393],\n",
      "        [ 0.3011,  0.2069,  0.0117, -0.1039,  0.2057,  0.0415,  0.1787, -0.1525,\n",
      "         -0.2384,  0.0140],\n",
      "        [ 0.2393,  0.0739, -0.0293,  0.1555, -0.0989, -0.1281,  0.0726, -0.2141,\n",
      "         -0.2731,  0.2803],\n",
      "        [-0.2862, -0.0710,  0.1642, -0.0646,  0.1532,  0.2103, -0.3158,  0.1760,\n",
      "         -0.0242,  0.1615],\n",
      "        [ 0.2635, -0.2931,  0.2193,  0.1610,  0.1634,  0.2670, -0.1961,  0.0385,\n",
      "         -0.1649,  0.2150],\n",
      "        [ 0.0489, -0.1446, -0.1261, -0.1599,  0.0826,  0.0774,  0.1499,  0.1003,\n",
      "         -0.0182, -0.1993],\n",
      "        [ 0.0345,  0.1907,  0.2640,  0.1186,  0.2713,  0.0633, -0.2910, -0.0925,\n",
      "         -0.1255,  0.1823],\n",
      "        [ 0.0129, -0.0088, -0.0415, -0.0050, -0.1445, -0.2624,  0.2355, -0.0849,\n",
      "         -0.1777,  0.1406]], device='cuda:0', requires_grad=True)\n",
      "tensor([[ 3.8795e-03,  6.0586e-03,  5.6939e-03,  5.4721e-03,  1.0576e-02,\n",
      "          1.9549e-03,  7.3936e-03,  1.2069e-02,  1.0097e-02,  6.1300e-03],\n",
      "        [-7.1896e-04,  6.4245e-03,  9.6113e-03,  2.9831e-03,  6.2510e-03,\n",
      "          1.4978e-02,  8.2589e-03, -2.0438e-03,  9.3776e-03,  2.9847e-03],\n",
      "        [-1.9951e-03,  2.9069e-03,  1.1641e-03, -4.7270e-03,  2.8601e-03,\n",
      "          6.8141e-03,  1.0501e-02,  5.4400e-03,  7.5139e-03,  2.8605e-03],\n",
      "        [ 5.1226e-03,  4.5708e-03,  6.5316e-03,  8.8088e-03,  1.0333e-02,\n",
      "         -4.8290e-03,  5.0598e-03,  7.1535e-03, -1.1534e-04,  2.8319e-03],\n",
      "        [ 6.4868e-04, -6.5062e-05,  1.8056e-03,  2.1849e-03,  1.7698e-04,\n",
      "         -2.9299e-05,  4.8548e-04,  5.2552e-03, -4.9184e-03,  4.0043e-04],\n",
      "        [ 3.7592e-03, -9.5806e-04,  6.2904e-03,  4.9310e-03, -4.3911e-03,\n",
      "          1.3736e-02, -4.4942e-03, -1.1907e-03,  5.7458e-03,  8.0530e-04],\n",
      "        [ 5.5800e-03, -7.8815e-04,  3.9003e-03,  4.2391e-03,  4.0082e-03,\n",
      "         -4.8440e-03, -2.2392e-03,  2.6641e-03,  2.0847e-03,  5.1543e-03],\n",
      "        [ 5.3171e-03,  2.9420e-03,  3.4193e-03,  6.2402e-03, -1.1594e-03,\n",
      "          5.4801e-03, -3.3107e-03,  1.4809e-03,  1.6367e-03,  1.0028e-03],\n",
      "        [ 5.4796e-03, -1.9692e-03,  4.7487e-03,  3.0604e-03,  2.1733e-04,\n",
      "         -1.1198e-03,  3.9973e-04,  2.8854e-03,  4.2622e-03,  3.5907e-03],\n",
      "        [ 4.8689e-03,  1.2186e-03,  4.6319e-03,  1.4633e-03,  5.5565e-04,\n",
      "          7.7825e-03,  5.3101e-03,  9.6902e-03,  8.2637e-03,  4.7216e-03]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 3.8795e-03,  6.0586e-03,  5.6939e-03,  5.4721e-03,  1.0576e-02,\n",
      "          1.9549e-03,  7.3936e-03,  1.2069e-02,  1.0097e-02,  6.1300e-03],\n",
      "        [-7.1896e-04,  6.4245e-03,  9.6113e-03,  2.9831e-03,  6.2510e-03,\n",
      "          1.4978e-02,  8.2589e-03, -2.0438e-03,  9.3776e-03,  2.9847e-03],\n",
      "        [-1.9951e-03,  2.9069e-03,  1.1641e-03, -4.7270e-03,  2.8601e-03,\n",
      "          6.8141e-03,  1.0501e-02,  5.4400e-03,  7.5139e-03,  2.8605e-03],\n",
      "        [ 5.1226e-03,  4.5708e-03,  6.5316e-03,  8.8088e-03,  1.0333e-02,\n",
      "         -4.8290e-03,  5.0598e-03,  7.1535e-03, -1.1534e-04,  2.8319e-03],\n",
      "        [ 6.4868e-04, -6.5062e-05,  1.8056e-03,  2.1849e-03,  1.7698e-04,\n",
      "         -2.9299e-05,  4.8548e-04,  5.2552e-03, -4.9184e-03,  4.0043e-04],\n",
      "        [ 3.7592e-03, -9.5806e-04,  6.2904e-03,  4.9310e-03, -4.3911e-03,\n",
      "          1.3736e-02, -4.4942e-03, -1.1907e-03,  5.7458e-03,  8.0530e-04],\n",
      "        [ 5.5800e-03, -7.8815e-04,  3.9003e-03,  4.2391e-03,  4.0082e-03,\n",
      "         -4.8440e-03, -2.2392e-03,  2.6641e-03,  2.0847e-03,  5.1543e-03],\n",
      "        [ 5.3171e-03,  2.9420e-03,  3.4193e-03,  6.2402e-03, -1.1594e-03,\n",
      "          5.4801e-03, -3.3107e-03,  1.4809e-03,  1.6367e-03,  1.0028e-03],\n",
      "        [ 5.4796e-03, -1.9692e-03,  4.7487e-03,  3.0604e-03,  2.1733e-04,\n",
      "         -1.1198e-03,  3.9973e-04,  2.8854e-03,  4.2622e-03,  3.5907e-03],\n",
      "        [ 4.8689e-03,  1.2186e-03,  4.6319e-03,  1.4633e-03,  5.5565e-04,\n",
      "          7.7825e-03,  5.3101e-03,  9.6902e-03,  8.2637e-03,  4.7216e-03]],\n",
      "       device='cuda:0')\n",
      "Epoch 1/1 - Loss: 2.308548331260681\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "f_opt = AdaSecant\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2db22245dc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2db205c1e80>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.109375\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPr0lEQVR4nO3dcaydd13H8ffH1iYSKUx7R7HtbCVl9WqA4KEQIgqYxZYoxYQ/OogsuNgUU+YgFaqQEeI/iEtQw8zSYDNNzJqpE0oi1gUjM6GDnpJtrMzCtcJ6V2LvrHEJxHZlX/+4B3O9nPY+t+f0Xm9/71fS9Dy/5/f7Pb9v7s3zOc9zzrknVYUkqT0/tNwLkCQtDwNAkhplAEhSowwASWqUASBJjVq93AtYjHXr1tXmzZuXexmStKKcOHHimaqamN++ogJg8+bN9Pv95V6GJK0oSb41rN1bQJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJalSnAEiyI8mpJFNJDgzZvy3JsSQXkuyft+9QknNJnrjM3PuTVJJ1V1eCJOlqLBgASVYB9wA7gUng1iST87qdB+4A7h4yxX3AjsvMvQm4BXiq+5IlSePQ5QpgOzBVVaer6iJwGNg1t0NVnauq48Bz8wdX1cPMBsQwnwA+ANSiVi1JGlmXANgAnJmzPT1oG0mStwJPV9VjC/Tbk6SfpD8zMzPqYSVJA10CIEPaRnrGnuQFwIeAuxbqW1UHq6pXVb2JiYlRDitJmqNLAEwDm+ZsbwTOjnjclwFbgMeSfHMw51eSrB9xXklSR6s79DkObE2yBXga2A28Y5SDVtVXgRu/vz0IgV5VPTPKvJKk7ha8AqiqS8A+4CjwJPBAVZ1MsjfJXoAk65NMA+8HPpxkOsnawb77gWPAzYP2269VMZKk7lK1ct6A0+v1qt/vL/cyJGlFSXKiqnrz2/0ksCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUqE4BkGRHklNJppIcGLJ/W5JjSS4k2T9v36Ek55I8Ma/9D5P8S5LHk/xtkhePVIkkaVEWDIAkq4B7gJ3AJHBrksl53c4DdwB3D5niPmDHkPaHgJ+tqlcAXwd+t/uyJUmj6nIFsB2YqqrTVXUROAzsmtuhqs5V1XHgufmDq+phZgNifvs/VNWlweYjwMbFLl6SdPW6BMAG4Myc7elB2zj9BvC5Mc8pSbqCLgGQIW01rgUk+RBwCfjLy+zfk6SfpD8zMzOuw0pS87oEwDSwac72RuDsOA6e5DbgV4B3VtXQUKmqg1XVq6rexMTEOA4rSaJbABwHtibZkmQNsBs4MuqBk+wAPgi8taq+O+p8kqTFWTAABi/U7gOOAk8CD1TVySR7k+wFSLI+yTTwfuDDSaaTrB3sux84Btw8aL99MPUngRcCDyV5NMm9Y69OknRZucydl/+Xer1e9fv95V6GJK0oSU5UVW9+u58ElqRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqM6BUCSHUlOJZlKcmDI/m1JjiW5kGT/vH2HkpxL8sS89h9L8lCSbwz+v2G0UiRJi7FgACRZBdwD7AQmgVuTTM7rdh64A7h7yBT3ATuGtB8APl9VW4HPD7YlSUukyxXAdmCqqk5X1UXgMLBrboeqOldVx4Hn5g+uqoeZDYj5dgF/Pnj858DbFrFuSdKIugTABuDMnO3pQduoXlJV3wYY/H/jsE5J9iTpJ+nPzMyM4bCSJOgWABnSVuNeyOVU1cGq6lVVb2JiYqkOK0nXvS4BMA1smrO9ETg7hmP/e5KXAgz+PzeGOSVJHXUJgOPA1iRbkqwBdgNHxnDsI8Btg8e3AZ8Zw5ySpI4WDICqugTsA44CTwIPVNXJJHuT7AVIsj7JNPB+4MNJppOsHey7HzgG3Dxov30w9ceAW5J8A7hlsC1JWiKpWrLb+SPr9XrV7/eXexmStKIkOVFVvfntfhJYkhplAEhSowwASWqUASBJjVq93AtYCh/97Em+dvbZ5V6GJF21yZ9Yy0d+9WfGOqdXAJLUqCauAMadmpJ0PfAKQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjeoUAEl2JDmVZCrJgSH7tyU5luRCkv1dxiZ5VZJHkjyapJ9k++jlSJK6WjAAkqwC7gF2ApPArUkm53U7D9wB3L2IsR8HPlpVrwLuGmxLkpZIlyuA7cBUVZ2uqovAYWDX3A5Vda6qjgPPLWJsAWsHj18EnL3KGiRJV2F1hz4bgDNztqeB13ac/0pj7wSOJrmb2SB6/bAJkuwB9gDcdNNNHQ8rSVpIlyuADGmrjvNfaex7gPdV1SbgfcCfDZugqg5WVa+qehMTEx0PK0laSJcAmAY2zdneSPfbNVcaexvw4ODxXzF7u0iStES6BMBxYGuSLUnWALuBIx3nv9LYs8AvDh6/GfhG92VLkka14GsAVXUpyT7gKLAKOFRVJ5PsHey/N8l6oM/si7rPJ7kTmKyqZ4eNHUz9m8AfJ1kN/DeD+/ySpKWRqq6385dfr9erfr+/3MuQpBUlyYmq6s1v95PAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjOgVAkh1JTiWZSnJgyP5tSY4luZBkf9exSd472HcyycdHK0WStBirF+qQZBVwD3ALMA0cT3Kkqr42p9t54A7gbV3HJnkTsAt4RVVdSHLjOAqSJHXT5QpgOzBVVaer6iJwmNkT9/+qqnNVdRx4bhFj3wN8rKoufH+OEeqQJC1SlwDYAJyZsz09aOviSmNfDrwhyZeSfCHJa4ZNkGRPkn6S/szMTMfDSpIW0iUAMqStOs5/pbGrgRuA1wG/AzyQ5Af6V9XBqupVVW9iYqLjYSVJC+kSANPApjnbG4GzHee/0thp4MGa9WXgeWBdx3klSSPqEgDHga1JtiRZA+wGjnSc/0pjPw28GSDJy4E1wDOLWLskaQQLvguoqi4l2QccBVYBh6rqZJK9g/33JlkP9IG1wPNJ7gQmq+rZYWMHUx8CDiV5ArgI3FZVXW8tSZJGlJV0zu31etXv95d7GZK0oiQ5UVW9+e1+EliSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJalSnAEiyI8mpJFNJDgzZvy3JsSQXkuxf5Nj9SSrJuqsvQ5K0WAsGQJJVwD3ATmASuDXJ5Lxu54E7gLsXMzbJJuAW4KkRapAkXYUuVwDbgamqOl1VF4HDwK65HarqXFUdB55b5NhPAB8A6moLkCRdnS4BsAE4M2d7etDWxWXHJnkr8HRVPXalCZLsSdJP0p+Zmel4WEnSQroEQIa0dX3GPnRskhcAHwLuWmiCqjpYVb2q6k1MTHQ8rCRpIV0CYBrYNGd7I3C24/yXG/syYAvwWJJvDtq/kmR9x3klSSNa3aHPcWBrki3A08Bu4B0d5x86tqpOAjd+v9MgBHpV9cwi1i5JGsGCAVBVl5LsA44Cq4BDVXUyyd7B/nsHz9z7wFrg+SR3ApNV9eywsdeoFknSIqRq5bwBp9frVb/fX+5lSNKKkuREVfXmt/tJYElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktSoFfU20CQzwLeucvg6oLUPmllzG6y5DaPU/JNV9QN/S2dFBcAokvSHvQ/2embNbbDmNlyLmr0FJEmNMgAkqVEtBcDB5V7AMrDmNlhzG8ZeczOvAUiS/q+WrgAkSXMYAJLUqOsuAJLsSHIqyVSSA0P2J8mfDPY/nuTVy7HOcepQ8zsHtT6e5ItJXrkc6xynhWqe0+81Sb6X5O1Lub5x61JvkjcmeTTJySRfWOo1jluH3+sXJflskscGNb97OdY5TkkOJTmX5InL7B/v+auqrpt/zH7pzL8CPwWsAR5j9otp5vZ5C/A5Zr+v+HXAl5Z73UtQ8+uBGwaPd7ZQ85x+/wj8HfD25V73Nf4Zvxj4GnDTYPvG5V73EtT8e8AfDB5PAOeBNcu99hHr/gXg1cATl9k/1vPX9XYFsB2YqqrTVXUROAzsmtdnF/AXNesR4MVJXrrUCx2jBWuuqi9W1X8ONh9h9juYV7IuP2eA9wJ/A5xbysVdA13qfQfwYFU9BVBVLdRcwAuTBPhRZgPg0tIuc7yq6mFm67icsZ6/rrcA2ACcmbM9PWhbbJ+VZLH13M7sM4iVbMGak2wAfg24dwnXda10+Rm/HLghyT8lOZHkXUu2umujS82fBH4aOAt8Ffjtqnp+aZa3bMZ6/urypfArSYa0zX+fa5c+K0nnepK8idkA+PlruqJrr0vNfwR8sKq+N/sEcUXrUu9q4OeAXwJ+BDiW5JGq+vq1Xtw10qXmXwYeBd4MvAx4KMk/V9Wz13hty2ms56/rLQCmgU1ztjcy++xgsX1Wkk71JHkF8ClgZ1X9xxKt7VrpUnMPODw4+a8D3pLkUlV9eklWOF5df6+fqarvAN9J8jDwSmClBkCXmt8NfKxmb45PJfk3YBvw5aVZ4rIY6/nrersFdBzYmmRLkjXAbuDIvD5HgHcNXk1/HfBfVfXtpV7oGC1Yc5KbgAeBX1/BzwjnWrDmqtpSVZurajPw18BvrdCTP3T7vf4M8IYkq5O8AHgt8OQSr3OcutT8FLNXPCR5CXAzcHpJV7n0xnr+uq6uAKrqUpJ9wFFm30VwqKpOJtk72H8vs+8IeQswBXyX2WcRK1bHmu8Cfhz408Ez4ku1gv+SYsearxtd6q2qJ5P8PfA48Dzwqaoa+lbClaDjz/j3gfuSfJXZWyMfrKoV/Seik9wPvBFYl2Qa+Ajww3Btzl/+KQhJatT1dgtIktSRASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIa9T9d+IUTHcK5ZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
