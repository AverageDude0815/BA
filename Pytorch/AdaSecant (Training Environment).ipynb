{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist = False\n",
    "cifar10 = True\n",
    "cifar100 = False\n",
    "assert mnist ^ cifar10 ^ cifar100\n",
    "\n",
    "n_classes = 10\n",
    "if mnist:\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "if cifar10:\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
    "if cifar100:\n",
    "    n_classes = 100\n",
    "    trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=None)\n",
    "    testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([50000, 3, 32, 32])\n",
      "y_train: torch.Size([50000])\n",
      "X_test: torch.Size([10000, 3, 32, 32])\n",
      "y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    if len(x_grey.size()) == 3:\n",
    "        helper = torch.unsqueeze(x_grey, 1)\n",
    "        return helper.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 1:\n",
    "        return x_grey.repeat(1, 3, 1, 1).float()\n",
    "    elif len(x_grey.size()) == 4 and x_grey.size()[1] == 3:\n",
    "        return x_grey\n",
    "    elif len(x_grey.size()) == 4:\n",
    "        raise ValueError(f'The size of this image tensor is not valid.\\\n",
    "        A 4th order image tensor must have dim1==1 (grey-scale) or dim1==3 (rgb).\\\n",
    "        Unknown format cannot be transformed to rgb.')\n",
    "    else:\n",
    "        raise ValueError(f'The size of this image-tensor is not valid.\\\n",
    "        Must be either 3rd (grey-scale) order tensor or 4th order tensor (rgb).\\\n",
    "        Got order {len(x_grey.size())}')\n",
    "        \n",
    "def swap_data(X):\n",
    "    X1 = np.swapaxes(X, 1, 3)\n",
    "    X2 = np.swapaxes(X1, 2, 3)\n",
    "    return X2\n",
    "\n",
    "if mnist:\n",
    "    X_train_grey = trainset.train_data\n",
    "    X_train = to_rgb(X_train_grey)\n",
    "    X_test_grey = testset.test_data\n",
    "    X_test = to_rgb(X_test_grey)\n",
    "    y_train = trainset.train_labels\n",
    "    y_test = testset.test_labels\n",
    "else:\n",
    "    X_train = torch.tensor(swap_data(trainset.data))\n",
    "    y_train = torch.tensor(trainset.targets)\n",
    "    X_test = torch.tensor(swap_data(testset.data))\n",
    "    y_test = torch.tensor(testset.targets)\n",
    "    \n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load, modifications and GPU optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([256, 10])\n",
      "y_train: torch.Size([256])\n",
      "X_test: torch.Size([256, 10])\n",
      "y_test: torch.Size([256])\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "'''\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "'''\n",
    "\n",
    "# redefining stuff for AdaSecant test runs\n",
    "X_train = torch.rand(256, 10)\n",
    "X_test = torch.rand(256, 10)\n",
    "y_train = torch.randint(10, (256,))\n",
    "y_test = torch.randint(10, (256,))\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "model = torch.nn.Sequential(nn.Linear(10, 10), nn.Linear(10, 10))\n",
    "for param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    print(param.grad)\n",
    "\n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdaSecant\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py\n",
    "\n",
    "<ins>Indices:\n",
    "\n",
    "- $i\\in\\{1, ..., n\\}$: parameter\n",
    "\n",
    "- $k\\in\\{1, ..., m\\}$: sample in minibatch\n",
    "\n",
    "- $j\\in\\{1, ..., n_{iter}\\}$: iteration, $n_{iter} = \\lfloor\\frac{epochs}{m}\\rfloor$ if drop_last else $n_{iter} = \\lceil\\frac{epochs}{m}\\rceil$\n",
    "\n",
    "\n",
    "\n",
    "<ins>Gradients $g, g'$:\n",
    "\n",
    "$g^{(j)} = \\frac{1}{m}\\sum_{k=1}^m\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the current gradient. $g'^{(j)} = \\frac{1}{m}\\sum_{k=m+1}^{2m}\\nabla_{\\theta_i}f(\\theta^{(k)})$ is the gradient calculated on the consecutive minibatch with the current parameters.\n",
    "    \n",
    "Second test run: apply normalization $\\hat{g} = \\frac{g}{||E[g]||_2}$.\n",
    "\n",
    "\n",
    "<ins>Moving averages $E[x]_j$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    E[x]_j &= \\left(1 - \\frac{1}{\\tau^{(j)}}\\right)E[x]_{j-1} + \\frac{1}{\\tau^{(j)}}x^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Elements of the Hessian diagnonal $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i &= \\nabla_{\\theta_i}f(\\theta + \\Delta) - \\nabla_{\\theta_i}f(\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\"The Equation 13 can be easily computed in a stochastic setting from the consecutive minibatches\":\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\alpha_i^{(j)} &= \\nabla_{\\theta_i}f(\\theta^{(j-1)} + \\Delta^{(j-1)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= \\nabla_{\\theta_i}f(\\theta^{(j)}) - \\nabla_{\\theta_i}f(\\theta^{(j-1)}) \\\\\n",
    "    &= g^{(j)} - g^{(j-1)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Correction term $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\gamma_i &= \\frac{E[(g_i - g_i')(g_i - E[g_i]_j)]_j}{E[(g_i - E[g_i]_j)(g_i' - E[g_i]_j)]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Corrected gradient $\\tilde{g}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tilde{g}_i &= \\frac{g_i + \\gamma_i E[g_i]_j}{1+\\gamma_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Estimated learning rate $\\eta^{(j)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\eta_i^{(j)} &= \\frac{\\sqrt{E[\\Delta_i^2]_j}}{\\sqrt{E[\\alpha_i^2]_j}} - \\frac{E[\\alpha_i\\Delta_i]_j}{E[\\alpha_i^2]_j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update memory size:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\tau_i^{(j+1)} &= (1 - \\frac{E^2[\\Delta_i]_{j}}{E[\\Delta_i^2]_{j}})\\tau_i^{(j)} + 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "<ins>Update parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\theta^{(j+1)} &= \\theta^{(j)} - \\eta^{(j)}\\cdot\\tilde{g}^{(j)}\\\\\n",
    "    &= \\theta^{(j)} + \\Delta^{(j)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\Delta^{(j)} &= -\\eta^{(j)}\\cdot\\tilde{g}^{(j)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "\n",
    "class AdaSecant(optim.Optimizer):\n",
    "    r\"\"\"Documentation\n",
    "    Basis copied from https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py.\n",
    "    Left out closure, momentum-related stuff, __setstate__ as it does not seem to be necessary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=None):\n",
    "        if lr is not None:\n",
    "            print('Warning: lr is not a parameter for AdaSecant. Your lr will be set to None')\n",
    "            lr = None\n",
    "        defaults = dict(lr=lr)\n",
    "        super().__init__(params, defaults)\n",
    "        print(len(self.param_groups))\n",
    "        self.ready = False\n",
    "        self.current_gradients = None\n",
    "        self.gamma_numerators = []\n",
    "        self.gamma_denomenators = []\n",
    "        self.mean_gradients = []\n",
    "        self.mean_gradient_squares = []\n",
    "        self.mean_deltas = []\n",
    "        self.mean_delta_squares = []\n",
    "        self.mean_alphas = []\n",
    "        self.mean_alpha_squares = []\n",
    "        self.mean_delta_times_alphas = []\n",
    "        self.old_gradients = []\n",
    "        self.old_deltas = []\n",
    "        self.taus = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    print('hi')\n",
    "                    self.gamma_numerators.append(torch.zeros_like(p))\n",
    "                    self.gamma_denomenators.append(torch.zeros_like(p))\n",
    "                    self.mean_gradients.append(torch.zeros_like(p))\n",
    "                    self.mean_gradient_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_deltas.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_alphas.append(torch.zeros_like(p))\n",
    "                    self.mean_alpha_squares.append(torch.zeros_like(p))\n",
    "                    self.mean_delta_times_alphas.append(torch.zeros_like(p))\n",
    "                    self.taus.append(torch.ones_like(p))\n",
    "                    self.old_gradients.append(None)\n",
    "                    self.old_deltas.append(None)\n",
    "                    \n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def pre_step(self):\n",
    "        for group in self.param_groups:\n",
    "            d_p_list = []\n",
    "        \n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    d_p_list.append(copy.deepcopy(p.grad))\n",
    "        \n",
    "        self.current_gradients = d_p_list\n",
    "        self.ready = True\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.ready:\n",
    "            raise RuntimeError('You must perform optimizer.pre_step() before performing ' +\n",
    "                               'optimizer.step() when using AdaSecant.\\n' +\n",
    "                               'pre_step ensures that the gradient is saved while step will generate the ' +\n",
    "                               'gradient on the new batch.\\n' +\n",
    "                               'Recommended call sequence:' +\n",
    "                               ' \\n model.zero_grad(), \\n loss = ..., \\n loss.backwards(), \\n ' +\n",
    "                               'optimizer.pre_step(), \\n model.zero_grad(), \\n loss = ..., \\n ' +\n",
    "                               'loss.backwards(), \\n optimizer.step()')\n",
    "        else:\n",
    "            self.ready = False\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            next_gradients = []\n",
    "\n",
    "            for p in group['params']:\n",
    "                # subgrouping of parameters for each layer, bias and weights separately (each tensor)\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    next_gradients.append(p.grad)\n",
    "            \n",
    "            #print(group)\n",
    "            print('enter adasecant')\n",
    "            adasecant(self, params_with_grad, next_gradients)\n",
    "            \n",
    "    \n",
    "def adasecant(optimizer: AdaSecant, params: List[torch.Tensor], next_gradients: List[torch.Tensor]):\n",
    "    \n",
    "    for i, param in enumerate(params):\n",
    "        \n",
    "        g = optimizer.current_gradients[i]\n",
    "        g_next = next_gradients[i]\n",
    "        print(i)\n",
    "        \n",
    "        # TODO later: build in normalization of gradients\n",
    "        \n",
    "        optimizer.mean_gradients[i] = moving_average(optimizer.mean_gradients[i], g, optimizer.taus[i])\n",
    "        optimizer.mean_gradient_squares[i] = moving_average(optimizer.mean_gradient_squares[i], g ** 2, optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_gradients[i] is None:\n",
    "            # alpha = g for first iteration ==> lr = 0\n",
    "            alpha = copy.deepcopy(g)\n",
    "        else:\n",
    "            # normal calculation of alpha\n",
    "            alpha = g - optimizer.old_gradients[i]\n",
    "        optimizer.mean_alphas[i] = moving_average(optimizer.mean_alphas[i], alpha, optimizer.taus[i])\n",
    "        optimizer.mean_alpha_squares[i] = moving_average(optimizer.mean_alpha_squares[i], alpha ** 2, optimizer.taus[i])\n",
    "        \n",
    "        optimizer.gamma_numerators[i] = moving_average(optimizer.gamma_numerators[i],\n",
    "                                                       (g - g_next)\n",
    "                                                       * (g - optimizer.mean_gradients[i]),\n",
    "                                                       optimizer.taus[i])\n",
    "        optimizer.gamma_denomenators[i] = moving_average(optimizer.gamma_denomenators[i],\n",
    "                                                         (g - optimizer.mean_gradients[i])\n",
    "                                                         * (g_next - optimizer.mean_gradients[i]),\n",
    "                                                         optimizer.taus[i])\n",
    "        if torch.is_nonzero(optimizer.gamma_denomenators[i].sum()):\n",
    "            # normal calculation of variance reduction term gamma\n",
    "            gamma = optimizer.gamma_numerators[i] / optimizer.gamma_denomenators[i]\n",
    "        else:\n",
    "            # gradient is equal to expected gradient in first epoch\n",
    "            # gamma is therefore zero (equivalent to no variance reduction)\n",
    "            gamma = torch.zeros_like(optimizer.gamma_denomenators[i])\n",
    "            \n",
    "        corrected_gradient = (g + gamma * optimizer.mean_gradients[i]) / (1 + gamma)\n",
    "        \n",
    "        optimizer.taus[i] = torch.where(needs_memory_reset(g, alpha, optimizer, i),\n",
    "                                        torch.full_like(optimizer.taus[i], 2.2),\n",
    "                                        optimizer.taus[i])\n",
    "        \n",
    "        if optimizer.old_deltas[i] is None:\n",
    "            delta = copy.deepcopy(corrected_gradient)\n",
    "        else:\n",
    "            delta = optimizer.old_deltas[i]\n",
    "        optimizer.mean_deltas[i] = moving_average(optimizer.mean_deltas[i], delta, optimizer.taus[i])\n",
    "        optimizer.mean_delta_squares[i] = moving_average(optimizer.mean_delta_squares[i], delta ** 2, optimizer.taus[i])\n",
    "        optimizer.mean_delta_times_alphas[i] = moving_average(optimizer.mean_delta_times_alphas[i],\n",
    "                                                             delta * alpha,\n",
    "                                                             optimizer.taus[i])\n",
    "        if i == 1:\n",
    "            print('md', optimizer.mean_deltas[i])\n",
    "            print('mds', optimizer.mean_delta_squares[i])\n",
    "            print('ma', optimizer.mean_alphas[i])\n",
    "            print('mas', optimizer.mean_alpha_squares[i])\n",
    "        \n",
    "        \n",
    "        # should I update moving averages for g, gamma, and alpha as well? -> memory size will be set again later\n",
    "        \n",
    "        lr = (torch.sqrt(optimizer.mean_delta_squares[i]) / torch.sqrt(optimizer.mean_alpha_squares[i])\n",
    "             - optimizer.mean_delta_times_alphas[i] / optimizer.mean_alpha_squares[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('corrected_gradient', corrected_gradient)\n",
    "            print('lr', lr)\n",
    "        \n",
    "        if torch.is_nonzero(optimizer.mean_delta_squares[i].sum()):\n",
    "            optimizer.taus[i] = ((1 - optimizer.mean_deltas[i] ** 2 / optimizer.mean_delta_squares[i])\n",
    "                                 * optimizer.taus[i] + 1)\n",
    "        else:\n",
    "            optimizer.taus[i] = torch.ones_like(optimizer.taus[i])\n",
    "        \n",
    "        if i == 1:\n",
    "            print('tau', optimizer.taus[i])\n",
    "        \n",
    "        new_delta = -lr * corrected_gradient\n",
    "        params[i] += new_delta\n",
    "        \n",
    "        if i == 1:\n",
    "            print('params', params[i])\n",
    "        \n",
    "        optimizer.old_deltas[i] = new_delta\n",
    "        optimizer.old_gradients[i] = g\n",
    "        \n",
    "\n",
    "def moving_average(mean, new_value, tau):\n",
    "    return (1 - 1 / tau) * mean + (1 / tau) * new_value\n",
    "\n",
    "\n",
    "def needs_memory_reset(g, alpha, optimizer, i):\n",
    "    return torch.logical_or(torch.gt(torch.abs(g - optimizer.mean_gradients[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_gradient_squares[i] \n",
    "                                                              - optimizer.mean_gradients[i] ** 2))),\n",
    "                            torch.gt(torch.abs(alpha - optimizer.mean_alphas[i]),\n",
    "                                     2 * torch.sqrt(torch.abs(optimizer.mean_alpha_squares[i]\n",
    "                                                              - optimizer.mean_alphas[i] ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from more_itertools import peekable\n",
    "\n",
    "def adasecant_dataloader(dataset, batch_size, shuffle=False, drop_last=False):\n",
    "    data_loader = peekable(iter(data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)))\n",
    "    return data_loader\n",
    "\n",
    "data_loader = adasecant_dataloader(dataset_test, 60, True, True)\n",
    "for batch in data_loader:\n",
    "    #print('current', batch['y'])\n",
    "    try:\n",
    "        peek = data_loader.peek()\n",
    "        #print('next', peek['y'])\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, batch_size=1000):\n",
    "    \n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        yhat = model.forward(batch['X'].float().to(device))\n",
    "        y = batch['y'].long().to(device)\n",
    "        batch_loss = f_loss(yhat, y)\n",
    "        loss += batch_loss.item() * len(batch['X'])\n",
    "        correct += (torch.argmax(yhat, dim=1) == y).float().sum().item()\n",
    "    accuracy = correct / len(dataset)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def get_scheduler(optimizer, base_lr, max_lr, epochs_per_cycle, len_dataset, batch_size):\n",
    "    if epochs_per_cycle is None:\n",
    "        epochs_per_cycle = epochs\n",
    "    iterations_per_cycle = epochs_per_cycle * (len_dataset // batch_size)\n",
    "    return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=iterations_per_cycle / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, batch_size=64, epochs=1, \n",
    "                f_loss=F.cross_entropy, epochs_per_cycle=None):\n",
    "    \n",
    "    #prepare_model(model)\n",
    "    optimizer = AdaSecant(model.parameters())\n",
    "    lr_history = []\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # evaluate initial state of model\n",
    "    initial_training_loss, _ = evaluate_model(model, dataset)\n",
    "    epoch_losses.append(initial_training_loss)\n",
    "    validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_accuracy.append(accuracy)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # training and epoch loss logging\n",
    "        # drop last to avoid stochastic outliers in gradient update\n",
    "        data_loader = adasecant_dataloader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in data_loader:\n",
    "            \n",
    "            # prepare adasecant with current gradient\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item() * len(batch['X'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.pre_step()\n",
    "            \n",
    "            # run adasecant with next gradient (addiotionally to current gradient)\n",
    "            model.zero_grad()\n",
    "            try:\n",
    "                next_batch = data_loader.peek()\n",
    "                yhat = model.forward(next_batch['X'].float().to(device))\n",
    "                batch_loss = f_loss(yhat, next_batch['y'].long().to(device))\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            except StopIteration:\n",
    "                # peek in first\n",
    "                pass\n",
    "            #return\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss / len(dataset)}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # calculate validation loss and accuracy\n",
    "        validation_loss, accuracy = evaluate_model(model, validation_set)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracy.append(accuracy)\n",
    "        \n",
    "    return (np.array(epoch_losses) / len(dataset), \n",
    "            np.array(validation_losses) / len(validation_set), \n",
    "            validation_accuracy, \n",
    "            lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([-0.0434, -0.0337, -0.0233,  0.0177,  0.0304,  0.0321,  0.0077,  0.0573,\n",
      "         0.0095, -0.0284], device='cuda:0')\n",
      "mds tensor([1.8803e-03, 1.1329e-03, 5.4494e-04, 3.1158e-04, 9.2546e-04, 1.0275e-03,\n",
      "        5.9015e-05, 3.2805e-03, 9.0970e-05, 8.0474e-04], device='cuda:0')\n",
      "ma tensor([-0.0434, -0.0337, -0.0233,  0.0177,  0.0304,  0.0321,  0.0077,  0.0573,\n",
      "         0.0095, -0.0284], device='cuda:0')\n",
      "mas tensor([1.8803e-03, 1.1329e-03, 5.4494e-04, 3.1158e-04, 9.2546e-04, 1.0275e-03,\n",
      "        5.9015e-05, 3.2805e-03, 9.0970e-05, 8.0474e-04], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0434, -0.0337, -0.0233,  0.0177,  0.0304,  0.0321,  0.0077,  0.0573,\n",
      "         0.0095, -0.0284], device='cuda:0')\n",
      "lr tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.2281, -0.1466,  0.1002,  0.0343, -0.2223,  0.2747, -0.1972,  0.2510,\n",
      "         0.3148, -0.1655], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mds tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "ma tensor([ 0.0406,  0.0333,  0.0073,  0.0189, -0.0275, -0.0142, -0.0236, -0.0929,\n",
      "        -0.0081,  0.0505], device='cuda:0')\n",
      "mas tensor([1.6483e-03, 1.1086e-03, 5.2861e-05, 3.5626e-04, 7.5804e-04, 2.0278e-04,\n",
      "        5.5761e-04, 8.6234e-03, 6.5489e-05, 2.5551e-03], device='cuda:0')\n",
      "corrected_gradient tensor([-0.0028, -0.0004, -0.0161,  0.0365,  0.0029,  0.0178, -0.0159, -0.0356,\n",
      "         0.0014,  0.0222], device='cuda:0')\n",
      "lr tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.2281, -0.1466,  0.1002,  0.0343, -0.2223,  0.2747, -0.1972,  0.2510,\n",
      "         0.3148, -0.1655], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "enter adasecant\n",
      "0\n",
      "1\n",
      "md tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "mds tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "ma tensor([ 0.0215,  0.0024,  0.0402, -0.0424, -0.0006, -0.0517, -0.0029,  0.0167,\n",
      "         0.0322, -0.0412], device='cuda:0')\n",
      "mas tensor([4.6054e-04, 5.8036e-06, 1.6161e-03, 1.7991e-03, 3.1784e-07, 2.6690e-03,\n",
      "        8.6168e-06, 2.7824e-04, 1.0398e-03, 1.6992e-03], device='cuda:0')\n",
      "corrected_gradient tensor([ 0.0187,  0.0020,  0.0241, -0.0059,  0.0023, -0.0338, -0.0189, -0.0189,\n",
      "         0.0337, -0.0190], device='cuda:0')\n",
      "lr tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tau tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "params Parameter containing:\n",
      "tensor([ 0.2281, -0.1466,  0.1002,  0.0343, -0.2223,  0.2747, -0.1972,  0.2510,\n",
      "         0.3148, -0.1655], device='cuda:0', requires_grad=True)\n",
      "2\n",
      "3\n",
      "Epoch 1/1 - Loss: 2.3242886066436768\n"
     ]
    }
   ],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "f_loss = F.cross_entropy\n",
    "cycle = epochs\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy, lr_history = train_model(model.to(device),\n",
    "                                                                              dataset_train,\n",
    "                                                                              dataset_test,\n",
    "                                                                              batch_size,\n",
    "                                                                              epochs,\n",
    "                                                                              f_loss,\n",
    "                                                                              cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ab253f3e20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label='training loss')\n",
    "plt.plot(validation_loss, label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ab258315b0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09765625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(validation_accuracy)\n",
    "print(max(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1045e0c59eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "with open('val_accuracy', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_accuracy)\n",
    "    \n",
    "with open('lr_history', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(lr_history)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')\n",
    "files.download('val_accuracy')\n",
    "files.download('lr_history')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
