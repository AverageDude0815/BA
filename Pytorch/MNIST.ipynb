{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch  # Package name: torch (for pip), pytorch (for conda)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class BasicDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return dict(X=self.X[idx], y=self.y[idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "#Some methods for evaluating results\n",
    "def get_confusion_matrix(y_pred, y_true, n_classes):\n",
    "    assert len(y_pred) == len(y_true)\n",
    "    results = np.zeros(shape=(n_classes, n_classes))\n",
    "    for i in range(len(y_pred)):\n",
    "        results[y_pred[i], y_true[i]] += 1\n",
    "    return results\n",
    "    \n",
    "def precision(y_pred, y_true, n_classes):\n",
    "    precision = np.zeros(shape=n_classes)\n",
    "    confusion_matrix = get_confusion_matrix(y_pred, y_true, n_classes)\n",
    "    for i in range(n_classes):\n",
    "        sum_of_row = confusion_matrix[i].sum()\n",
    "        if confusion_matrix[i, i] != 0:\n",
    "            precision[i] = confusion_matrix[i, i] / sum_of_row\n",
    "    return precision\n",
    "\n",
    "def recall(y_pred, y_true, n_classes):\n",
    "    recall = np.zeros(shape=n_classes)\n",
    "    confusion_matrix = get_confusion_matrix(y_pred, y_true, n_classes)\n",
    "    for i in range(n_classes):\n",
    "        sum_of_column = confusion_matrix[:,i].sum()\n",
    "        if confusion_matrix[i, i] != 0:\n",
    "            recall[i] = confusion_matrix[i, i] / sum_of_column\n",
    "    return recall\n",
    "\n",
    "def f1score(y_pred, y_true, n_classes):\n",
    "    p = precision(y_pred, y_true, n_classes)\n",
    "    r = recall(y_pred, y_true, n_classes)\n",
    "    f1 = np.zeros(shape=n_classes)\n",
    "    for i in range(n_classes):\n",
    "        if r[i] + p[i] != 0:\n",
    "            f1[i] = 2 * p[i] * r[i] / (r[i] + p[i])\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "n_classes = 10\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "print(mnist_trainset)\n",
    "print(mnist_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:62: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([60000, 3, 28, 28])\n",
      "X_test: torch.Size([10000, 3, 28, 28])\n",
      "y_train: torch.Size([60000])\n",
      "y_test: torch.Size([10000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:67: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:52: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "def to_rgb(x_grey: torch.Tensor) -> torch.Tensor:\n",
    "    helper = torch.unsqueeze(x_grey, 1)\n",
    "    return helper.repeat(1, 3, 1, 1).float()\n",
    "\n",
    "X_train_grey = mnist_trainset.train_data\n",
    "X_train = to_rgb(X_train_grey)\n",
    "X_test_grey = mnist_testset.test_data\n",
    "X_test = to_rgb(X_test_grey)\n",
    "y_train = mnist_trainset.train_labels\n",
    "y_test = mnist_testset.test_labels\n",
    "print('X_train:', X_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(pretrained=False) # set model here\n",
    "in_ftr = model.fc.in_features\n",
    "out_ftr = n_classes\n",
    "model.fc = nn.Linear(in_ftr,out_ftr,bias=True)\n",
    "    \n",
    "dataset_train = BasicDataset(X_train, y_train)\n",
    "dataset_test = BasicDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check specs for GPU-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "cuda device name: NVIDIA GeForce 940MX\n",
      "cuda device id 0\n"
     ]
    }
   ],
   "source": [
    "print('cuda available:', torch.cuda.is_available())\n",
    "print('cuda device name:', torch.cuda.get_device_name())\n",
    "print('cuda device id', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move model and data to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worked? True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train.to(device)\n",
    "print('worked?', X_train.to(device).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 8, 1, 5, 6], device='cuda:0')\n",
      "tensor([5, 0, 4, 1, 9])\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "yhat = torch.argmax(model.forward(X_train[:5].to(device)), dim=1)\n",
    "print(yhat)\n",
    "print(y_train[:5])\n",
    "correct = (yhat == y_train[:5].to(device)).float().sum()\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataset):\n",
    "    data_loader = data.DataLoader(dataset=dataset, batch_size=100, shuffle=True, drop_last=True)\n",
    "    correct = 0\n",
    "    for batch in data_loader:\n",
    "        yhat = torch.argmax(model.forward(batch['X'].float().to(device)), dim=1)\n",
    "        y = batch['y'].long().to(device)\n",
    "        correct += (yhat == y).float().sum().item()\n",
    "    return correct / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0736\n"
     ]
    }
   ],
   "source": [
    "print(get_accuracy(model, dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, validation_set, base_lr=0.01, max_lr=0.01, batch_size=64, epochs=1, \n",
    "                f_opt=optim.SGD, f_loss=F.cross_entropy):\n",
    "    \n",
    "    optimizer = f_opt(model.parameters(), lr=base_lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr)\n",
    "    validation_accuracy = []\n",
    "    epoch_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        data_loader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        val_data_loader = data.DataLoader(dataset=validation_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        epoch_loss = 0.0 \n",
    "        for batch in data_loader:\n",
    "            model.zero_grad()\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            epoch_loss += batch_loss.item()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            validation_accuracy.append(get_accuracy(model, validation_set))\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {epoch_loss}')\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # Calculate validation loss\n",
    "        validation_loss = 0.0\n",
    "        for batch in val_data_loader:\n",
    "            yhat = model.forward(batch['X'].float().to(device))\n",
    "            batch_loss = f_loss(yhat, batch['y'].long().to(device))\n",
    "            validation_loss += batch_loss.item()\n",
    "        validation_losses.append(validation_loss)\n",
    "        \n",
    "    return epoch_losses, validation_losses, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 0.01\n",
    "max_lr = 0.1\n",
    "batch_size = 100\n",
    "epochs = 3\n",
    "f_opt=optim.SGD\n",
    "f_loss=F.cross_entropy\n",
    "\n",
    "training_loss, validation_loss, validation_accuracy = train_model(model.to(device), dataset_train, dataset_test,\n",
    "                                                                  base_lr, max_lr, batch_size, epochs,\n",
    "                                                                  f_opt, f_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_size / len(X_train))\n",
    "print(batch_size / len(X_test))\n",
    "\n",
    "plt.plot(np.array(training_loss) * batch_size / len(X_train), label='training loss')\n",
    "plt.plot(np.array(validation_loss) * batch_size / len(X_test), label='validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Cross Entropy Loss')\n",
    "#plt.ylim(0.0, 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from google.colab import files\n",
    "\n",
    "with open('train_loss', 'w') as f:\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(training_loss)\n",
    "\n",
    "with open('val_loss', 'w') as f: \n",
    "    write = csv.writer(f)  \n",
    "    write.writerow(validation_loss)\n",
    "\n",
    "files.download('train_loss')\n",
    "files.download('val_loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
